"""
ç”Ÿäº§çº§ç‰¹å¾ç¼“å­˜ç³»ç»Ÿ - Week 2 ä¼˜åŒ–
å®ç°SQLite-basedçš„ç‰¹å¾ç¼“å­˜ï¼Œæä¾›æŒä¹…åŒ–å­˜å‚¨å’Œæ™ºèƒ½ç®¡ç†
"""

import sqlite3
import pickle
import hashlib
import time
import os
import threading
from pathlib import Path
from typing import Any, Optional, Dict, List, Tuple, Union
import numpy as np
import pandas as pd
import warnings


class ProductionFeatureCache:
    """
    ç”Ÿäº§çº§ç‰¹å¾ç¼“å­˜ç³»ç»Ÿ
    æä¾›é«˜æ€§èƒ½çš„ç‰¹å¾å­˜å‚¨ã€æ£€ç´¢å’Œç®¡ç†åŠŸèƒ½
    """

    def __init__(self, cache_dir: str = "cache/features", max_size_gb: float = 2.0,
                 cleanup_threshold: float = 0.8, compression: bool = True):
        """
        åˆå§‹åŒ–ç‰¹å¾ç¼“å­˜ç³»ç»Ÿ

        Args:
            cache_dir: ç¼“å­˜ç›®å½•
            max_size_gb: æœ€å¤§ç¼“å­˜å¤§å°ï¼ˆGBï¼‰
            cleanup_threshold: è§¦å‘æ¸…ç†çš„é˜ˆå€¼ï¼ˆå æœ€å¤§å®¹é‡çš„æ¯”ä¾‹ï¼‰
            compression: æ˜¯å¦å¯ç”¨æ•°æ®å‹ç¼©
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        self.max_size_bytes = int(max_size_gb * 1024**3)
        self.cleanup_threshold = cleanup_threshold
        self.compression = compression

        # æ•°æ®åº“è¿æ¥
        self.db_path = self.cache_dir / "cache_metadata.db"
        self.connection_lock = threading.Lock()

        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'hits': 0,
            'misses': 0,
            'sets': 0,
            'deletions': 0,
            'cleanup_operations': 0,
            'total_bytes_stored': 0,
            'total_bytes_retrieved': 0
        }

        self._init_database()
        print(f"  ç‰¹å¾ç¼“å­˜åˆå§‹åŒ–: ç›®å½•={cache_dir}, æœ€å¤§å®¹é‡={max_size_gb}GB")

    def _init_database(self):
        """åˆå§‹åŒ–ç¼“å­˜å…ƒæ•°æ®æ•°æ®åº“"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS cache_entries (
                    key TEXT PRIMARY KEY,
                    file_path TEXT NOT NULL,
                    size_bytes INTEGER NOT NULL,
                    access_time REAL NOT NULL,
                    creation_time REAL NOT NULL,
                    hit_count INTEGER DEFAULT 0,
                    data_hash TEXT,
                    metadata TEXT,
                    is_compressed BOOLEAN DEFAULT FALSE
                )
            """)

            # åˆ›å»ºç´¢å¼•æé«˜æŸ¥è¯¢æ€§èƒ½
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_access_time ON cache_entries(access_time)
            """)
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_hit_count ON cache_entries(hit_count)
            """)
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_size_bytes ON cache_entries(size_bytes)
            """)

            conn.commit()

    def _generate_key(self, data_identifier: Union[str, Dict, np.ndarray, pd.DataFrame]) -> str:
        """
        ç”Ÿæˆæ•°æ®å”¯ä¸€æ ‡è¯†é”®

        Args:
            data_identifier: æ•°æ®æ ‡è¯†ç¬¦ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²ã€å­—å…¸ã€æ•°ç»„ç­‰ï¼‰

        Returns:
            å”¯ä¸€çš„ç¼“å­˜é”®
        """
        if isinstance(data_identifier, str):
            content = data_identifier
        elif isinstance(data_identifier, dict):
            # å°†å­—å…¸è½¬æ¢ä¸ºæ’åºçš„å­—ç¬¦ä¸²è¡¨ç¤º
            content = str(sorted(data_identifier.items()))
        elif isinstance(data_identifier, (np.ndarray, pd.DataFrame)):
            # å¯¹äºæ•°æ®æ•°ç»„ï¼Œä½¿ç”¨å½¢çŠ¶ã€æ•°æ®ç±»å‹å’Œéƒ¨åˆ†æ•°æ®ç”Ÿæˆå“ˆå¸Œ
            if isinstance(data_identifier, pd.DataFrame):
                data_identifier = data_identifier.values

            shape_str = str(data_identifier.shape)
            dtype_str = str(data_identifier.dtype)
            # ä½¿ç”¨æ•°æ®çš„ç»Ÿè®¡ç‰¹å¾è€Œä¸æ˜¯åŸå§‹æ•°æ®æ¥ç”Ÿæˆé”®
            stats = {
                'mean': float(np.mean(data_identifier)),
                'std': float(np.std(data_identifier)),
                'min': float(np.min(data_identifier)),
                'max': float(np.max(data_identifier))
            }
            content = f"{shape_str}_{dtype_str}_{str(stats)}"
        else:
            content = str(data_identifier)

        # ç”ŸæˆSHA256å“ˆå¸Œ
        return hashlib.sha256(content.encode('utf-8')).hexdigest()[:32]

    def _calculate_data_hash(self, data: Any) -> str:
        """è®¡ç®—æ•°æ®çš„å“ˆå¸Œå€¼ï¼Œç”¨äºéªŒè¯æ•°æ®å®Œæ•´æ€§"""
        try:
            if isinstance(data, (np.ndarray, pd.DataFrame)):
                if isinstance(data, pd.DataFrame):
                    data_bytes = data.values.tobytes()
                else:
                    data_bytes = data.tobytes()
            else:
                data_bytes = pickle.dumps(data)

            return hashlib.md5(data_bytes).hexdigest()
        except:
            return "unknown"

    def _compress_data(self, data: Any) -> bytes:
        """å‹ç¼©æ•°æ®"""
        try:
            import gzip
            raw_data = pickle.dumps(data)

            if self.compression:
                compressed_data = gzip.compress(raw_data)
                # åªæœ‰å‹ç¼©æ¯”è¶…è¿‡10%æ‰ä½¿ç”¨å‹ç¼©æ•°æ®
                if len(compressed_data) < len(raw_data) * 0.9:
                    return compressed_data, True
                else:
                    return raw_data, False
            else:
                return raw_data, False
        except Exception as e:
            print(f"    æ•°æ®å‹ç¼©å¤±è´¥: {e}")
            return pickle.dumps(data), False

    def _decompress_data(self, data: bytes, is_compressed: bool) -> Any:
        """è§£å‹ç¼©æ•°æ®"""
        try:
            if is_compressed:
                import gzip
                raw_data = gzip.decompress(data)
            else:
                raw_data = data

            return pickle.loads(raw_data)
        except Exception as e:
            print(f"    æ•°æ®è§£å‹å¤±è´¥: {e}")
            raise

    def get(self, key: str) -> Optional[Any]:
        """
        è·å–ç¼“å­˜æ•°æ®

        Args:
            key: ç¼“å­˜é”®

        Returns:
            ç¼“å­˜çš„æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        with self.connection_lock:
            conn = sqlite3.connect(self.db_path)

            try:
                cursor = conn.execute(
                    "SELECT file_path, is_compressed, data_hash FROM cache_entries WHERE key = ?",
                    (key,)
                )
                result = cursor.fetchone()

                if result:
                    file_path, is_compressed, data_hash = result
                    file_path = Path(file_path)

                    if file_path.exists():
                        try:
                            # è¯»å–æ•°æ®
                            with open(file_path, 'rb') as f:
                                data_bytes = f.read()

                            data = self._decompress_data(data_bytes, is_compressed)

                            # éªŒè¯æ•°æ®å®Œæ•´æ€§
                            if data_hash and data_hash != "unknown":
                                current_hash = self._calculate_data_hash(data)
                                if current_hash != data_hash:
                                    print(f"    âš ï¸ æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥: {key}")
                                    self._remove_entry(conn, key, file_path)
                                    self.stats['misses'] += 1
                                    return None

                            # æ›´æ–°è®¿é—®ç»Ÿè®¡
                            conn.execute("""
                                UPDATE cache_entries
                                SET access_time = ?, hit_count = hit_count + 1
                                WHERE key = ?
                            """, (time.time(), key))

                            self.stats['hits'] += 1
                            self.stats['total_bytes_retrieved'] += len(data_bytes)

                            return data

                        except Exception as e:
                            print(f"    ç¼“å­˜è¯»å–å¤±è´¥ {key}: {e}")
                            self._remove_entry(conn, key, file_path)
                    else:
                        # æ¸…ç†å¤±æ•ˆæ¡ç›®
                        self._remove_entry(conn, key)

                self.stats['misses'] += 1
                return None

            finally:
                conn.commit()
                conn.close()

    def set(self, key: str, data: Any, metadata: Optional[Dict] = None) -> bool:
        """
        è®¾ç½®ç¼“å­˜æ•°æ®

        Args:
            key: ç¼“å­˜é”®
            data: è¦ç¼“å­˜çš„æ•°æ®
            metadata: é¢å¤–çš„å…ƒæ•°æ®

        Returns:
            æ˜¯å¦æˆåŠŸè®¾ç½®
        """
        try:
            # å‹ç¼©æ•°æ®
            data_bytes, is_compressed = self._compress_data(data)
            data_size = len(data_bytes)

            # æ£€æŸ¥å•ä¸ªæ•°æ®æ˜¯å¦è¿‡å¤§
            if data_size > self.max_size_bytes * 0.1:  # ä¸å…è®¸å•ä¸ªæ•°æ®è¶…è¿‡æ€»å®¹é‡çš„10%
                print(f"    æ•°æ®è¿‡å¤§æ— æ³•ç¼“å­˜: {data_size / 1024**2:.2f}MB (key: {key})")
                return False

            # è®¡ç®—æ•°æ®å“ˆå¸Œ
            data_hash = self._calculate_data_hash(data)

            # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
            file_path = self.cache_dir / f"{key}.pkl"

            # å†™å…¥æ–‡ä»¶
            with open(file_path, 'wb') as f:
                f.write(data_bytes)

            # æ›´æ–°æ•°æ®åº“
            with self.connection_lock:
                conn = sqlite3.connect(self.db_path)

                try:
                    current_time = time.time()
                    conn.execute("""
                        INSERT OR REPLACE INTO cache_entries
                        (key, file_path, size_bytes, access_time, creation_time,
                         data_hash, metadata, is_compressed)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        key, str(file_path), data_size, current_time, current_time,
                        data_hash, str(metadata) if metadata else None, is_compressed
                    ))

                    self.stats['sets'] += 1
                    self.stats['total_bytes_stored'] += data_size

                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†
                    if self._check_cleanup_needed(conn):
                        self._cleanup_cache(conn)

                    return True

                finally:
                    conn.commit()
                    conn.close()

        except Exception as e:
            print(f"    ç¼“å­˜è®¾ç½®å¤±è´¥ {key}: {e}")
            return False

    def _remove_entry(self, conn: sqlite3.Connection, key: str, file_path: Optional[Path] = None):
        """ä»ç¼“å­˜ä¸­åˆ é™¤æ¡ç›®"""
        if file_path:
            file_path.unlink(missing_ok=True)

        conn.execute("DELETE FROM cache_entries WHERE key = ?", (key,))
        self.stats['deletions'] += 1

    def _check_cleanup_needed(self, conn: sqlite3.Connection) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†ç¼“å­˜"""
        total_size = conn.execute(
            "SELECT COALESCE(SUM(size_bytes), 0) FROM cache_entries"
        ).fetchone()[0]

        return total_size > self.max_size_bytes * self.cleanup_threshold

    def _cleanup_cache(self, conn: sqlite3.Connection):
        """æ™ºèƒ½ç¼“å­˜æ¸…ç†"""
        print("    æ‰§è¡Œç¼“å­˜æ¸…ç†...")

        # è·å–å½“å‰ç¼“å­˜ç»Ÿè®¡
        current_size = conn.execute(
            "SELECT COALESCE(SUM(size_bytes), 0) FROM cache_entries"
        ).fetchone()[0]

        target_size = int(self.max_size_bytes * self.cleanup_threshold * 0.7)  # æ¸…ç†åˆ°70%
        bytes_to_clean = current_size - target_size

        if bytes_to_clean <= 0:
            return

        # æ¸…ç†ç­–ç•¥ï¼šä¼˜å…ˆåˆ é™¤è®¿é—®æ¬¡æ•°å°‘ä¸”æœ€æ—§çš„æ¡ç›®
        # ä½¿ç”¨åŠ æƒè¯„åˆ†ï¼šä½è®¿é—®æ¬¡æ•° + æ—§è®¿é—®æ—¶é—´ = é«˜åˆ é™¤ä¼˜å…ˆçº§
        entries_to_delete = conn.execute("""
            SELECT key, file_path, size_bytes
            FROM cache_entries
            ORDER BY
                (access_time * 0.3 + hit_count * 0.7) ASC,
                creation_time ASC
        """).fetchall()

        bytes_cleaned = 0
        cleaned_count = 0

        for key, file_path, size_bytes in entries_to_delete:
            if bytes_cleaned >= bytes_to_clean:
                break

            self._remove_entry(conn, key, Path(file_path))
            bytes_cleaned += size_bytes
            cleaned_count += 1

        self.stats['cleanup_operations'] += 1
        print(f"    ç¼“å­˜æ¸…ç†å®Œæˆ: åˆ é™¤{cleaned_count}ä¸ªæ¡ç›®, é‡Šæ”¾{bytes_cleaned/1024**2:.2f}MB")

    def clear(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        with self.connection_lock:
            conn = sqlite3.connect(self.db_path)

            try:
                # è·å–æ‰€æœ‰æ–‡ä»¶è·¯å¾„
                file_paths = conn.execute("SELECT file_path FROM cache_entries").fetchall()

                # åˆ é™¤æ‰€æœ‰æ–‡ä»¶
                for (file_path,) in file_paths:
                    Path(file_path).unlink(missing_ok=True)

                # æ¸…ç©ºæ•°æ®åº“
                conn.execute("DELETE FROM cache_entries")

                print("    ç¼“å­˜å·²æ¸…ç©º")

                # é‡ç½®ç»Ÿè®¡
                for key in self.stats:
                    if key.endswith('_operations') or key.endswith('deletions'):
                        continue  # ä¿ç•™æ“ä½œè®¡æ•°
                    self.stats[key] = 0

            finally:
                conn.commit()
                conn.close()

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self.connection_lock:
            conn = sqlite3.connect(self.db_path)

            try:
                db_stats = conn.execute("""
                    SELECT
                        COUNT(*) as entries,
                        COALESCE(SUM(size_bytes), 0) as total_bytes,
                        COALESCE(AVG(hit_count), 0) as avg_hits,
                        COALESCE(SUM(hit_count), 0) as total_hits,
                        COALESCE(MAX(access_time) - MIN(creation_time), 0) as age_range
                    FROM cache_entries
                """).fetchone()

                entries, total_bytes, avg_hits, total_hits, age_range = db_stats

                stats = {
                    'cache_entries': entries,
                    'total_size_mb': total_bytes / 1024**2,
                    'total_size_gb': total_bytes / 1024**3,
                    'capacity_usage_percent': (total_bytes / self.max_size_bytes) * 100,
                    'avg_hits_per_entry': avg_hits,
                    'total_hits': total_hits,
                    'cache_age_hours': age_range / 3600 if age_range else 0,
                    'hit_rate_percent': (self.stats['hits'] / max(self.stats['hits'] + self.stats['misses'], 1)) * 100,
                    **self.stats
                }

                return stats

            finally:
                conn.close()

    def get_detailed_info(self) -> List[Dict]:
        """è·å–è¯¦ç»†çš„ç¼“å­˜æ¡ç›®ä¿¡æ¯"""
        with self.connection_lock:
            conn = sqlite3.connect(self.db_path)

            try:
                entries = conn.execute("""
                    SELECT key, size_bytes, access_time, creation_time, hit_count, metadata
                    FROM cache_entries
                    ORDER BY access_time DESC
                """).fetchall()

                detailed_info = []
                current_time = time.time()

                for key, size_bytes, access_time, creation_time, hit_count, metadata in entries:
                    info = {
                        'key': key,
                        'size_mb': size_bytes / 1024**2,
                        'age_hours': (current_time - creation_time) / 3600,
                        'last_access_hours': (current_time - access_time) / 3600,
                        'hit_count': hit_count,
                        'metadata': metadata
                    }
                    detailed_info.append(info)

                return detailed_info

            finally:
                conn.close()

    def optimize_cache(self):
        """ä¼˜åŒ–ç¼“å­˜æ€§èƒ½"""
        print("  å¼€å§‹ç¼“å­˜ä¼˜åŒ–...")

        with self.connection_lock:
            conn = sqlite3.connect(self.db_path)

            try:
                # é‡å»ºç´¢å¼•
                conn.execute("REINDEX")

                # æ¸…ç†ç¢ç‰‡
                conn.execute("VACUUM")

                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                conn.execute("ANALYZE")

                print("  ç¼“å­˜ä¼˜åŒ–å®Œæˆ")

            finally:
                conn.close()


class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨ï¼Œæä¾›é«˜çº§ç¼“å­˜åŠŸèƒ½"""

    def __init__(self, cache_dir: str = "cache/features", max_size_gb: float = 2.0):
        self.cache = ProductionFeatureCache(cache_dir, max_size_gb)

    def cache_features(self, extractor_func, data, cache_key_base: str, **kwargs):
        """
        ç¼“å­˜ç‰¹å¾æå–ç»“æœçš„è£…é¥°å™¨é£æ ¼å‡½æ•°

        Args:
            extractor_func: ç‰¹å¾æå–å‡½æ•°
            data: è¾“å…¥æ•°æ®
            cache_key_base: ç¼“å­˜é”®åŸºç¡€
            **kwargs: ä¼ é€’ç»™ç‰¹å¾æå–å‡½æ•°çš„å‚æ•°

        Returns:
            ç‰¹å¾æå–ç»“æœ
        """
        # ç”Ÿæˆç¼“å­˜é”®
        key_components = {
            'base': cache_key_base,
            'data_shape': data.shape if hasattr(data, 'shape') else str(type(data)),
            'params': kwargs
        }
        cache_key = self.cache._generate_key(key_components)

        # å°è¯•ä»ç¼“å­˜è·å–
        print(f"    æ£€æŸ¥ç‰¹å¾ç¼“å­˜: {cache_key_base}")
        cached_result = self.cache.get(cache_key)

        if cached_result is not None:
            print(f"    âœ… ç¼“å­˜å‘½ä¸­: {cache_key_base}")
            return cached_result

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œç‰¹å¾æå–
        print(f"    âŒ ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œç‰¹å¾æå–: {cache_key_base}")

        start_time = time.time()
        result = extractor_func(data, **kwargs)
        extraction_time = time.time() - start_time

        # ç¼“å­˜ç»“æœ
        metadata = {
            'extraction_time': extraction_time,
            'extractor_func': extractor_func.__name__ if hasattr(extractor_func, '__name__') else str(extractor_func),
            'data_info': str(type(data))
        }

        success = self.cache.set(cache_key, result, metadata)
        if success:
            print(f"    ğŸ’¾ ç»“æœå·²ç¼“å­˜: {cache_key_base} (ç”¨æ—¶: {extraction_time:.2f}s)")
        else:
            print(f"    âš ï¸ ç¼“å­˜å¤±è´¥: {cache_key_base}")

        return result

    def print_cache_report(self):
        """æ‰“å°ç¼“å­˜æŠ¥å‘Š"""
        stats = self.cache.get_stats()

        print("\n=== ç¼“å­˜æ€§èƒ½æŠ¥å‘Š ===")
        print(f"ç¼“å­˜æ¡ç›®: {stats['cache_entries']}")
        print(f"ä½¿ç”¨å®¹é‡: {stats['total_size_mb']:.2f}MB ({stats['capacity_usage_percent']:.1f}%)")
        print(f"å‘½ä¸­ç‡: {stats['hit_rate_percent']:.1f}%")
        print(f"æ€»å‘½ä¸­: {stats['total_hits']}, å¹³å‡å‘½ä¸­/æ¡ç›®: {stats['avg_hits_per_entry']:.1f}")
        print(f"ç¼“å­˜å¹´é¾„: {stats['cache_age_hours']:.1f}å°æ—¶")
        print(f"æ¸…ç†æ“ä½œ: {stats['cleanup_operations']}æ¬¡")


# æµ‹è¯•ä»£ç 
if __name__ == '__main__':
    print("=== ç”Ÿäº§çº§ç‰¹å¾ç¼“å­˜æµ‹è¯• ===")

    # åˆ›å»ºæµ‹è¯•ç¼“å­˜
    cache_manager = CacheManager(cache_dir="test_cache", max_size_gb=0.1)  # 100MBæµ‹è¯•

    # æ¨¡æ‹Ÿç‰¹å¾æå–å‡½æ•°
    def dummy_feature_extractor(data, window_size=10, feature_type='basic'):
        """æ¨¡æ‹Ÿçš„ç‰¹å¾æå–å‡½æ•°"""
        print(f"        æ‰§è¡Œç‰¹å¾æå–: window_size={window_size}, feature_type={feature_type}")
        time.sleep(0.1)  # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´

        if isinstance(data, np.ndarray):
            # ç®€å•çš„ç‰¹å¾è®¡ç®—
            features = {
                'mean': np.mean(data, axis=0),
                'std': np.std(data, axis=0),
                'shape': data.shape
            }
        else:
            features = {'type': str(type(data)), 'value': str(data)[:100]}

        return features

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data_1 = np.random.randn(1000, 5)
    test_data_2 = np.random.randn(500, 10)

    print("\n--- ç¼“å­˜æ€§èƒ½æµ‹è¯• ---")

    # ç¬¬ä¸€æ¬¡æå–ï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
    print("ç¬¬ä¸€æ¬¡ç‰¹å¾æå–:")
    start_time = time.time()
    features_1 = cache_manager.cache_features(
        dummy_feature_extractor, test_data_1, "test_features_1",
        window_size=15, feature_type='advanced'
    )
    time_1 = time.time() - start_time
    print(f"ç”¨æ—¶: {time_1:.3f}ç§’")

    # ç¬¬äºŒæ¬¡æå–ç›¸åŒæ•°æ®ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
    print("\nç¬¬äºŒæ¬¡æå–ç›¸åŒæ•°æ®:")
    start_time = time.time()
    features_1_cached = cache_manager.cache_features(
        dummy_feature_extractor, test_data_1, "test_features_1",
        window_size=15, feature_type='advanced'
    )
    time_2 = time.time() - start_time
    print(f"ç”¨æ—¶: {time_2:.3f}ç§’")

    # éªŒè¯ç»“æœä¸€è‡´æ€§
    print(f"ç»“æœä¸€è‡´æ€§: {str(features_1) == str(features_1_cached)}")
    print(f"åŠ é€Ÿæ¯”: {time_1/time_2:.1f}x")

    # ç¬¬ä¸‰æ¬¡æå–ä¸åŒæ•°æ®
    print("\nç¬¬ä¸‰æ¬¡æå–ä¸åŒæ•°æ®:")
    features_2 = cache_manager.cache_features(
        dummy_feature_extractor, test_data_2, "test_features_2",
        window_size=10, feature_type='basic'
    )

    # æ‰“å°ç¼“å­˜æŠ¥å‘Š
    cache_manager.print_cache_report()

    # è·å–è¯¦ç»†ä¿¡æ¯
    print("\n--- ç¼“å­˜è¯¦ç»†ä¿¡æ¯ ---")
    detailed_info = cache_manager.cache.get_detailed_info()
    for info in detailed_info:
        print(f"é”®: {info['key'][:20]}... | å¤§å°: {info['size_mb']:.3f}MB | "
              f"å‘½ä¸­: {info['hit_count']}æ¬¡ | å¹´é¾„: {info['age_hours']:.1f}h")

    # æ¸…ç†æµ‹è¯•
    print("\n--- ç¼“å­˜æ¸…ç†æµ‹è¯• ---")
    cache_manager.cache.clear()
    print("ç¼“å­˜å·²æ¸…ç†")

    print("\n=== ç”Ÿäº§çº§ç‰¹å¾ç¼“å­˜æµ‹è¯•å®Œæˆ ===")