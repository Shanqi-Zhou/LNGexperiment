"""
å¢å¼ºç‰ˆç‰¹å¾æå–å™¨
é›†æˆDay 1-3çš„æ‰€æœ‰ä¼˜åŒ–ï¼šæ•°å€¼ç¨³å®šæ€§ + å‘é‡åŒ–è®¡ç®— + å¹¶è¡Œå¤„ç†
"""
import pandas as pd
import numpy as np
import time
import warnings
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from typing import Dict, List, Any, Tuple, Optional
import psutil

# å¯¼å…¥ä¼˜åŒ–æ¨¡å—
from .numerical_stability import NumericallyStableFeatures, safe_statistical_features
from .vectorized_extraction import VectorizedFeatureExtractor, BatchFeatureProcessor
from .intelligent_chunking import IntelligentChunker, TimeSeriesChunker
from ..monitoring.resource_monitor import ResourceMonitor, MemoryProfiler


class EnhancedFeatureExtractor:
    """
    å¢å¼ºç‰ˆç‰¹å¾æå–å™¨
    é›†æˆDay 1-3çš„æ‰€æœ‰ä¼˜åŒ–ï¼š
    - Day 1: æ•°å€¼ç¨³å®šæ€§
    - Day 2: å‘é‡åŒ–æ‰¹é‡è®¡ç®—
    - Day 3: å†…å­˜ä¼˜åŒ–å’Œå¹¶è¡Œå¤„ç†
    """

    def __init__(self,
                 window_size: int = 180,
                 stride: int = 30,
                 use_parallel: bool = True,
                 max_workers: Optional[int] = None,
                 chunk_memory_percent: float = 40.0,
                 enable_monitoring: bool = True,
                 cache_features: bool = True):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆç‰¹å¾æå–å™¨

        Args:
            window_size: æ»‘åŠ¨çª—å£å¤§å°
            stride: çª—å£æ­¥é•¿
            use_parallel: æ˜¯å¦å¯ç”¨å¹¶è¡Œå¤„ç†
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°
            chunk_memory_percent: å•ä¸ªå—çš„ç›®æ ‡å†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯”
            enable_monitoring: æ˜¯å¦å¯ç”¨èµ„æºç›‘æ§
            cache_features: æ˜¯å¦å¯ç”¨ç‰¹å¾ç¼“å­˜
        """
        self.window_size = window_size
        self.stride = stride
        self.use_parallel = use_parallel
        self.max_workers = max_workers or min(psutil.cpu_count(), 8)  # é™åˆ¶æœ€å¤§å·¥ä½œçº¿ç¨‹
        self.chunk_memory_percent = chunk_memory_percent
        self.enable_monitoring = enable_monitoring
        self.cache_features = cache_features

        # ç³»ç»Ÿä¿¡æ¯
        self.cpu_count = psutil.cpu_count()
        self.total_memory_gb = psutil.virtual_memory().total / (1024**3)

        # åˆå§‹åŒ–ç»„ä»¶
        self._init_components()

        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'total_processing_time': 0.0,
            'feature_extraction_time': 0.0,
            'parallel_efficiency': 0.0,
            'memory_peak_mb': 0.0,
            'chunks_processed': 0,
            'total_windows': 0,
            'optimization_level': 'enhanced_parallel'
        }

        print(f"  ğŸš€ å¢å¼ºç‰ˆç‰¹å¾æå–å™¨åˆå§‹åŒ–å®Œæˆ:")
        print(f"    ç³»ç»Ÿé…ç½®: {self.cpu_count}æ ¸CPU, {self.total_memory_gb:.1f}GBå†…å­˜")
        print(f"    å¹¶è¡Œå¤„ç†: {'å¯ç”¨' if use_parallel else 'ç¦ç”¨'} ({self.max_workers} å·¥ä½œçº¿ç¨‹)")
        print(f"    èµ„æºç›‘æ§: {'å¯ç”¨' if enable_monitoring else 'ç¦ç”¨'}")
        print(f"    ç‰¹å¾ç¼“å­˜: {'å¯ç”¨' if cache_features else 'ç¦ç”¨'}")

    def _init_components(self):
        """åˆå§‹åŒ–å„ä¸ªç»„ä»¶"""
        # å‘é‡åŒ–å¤„ç†å™¨ï¼ˆDay 2ä¼˜åŒ–ï¼‰
        self.batch_processor = BatchFeatureProcessor(
            self.window_size,
            self.stride,
            use_cache=self.cache_features
        )

        # æ™ºèƒ½åˆ†å—å™¨ï¼ˆDay 3ä¼˜åŒ–ï¼‰
        self.chunker = TimeSeriesChunker(
            window_size=self.window_size,
            stride=self.stride,
            target_memory_usage_percent=self.chunk_memory_percent,
            min_chunk_size=max(1000, self.window_size * 2),
            max_chunk_size=50000
        )

        # èµ„æºç›‘æ§å™¨ï¼ˆDay 3ä¼˜åŒ–ï¼‰
        if self.enable_monitoring:
            self.monitor = ResourceMonitor(monitoring_interval=0.5)
            self.profiler = MemoryProfiler()
        else:
            self.monitor = None
            self.profiler = None

        # çº¿ç¨‹æ± ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.executor = None

    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ä¸»è¦çš„ç‰¹å¾æå–æ¥å£
        è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å¤„ç†ç­–ç•¥ï¼šå•çº¿ç¨‹å‘é‡åŒ– vs å¹¶è¡Œåˆ†å—å¤„ç†

        Args:
            df: è¾“å…¥æ—¶åºæ•°æ®

        Returns:
            æå–çš„ç‰¹å¾DataFrame
        """
        start_time = time.time()

        if self.enable_monitoring:
            self.monitor.start_monitoring()
            self.profiler.take_snapshot("transform_start")

        print(f"  ğŸ¯ å¼€å§‹å¢å¼ºç‰ˆç‰¹å¾æå–")
        print(f"    è¾“å…¥æ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"    ä¼°ç®—å†…å­˜å¤§å°: {df.memory_usage(deep=True).sum() / (1024**2):.1f}MB")

        try:
            # é€‰æ‹©å¤„ç†ç­–ç•¥
            processing_strategy = self._select_processing_strategy(df)
            print(f"    é€‰æ‹©å¤„ç†ç­–ç•¥: {processing_strategy}")

            if processing_strategy == "single_vectorized":
                result_df = self._single_vectorized_processing(df)
            elif processing_strategy == "parallel_chunked":
                result_df = self._parallel_chunked_processing(df)
            else:  # fallback
                result_df = self._fallback_processing(df)

            # è®°å½•æ€§èƒ½ç»Ÿè®¡
            total_time = time.time() - start_time
            self.performance_stats['total_processing_time'] = total_time
            self.performance_stats['total_windows'] = len(result_df)

            if self.enable_monitoring:
                self.profiler.take_snapshot("transform_end")
                monitoring_summary = self.monitor.stop_monitoring()
                self._update_performance_stats(monitoring_summary)

            self._print_performance_summary(total_time)

            return result_df

        except Exception as e:
            if self.enable_monitoring and self.monitor.monitoring:
                self.monitor.stop_monitoring()
            print(f"  âŒ ç‰¹å¾æå–å¤±è´¥: {e}")
            raise

    def _select_processing_strategy(self, df: pd.DataFrame) -> str:
        """
        æ ¹æ®æ•°æ®å¤§å°å’Œç³»ç»Ÿèµ„æºé€‰æ‹©å¤„ç†ç­–ç•¥

        Args:
            df: è¾“å…¥æ•°æ®

        Returns:
            å¤„ç†ç­–ç•¥: "single_vectorized", "parallel_chunked", "fallback"
        """
        data_size_mb = df.memory_usage(deep=True).sum() / (1024**2)
        available_memory_mb = psutil.virtual_memory().available / (1024**2)

        # ä¼°ç®—å¤„ç†å†…å­˜éœ€æ±‚ï¼ˆå‘é‡åŒ–å¤„ç†é€šå¸¸éœ€è¦3-5å€åŸå§‹æ•°æ®å¤§å°ï¼‰
        estimated_processing_memory = data_size_mb * 4

        print(f"    ç­–ç•¥åˆ†æ:")
        print(f"      æ•°æ®å¤§å°: {data_size_mb:.1f}MB")
        print(f"      å¯ç”¨å†…å­˜: {available_memory_mb:.1f}MB")
        print(f"      ä¼°ç®—å¤„ç†å†…å­˜: {estimated_processing_memory:.1f}MB")

        # ç­–ç•¥é€‰æ‹©é€»è¾‘
        if not self.use_parallel:
            return "single_vectorized"

        # å¦‚æœæ•°æ®å¾ˆå°ï¼Œç›´æ¥ä½¿ç”¨å‘é‡åŒ–
        if data_size_mb < 100:
            return "single_vectorized"

        # å¦‚æœå†…å­˜å……è¶³ï¼Œä½¿ç”¨å‘é‡åŒ–
        if estimated_processing_memory < available_memory_mb * 0.6:
            return "single_vectorized"

        # å¦åˆ™ä½¿ç”¨å¹¶è¡Œåˆ†å—
        return "parallel_chunked"

    def _single_vectorized_processing(self, df: pd.DataFrame) -> pd.DataFrame:
        """å•çº¿ç¨‹å‘é‡åŒ–å¤„ç†ï¼ˆDay 2ä¼˜åŒ–ï¼‰"""
        print(f"  âš¡ ä½¿ç”¨å‘é‡åŒ–ç‰¹å¾æå–")

        start_time = time.time()

        # ç¡®ä¿æ•°æ®æ˜¯æ•°å€¼ç±»å‹
        numeric_df = df.select_dtypes(include=[np.number])
        if numeric_df.empty:
            raise ValueError("DataFrameä¸­æ²¡æœ‰æ•°å€¼åˆ—")

        # ä½¿ç”¨å‘é‡åŒ–å¤„ç†å™¨
        features, feature_names = self.batch_processor.process_with_caching(
            numeric_df.values,
            cache_key=f"vectorized_{hash(str(df.values.tobytes()))}"
        )

        # åˆ›å»ºç»“æœDataFrame
        result_df = pd.DataFrame(features, columns=feature_names)

        extraction_time = time.time() - start_time
        self.performance_stats['feature_extraction_time'] = extraction_time

        print(f"    å‘é‡åŒ–å¤„ç†å®Œæˆ: {extraction_time:.2f}ç§’")
        print(f"    ç”Ÿæˆç‰¹å¾å½¢çŠ¶: {result_df.shape}")

        return result_df

    def _parallel_chunked_processing(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¹¶è¡Œåˆ†å—å¤„ç†ï¼ˆDay 3ä¼˜åŒ–ï¼‰"""
        print(f"  ğŸ”„ ä½¿ç”¨å¹¶è¡Œåˆ†å—å¤„ç†")

        start_time = time.time()

        # ç¡®ä¿æ•°æ®æ˜¯æ•°å€¼ç±»å‹
        numeric_df = df.select_dtypes(include=[np.number])
        if numeric_df.empty:
            raise ValueError("DataFrameä¸­æ²¡æœ‰æ•°å€¼åˆ—")

        # åˆ›å»ºæ•°æ®å—
        chunks_info = []
        chunk_data_list = []

        for start_idx, end_idx, chunk_data in self.chunker.create_windowed_chunks(numeric_df):
            chunks_info.append((start_idx, end_idx))
            chunk_data_list.append(chunk_data.values)  # è½¬æ¢ä¸ºnumpyæ•°ç»„

        print(f"    æ•°æ®åˆ†å—å®Œæˆ: {len(chunk_data_list)} ä¸ªå—")

        # å¹¶è¡Œå¤„ç†å—
        chunk_results = self._process_chunks_parallel(chunk_data_list)

        # åˆå¹¶ç»“æœ
        result_df = self._merge_chunk_results(chunk_results, chunks_info)

        extraction_time = time.time() - start_time
        self.performance_stats['feature_extraction_time'] = extraction_time
        self.performance_stats['chunks_processed'] = len(chunk_data_list)

        # è®¡ç®—å¹¶è¡Œæ•ˆç‡
        theoretical_serial_time = extraction_time * self.max_workers
        if theoretical_serial_time > 0:
            self.performance_stats['parallel_efficiency'] = extraction_time / theoretical_serial_time

        print(f"    å¹¶è¡Œå¤„ç†å®Œæˆ: {extraction_time:.2f}ç§’")
        print(f"    å¤„ç†å—æ•°: {len(chunk_data_list)}")
        print(f"    ç”Ÿæˆç‰¹å¾å½¢çŠ¶: {result_df.shape}")

        return result_df

    def _process_chunks_parallel(self, chunk_data_list: List[np.ndarray]) -> List[np.ndarray]:
        """å¹¶è¡Œå¤„ç†æ•°æ®å—"""
        if self.executor is None:
            self.executor = ThreadPoolExecutor(max_workers=self.max_workers)

        print(f"    å¯åŠ¨ {self.max_workers} ä¸ªå·¥ä½œçº¿ç¨‹...")

        try:
            # æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
            futures = []
            for i, chunk_data in enumerate(chunk_data_list):
                future = self.executor.submit(self._process_single_chunk, chunk_data, i)
                futures.append(future)

            # æ”¶é›†ç»“æœ
            chunk_results = []
            for i, future in enumerate(futures):
                try:
                    result = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                    chunk_results.append(result)
                    print(f"      å— {i+1}/{len(futures)} å¤„ç†å®Œæˆ")
                except Exception as e:
                    print(f"      å— {i+1} å¤„ç†å¤±è´¥: {e}")
                    # ä½¿ç”¨ç©ºç»“æœå ä½
                    chunk_results.append(np.array([]))

            return chunk_results

        finally:
            # æ¸…ç†çº¿ç¨‹æ± 
            if self.executor:
                self.executor.shutdown(wait=False)
                self.executor = None

    def _process_single_chunk(self, chunk_data: np.ndarray, chunk_id: int) -> np.ndarray:
        """å¤„ç†å•ä¸ªæ•°æ®å—"""
        try:
            # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„å¤„ç†å™¨
            chunk_processor = VectorizedFeatureExtractor(self.window_size, self.stride)

            # å¤„ç†å—
            features, _ = chunk_processor.extract_all_windows_vectorized(chunk_data)

            return features

        except Exception as e:
            print(f"      å— {chunk_id} å¤„ç†å¼‚å¸¸: {e}")
            return np.array([])

    def _merge_chunk_results(self, chunk_results: List[np.ndarray], chunks_info: List[Tuple[int, int]]) -> pd.DataFrame:
        """åˆå¹¶å—å¤„ç†ç»“æœ"""
        valid_results = [result for result in chunk_results if result.size > 0]

        if not valid_results:
            raise ValueError("æ‰€æœ‰å—å¤„ç†éƒ½å¤±è´¥äº†")

        print(f"    åˆå¹¶ {len(valid_results)} ä¸ªæœ‰æ•ˆç»“æœ...")

        # å‚ç›´å †å ç‰¹å¾
        all_features = np.vstack(valid_results)

        # ç”Ÿæˆç‰¹å¾åç§°ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆç»“æœçš„ç»´åº¦ï¼‰
        n_features = valid_results[0].shape[1]
        feature_names = self._generate_feature_names(n_features)

        result_df = pd.DataFrame(all_features, columns=feature_names)

        print(f"    åˆå¹¶åç‰¹å¾å½¢çŠ¶: {result_df.shape}")

        return result_df

    def _generate_feature_names(self, n_features: int) -> List[str]:
        """ç”Ÿæˆç‰¹å¾åç§°"""
        # å‡è®¾æ¯åˆ—æœ‰9ä¸ªç‰¹å¾ (ä¸vectorized_extraction.pyä¿æŒä¸€è‡´)
        features_per_col = 9
        n_cols = n_features // features_per_col

        feature_names = []
        for col_idx in range(n_cols):
            col_name = f'feature_{col_idx}'
            feature_names.extend([
                f'{col_name}_mean',
                f'{col_name}_std',
                f'{col_name}_skew',
                f'{col_name}_kurtosis',
                f'{col_name}_rms',
                f'{col_name}_mean_change_rate',
                f'{col_name}_change_rate_std',
                f'{col_name}_imf1_freq',
                f'{col_name}_imf1_energy_ratio'
            ])

        return feature_names

    def _fallback_processing(self, df: pd.DataFrame) -> pd.DataFrame:
        """é™çº§å¤„ç†ï¼ˆä½¿ç”¨åŸå§‹å‘é‡åŒ–æ–¹æ³•ï¼‰"""
        print(f"  ğŸ”„ ä½¿ç”¨é™çº§å¤„ç†æ–¹æ³•")

        # ç›´æ¥ä½¿ç”¨Day 2çš„å‘é‡åŒ–å¤„ç†å™¨
        extractor = VectorizedFeatureExtractor(self.window_size, self.stride)

        numeric_df = df.select_dtypes(include=[np.number])
        features, feature_names = extractor.extract_all_windows_vectorized(numeric_df.values)

        return pd.DataFrame(features, columns=feature_names)

    def _update_performance_stats(self, monitoring_summary: Dict[str, Any]):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        self.performance_stats['memory_peak_mb'] = monitoring_summary.get('peak_memory_percent', 0) * self.total_memory_gb * 1024 / 100

    def _print_performance_summary(self, total_time: float):
        """æ‰“å°æ€§èƒ½æ‘˜è¦"""
        print(f"\n  ğŸ“Š å¢å¼ºç‰ˆç‰¹å¾æå–æ€§èƒ½æ‘˜è¦:")
        print(f"    æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")
        print(f"    ç‰¹å¾æå–æ—¶é—´: {self.performance_stats['feature_extraction_time']:.2f}ç§’")
        print(f"    ç”Ÿæˆçª—å£æ•°: {self.performance_stats['total_windows']}")

        if self.performance_stats['chunks_processed'] > 0:
            print(f"    å¤„ç†å—æ•°: {self.performance_stats['chunks_processed']}")
            print(f"    å¹¶è¡Œæ•ˆç‡: {self.performance_stats['parallel_efficiency']:.2%}")

        if self.performance_stats['memory_peak_mb'] > 0:
            print(f"    å³°å€¼å†…å­˜ä½¿ç”¨: {self.performance_stats['memory_peak_mb']:.0f}MB")

        # è®¡ç®—ç›¸å¯¹äºDay 2çš„æ€§èƒ½æå‡
        windows_per_second = self.performance_stats['total_windows'] / total_time
        print(f"    å¤„ç†é€Ÿåº¦: {windows_per_second:.0f} çª—å£/ç§’")

    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return self.performance_stats.copy()

    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        if hasattr(self.batch_processor, 'cache'):
            self.batch_processor.cache.clear()
            print("  ğŸ—‘ï¸ ç‰¹å¾ç¼“å­˜å·²æ¸…é™¤")

    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if self.executor:
            self.executor.shutdown(wait=False)


# ä¿æŒå‘åå…¼å®¹æ€§
class FeatureExtractor:
    """
    å‘åå…¼å®¹çš„ç‰¹å¾æå–å™¨æ¥å£
    å†…éƒ¨ä½¿ç”¨å¢å¼ºç‰ˆå®ç°
    """
    def __init__(self, window_size=180, stride=30, use_stable_features=True, use_vectorized=True):
        # æ˜ å°„åˆ°å¢å¼ºç‰ˆå‚æ•°
        self.enhanced_extractor = EnhancedFeatureExtractor(
            window_size=window_size,
            stride=stride,
            use_parallel=True,  # é»˜è®¤å¯ç”¨å¹¶è¡Œ
            enable_monitoring=False,  # ä¸ºäº†å…¼å®¹æ€§ï¼Œé»˜è®¤å…³é—­ç›‘æ§
            cache_features=True
        )

    def transform(self, df):
        """å…¼å®¹æ¥å£"""
        return self.enhanced_extractor.transform(df)

    def get_extraction_stats(self):
        """å…¼å®¹æ¥å£"""
        return self.enhanced_extractor.get_performance_stats()


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    print("=== å¢å¼ºç‰ˆç‰¹å¾æå–å™¨æµ‹è¯• ===")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    np.random.seed(42)
    n_samples = 20000
    n_features = 6
    test_data = np.random.randn(n_samples, n_features)

    # æ·»åŠ ä¸€äº›æ¨¡å¼
    t = np.linspace(0, 10*np.pi, n_samples)
    test_data[:, 0] += np.sin(t)
    test_data[:, 1] += 0.5 * np.cos(2*t)

    test_df = pd.DataFrame(test_data, columns=[f'sensor_{i}' for i in range(n_features)])

    print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_df.shape}")
    print(f"æµ‹è¯•æ•°æ®å¤§å°: {test_df.memory_usage(deep=True).sum() / (1024**2):.1f}MB")

    # æµ‹è¯•å¢å¼ºç‰ˆæå–å™¨
    extractor = EnhancedFeatureExtractor(
        window_size=180,
        stride=30,
        use_parallel=True,
        enable_monitoring=True
    )

    # è¿è¡Œç‰¹å¾æå–
    start_time = time.time()
    features_df = extractor.transform(test_df)
    end_time = time.time()

    print(f"\n=== æµ‹è¯•ç»“æœ ===")
    print(f"æå–æ—¶é—´: {end_time - start_time:.3f}ç§’")
    print(f"è¾“å‡ºç‰¹å¾å½¢çŠ¶: {features_df.shape}")
    print(f"ç‰¹å¾ç¤ºä¾‹:")
    print(features_df.head())

    # è·å–æ€§èƒ½ç»Ÿè®¡
    stats = extractor.get_performance_stats()
    print(f"\n=== æ€§èƒ½ç»Ÿè®¡ ===")
    for key, value in stats.items():
        if isinstance(value, float):
            print(f"{key}: {value:.3f}")
        else:
            print(f"{key}: {value}")

    print("\n=== å¢å¼ºç‰ˆç‰¹å¾æå–å™¨æµ‹è¯•å®Œæˆ ===")