"""
æ™ºèƒ½æ•°æ®åˆ†å—ç­–ç•¥
Day 3 ä¼˜åŒ–ï¼šä¸ºå¤§æ•°æ®é›†æä¾›å†…å­˜å‹å¥½çš„åˆ†å—å¤„ç†
"""
import numpy as np
import pandas as pd
import psutil
from typing import List, Tuple, Iterator, Any, Optional, Union
from pathlib import Path
import warnings


class IntelligentChunker:
    """æ™ºèƒ½æ•°æ®åˆ†å—å™¨ï¼ŒåŸºäºç³»ç»Ÿèµ„æºåŠ¨æ€è°ƒæ•´å—å¤§å°"""

    def __init__(self,
                 target_memory_usage_percent: float = 60.0,
                 min_chunk_size: int = 1000,
                 max_chunk_size: int = 100000,
                 overlap_ratio: float = 0.1):
        """
        åˆå§‹åŒ–æ™ºèƒ½åˆ†å—å™¨

        Args:
            target_memory_usage_percent: ç›®æ ‡å†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯”
            min_chunk_size: æœ€å°å—å¤§å°
            max_chunk_size: æœ€å¤§å—å¤§å°
            overlap_ratio: å—ä¹‹é—´çš„é‡å æ¯”ä¾‹ï¼ˆç”¨äºæ—¶åºæ•°æ®ï¼‰
        """
        self.target_memory_usage = target_memory_usage_percent
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
        self.overlap_ratio = overlap_ratio

        # ç³»ç»Ÿä¿¡æ¯
        self.total_memory_gb = psutil.virtual_memory().total / (1024**3)
        self.cpu_count = psutil.cpu_count()

        # åˆ†å—ç»Ÿè®¡
        self.chunking_stats = {
            'total_chunks': 0,
            'avg_chunk_size': 0,
            'memory_efficiency': 0.0,
            'processing_time': 0.0
        }

        print(f"  ğŸ“¦ æ™ºèƒ½åˆ†å—å™¨åˆå§‹åŒ–:")
        print(f"    ç³»ç»Ÿå†…å­˜: {self.total_memory_gb:.1f}GB")
        print(f"    CPUæ ¸å¿ƒæ•°: {self.cpu_count}")
        print(f"    ç›®æ ‡å†…å­˜ä½¿ç”¨ç‡: {self.target_memory_usage}%")

    def calculate_optimal_chunk_size(self,
                                   data_shape: Tuple[int, ...],
                                   dtype: np.dtype,
                                   processing_factor: float = 2.0) -> int:
        """
        è®¡ç®—æœ€ä¼˜åˆ†å—å¤§å°

        Args:
            data_shape: æ•°æ®å½¢çŠ¶
            dtype: æ•°æ®ç±»å‹
            processing_factor: å¤„ç†è¿‡ç¨‹ä¸­çš„å†…å­˜æ”¾å¤§å› å­

        Returns:
            æœ€ä¼˜åˆ†å—å¤§å°ï¼ˆè¡Œæ•°ï¼‰
        """
        # è®¡ç®—å•è¡Œå†…å­˜å ç”¨
        bytes_per_row = np.prod(data_shape[1:]) * dtype.itemsize

        # è·å–å¯ç”¨å†…å­˜
        available_memory = psutil.virtual_memory().available
        target_memory = available_memory * (self.target_memory_usage / 100.0)

        # è€ƒè™‘å¤„ç†è¿‡ç¨‹ä¸­çš„å†…å­˜æ”¾å¤§
        effective_memory = target_memory / processing_factor

        # è®¡ç®—ç†è®ºæœ€ä¼˜å—å¤§å°
        optimal_size = int(effective_memory / bytes_per_row)

        # åº”ç”¨çº¦æŸ
        chunk_size = max(self.min_chunk_size,
                        min(optimal_size, self.max_chunk_size))

        print(f"  ğŸ§® å—å¤§å°è®¡ç®—:")
        print(f"    æ•°æ®å½¢çŠ¶: {data_shape}")
        print(f"    å•è¡Œå†…å­˜: {bytes_per_row} bytes")
        print(f"    å¯ç”¨å†…å­˜: {available_memory/(1024**2):.0f}MB")
        print(f"    ç›®æ ‡å†…å­˜: {target_memory/(1024**2):.0f}MB")
        print(f"    è®¡ç®—å—å¤§å°: {chunk_size} è¡Œ")

        return chunk_size

    def create_chunks(self,
                     data: Union[np.ndarray, pd.DataFrame],
                     chunk_size: Optional[int] = None) -> Iterator[Tuple[int, int, Any]]:
        """
        åˆ›å»ºæ•°æ®å—è¿­ä»£å™¨

        Args:
            data: è¦åˆ†å—çš„æ•°æ®
            chunk_size: æ‰‹åŠ¨æŒ‡å®šå—å¤§å°ï¼Œä¸æŒ‡å®šåˆ™è‡ªåŠ¨è®¡ç®—

        Yields:
            (start_idx, end_idx, chunk_data)
        """
        if isinstance(data, pd.DataFrame):
            data_shape = data.shape
            dtype = data.dtypes.iloc[0] if len(data.dtypes) > 0 else np.float64
            total_rows = len(data)
        else:
            data_shape = data.shape
            dtype = data.dtype
            total_rows = data.shape[0]

        # è®¡ç®—å—å¤§å°
        if chunk_size is None:
            chunk_size = self.calculate_optimal_chunk_size(data_shape, dtype)

        # è®¡ç®—é‡å å¤§å°
        overlap_size = int(chunk_size * self.overlap_ratio)

        print(f"  ğŸ”„ åˆ›å»ºæ•°æ®å—:")
        print(f"    æ€»è¡Œæ•°: {total_rows}")
        print(f"    å—å¤§å°: {chunk_size}")
        print(f"    é‡å å¤§å°: {overlap_size}")

        chunk_count = 0
        start_idx = 0

        while start_idx < total_rows:
            # è®¡ç®—ç»“æŸç´¢å¼•
            end_idx = min(start_idx + chunk_size, total_rows)

            # æå–æ•°æ®å—
            if isinstance(data, pd.DataFrame):
                chunk_data = data.iloc[start_idx:end_idx].copy()
            else:
                chunk_data = data[start_idx:end_idx].copy()

            yield start_idx, end_idx, chunk_data

            chunk_count += 1

            # æ›´æ–°ä¸‹æ¬¡å¼€å§‹ä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
            if end_idx >= total_rows:
                break

            start_idx = end_idx - overlap_size

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.chunking_stats['total_chunks'] = chunk_count
        self.chunking_stats['avg_chunk_size'] = chunk_size

        print(f"  âœ… åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {chunk_count} ä¸ªå—")

    def create_balanced_chunks(self,
                             data: Union[np.ndarray, pd.DataFrame],
                             target_chunks: Optional[int] = None) -> List[Tuple[int, int]]:
        """
        åˆ›å»ºå‡è¡¡çš„æ•°æ®å—ç´¢å¼•ï¼ˆç”¨äºå¹¶è¡Œå¤„ç†ï¼‰

        Args:
            data: è¦åˆ†å—çš„æ•°æ®
            target_chunks: ç›®æ ‡å—æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°

        Returns:
            å—ç´¢å¼•åˆ—è¡¨ [(start, end), ...]
        """
        if target_chunks is None:
            target_chunks = self.cpu_count

        total_rows = len(data)
        base_chunk_size = total_rows // target_chunks
        remainder = total_rows % target_chunks

        chunks = []
        start_idx = 0

        for i in range(target_chunks):
            # å‰remainderä¸ªå—å¤šåˆ†é…ä¸€è¡Œ
            chunk_size = base_chunk_size + (1 if i < remainder else 0)
            end_idx = start_idx + chunk_size

            chunks.append((start_idx, min(end_idx, total_rows)))
            start_idx = end_idx

            if start_idx >= total_rows:
                break

        print(f"  âš–ï¸ å‡è¡¡åˆ†å—:")
        print(f"    ç›®æ ‡å—æ•°: {target_chunks}")
        print(f"    å®é™…å—æ•°: {len(chunks)}")
        print(f"    å—å¤§å°èŒƒå›´: {min(end-start for start, end in chunks)} - {max(end-start for start, end in chunks)}")

        return chunks

    def estimate_processing_memory(self,
                                 chunk_shape: Tuple[int, ...],
                                 dtype: np.dtype,
                                 window_size: int = 180,
                                 feature_multiplier: float = 8.0) -> float:
        """
        ä¼°ç®—å¤„ç†ç‰¹å®šå—æ—¶çš„å†…å­˜éœ€æ±‚

        Args:
            chunk_shape: æ•°æ®å—å½¢çŠ¶
            dtype: æ•°æ®ç±»å‹
            window_size: æ»‘åŠ¨çª—å£å¤§å°
            feature_multiplier: ç‰¹å¾æå–çš„å†…å­˜æ”¾å¤§å€æ•°

        Returns:
            ä¼°ç®—å†…å­˜éœ€æ±‚ï¼ˆMBï¼‰
        """
        # åŸå§‹æ•°æ®å†…å­˜
        raw_memory = np.prod(chunk_shape) * dtype.itemsize

        # æ»‘åŠ¨çª—å£å†…å­˜ï¼ˆéœ€è¦åˆ›å»ºçª—å£è§†å›¾ï¼‰
        window_memory = chunk_shape[0] * window_size * chunk_shape[1] * dtype.itemsize

        # ç‰¹å¾æå–å†…å­˜ï¼ˆé€šå¸¸æ˜¯åŸå§‹æ•°æ®çš„æ•°å€ï¼‰
        feature_memory = raw_memory * feature_multiplier

        # æ€»å†…å­˜éœ€æ±‚
        total_memory = (raw_memory + window_memory + feature_memory) / (1024**2)

        return total_memory

    def validate_chunk_memory(self,
                            chunk_shape: Tuple[int, ...],
                            dtype: np.dtype) -> bool:
        """
        éªŒè¯å—æ˜¯å¦åœ¨å†…å­˜é™åˆ¶å†…

        Args:
            chunk_shape: æ•°æ®å—å½¢çŠ¶
            dtype: æ•°æ®ç±»å‹

        Returns:
            æ˜¯å¦åœ¨å†…å­˜é™åˆ¶å†…
        """
        estimated_memory = self.estimate_processing_memory(chunk_shape, dtype)
        available_memory = psutil.virtual_memory().available / (1024**2)

        memory_ok = estimated_memory < (available_memory * 0.8)  # 80%å®‰å…¨è¾¹é™…

        if not memory_ok:
            print(f"  âš ï¸ å†…å­˜è­¦å‘Š:")
            print(f"    ä¼°ç®—éœ€æ±‚: {estimated_memory:.0f}MB")
            print(f"    å¯ç”¨å†…å­˜: {available_memory:.0f}MB")

        return memory_ok

    def adaptive_chunk_size(self,
                          data: Union[np.ndarray, pd.DataFrame],
                          processing_function: callable,
                          initial_chunk_size: Optional[int] = None) -> int:
        """
        è‡ªé€‚åº”è°ƒæ•´å—å¤§å°ï¼ŒåŸºäºå®é™…å¤„ç†æ€§èƒ½

        Args:
            data: æ•°æ®
            processing_function: å¤„ç†å‡½æ•°
            initial_chunk_size: åˆå§‹å—å¤§å°

        Returns:
            ä¼˜åŒ–åçš„å—å¤§å°
        """
        import time

        if isinstance(data, pd.DataFrame):
            data_shape = data.shape
            dtype = data.dtypes.iloc[0]
        else:
            data_shape = data.shape
            dtype = data.dtype

        # ä½¿ç”¨åˆå§‹å—å¤§å°æˆ–è®¡ç®—é»˜è®¤å€¼
        if initial_chunk_size is None:
            chunk_size = self.calculate_optimal_chunk_size(data_shape, dtype)
        else:
            chunk_size = initial_chunk_size

        print(f"  ğŸ”§ è‡ªé€‚åº”å—å¤§å°è°ƒæ•´:")
        print(f"    åˆå§‹å—å¤§å°: {chunk_size}")

        # æµ‹è¯•å‡ ä¸ªä¸åŒå¤§å°çš„å—
        test_sizes = [
            chunk_size // 2,
            chunk_size,
            min(chunk_size * 2, self.max_chunk_size)
        ]

        best_size = chunk_size
        best_efficiency = 0.0

        for test_size in test_sizes:
            if test_size < self.min_chunk_size:
                continue

            # åˆ›å»ºæµ‹è¯•å—
            test_chunk = data.iloc[:test_size] if isinstance(data, pd.DataFrame) else data[:test_size]

            # æ£€æŸ¥å†…å­˜çº¦æŸ
            if not self.validate_chunk_memory(test_chunk.shape, dtype):
                continue

            # æµ‹è¯•å¤„ç†æ€§èƒ½
            try:
                start_time = time.time()
                _ = processing_function(test_chunk)
                end_time = time.time()

                processing_time = end_time - start_time
                efficiency = test_size / processing_time  # è¡Œ/ç§’

                print(f"    æµ‹è¯•å—å¤§å° {test_size}: {efficiency:.0f} è¡Œ/ç§’")

                if efficiency > best_efficiency:
                    best_efficiency = efficiency
                    best_size = test_size

            except Exception as e:
                print(f"    æµ‹è¯•å—å¤§å° {test_size}: å¤„ç†å¤±è´¥ - {e}")
                continue

        print(f"    æœ€ä½³å—å¤§å°: {best_size} (æ•ˆç‡: {best_efficiency:.0f} è¡Œ/ç§’)")
        return best_size

    def get_chunking_stats(self) -> dict:
        """è·å–åˆ†å—ç»Ÿè®¡ä¿¡æ¯"""
        return self.chunking_stats.copy()


class TimeSeriesChunker(IntelligentChunker):
    """ä¸“é—¨ç”¨äºæ—¶åºæ•°æ®çš„åˆ†å—å™¨"""

    def __init__(self,
                 window_size: int = 180,
                 stride: int = 30,
                 **kwargs):
        """
        åˆå§‹åŒ–æ—¶åºæ•°æ®åˆ†å—å™¨

        Args:
            window_size: æ»‘åŠ¨çª—å£å¤§å°
            stride: çª—å£æ­¥é•¿
            **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™çˆ¶ç±»
        """
        super().__init__(**kwargs)
        self.window_size = window_size
        self.stride = stride

        # ç¡®ä¿é‡å è¶³å¤Ÿè¦†ç›–çª—å£éœ€æ±‚
        min_overlap = window_size * 1.5  # 1.5å€çª—å£å¤§å°çš„é‡å 
        self.overlap_ratio = max(self.overlap_ratio, min_overlap / self.min_chunk_size)

        print(f"  ğŸ“ˆ æ—¶åºåˆ†å—å™¨é…ç½®:")
        print(f"    æ»‘åŠ¨çª—å£: {self.window_size}")
        print(f"    æ­¥é•¿: {self.stride}")
        print(f"    é‡å æ¯”ä¾‹: {self.overlap_ratio:.3f}")

    def create_windowed_chunks(self,
                             data: Union[np.ndarray, pd.DataFrame],
                             ensure_complete_windows: bool = True) -> Iterator[Tuple[int, int, Any]]:
        """
        åˆ›å»ºè€ƒè™‘æ»‘åŠ¨çª—å£çš„æ—¶åºæ•°æ®å—

        Args:
            data: æ—¶åºæ•°æ®
            ensure_complete_windows: æ˜¯å¦ç¡®ä¿æ¯ä¸ªå—éƒ½èƒ½ç”Ÿæˆå®Œæ•´çš„çª—å£

        Yields:
            (start_idx, end_idx, chunk_data)
        """
        total_rows = len(data)

        # è®¡ç®—åŸºç¡€å—å¤§å°
        if isinstance(data, pd.DataFrame):
            chunk_size = self.calculate_optimal_chunk_size(data.shape, data.dtypes.iloc[0])
        else:
            chunk_size = self.calculate_optimal_chunk_size(data.shape, data.dtype)

        # å¦‚æœéœ€è¦å®Œæ•´çª—å£ï¼Œè°ƒæ•´å—å¤§å°
        if ensure_complete_windows:
            # ç¡®ä¿å—å¤§å°è‡³å°‘èƒ½ç”Ÿæˆä¸€äº›çª—å£
            min_size_for_windows = self.window_size + self.stride * 2
            chunk_size = max(chunk_size, min_size_for_windows)

        print(f"  ğŸªŸ æ—¶åºåˆ†å—:")
        print(f"    è°ƒæ•´åå—å¤§å°: {chunk_size}")
        print(f"    ç¡®ä¿å®Œæ•´çª—å£: {ensure_complete_windows}")

        # åˆ›å»ºé‡å å—
        overlap_size = max(self.window_size, int(chunk_size * self.overlap_ratio))

        chunk_count = 0
        start_idx = 0

        while start_idx < total_rows:
            # è®¡ç®—ç»“æŸç´¢å¼•
            end_idx = min(start_idx + chunk_size, total_rows)

            # æ£€æŸ¥æ˜¯å¦èƒ½ç”Ÿæˆè¶³å¤Ÿçš„çª—å£
            if ensure_complete_windows and (end_idx - start_idx) < self.window_size:
                print(f"    è·³è¿‡å— {chunk_count}: æ•°æ®ä¸è¶³ä»¥ç”Ÿæˆçª—å£")
                break

            # æå–æ•°æ®å—
            if isinstance(data, pd.DataFrame):
                chunk_data = data.iloc[start_idx:end_idx].copy()
            else:
                chunk_data = data[start_idx:end_idx].copy()

            yield start_idx, end_idx, chunk_data
            chunk_count += 1

            # æ›´æ–°ä¸‹æ¬¡å¼€å§‹ä½ç½®
            if end_idx >= total_rows:
                break

            start_idx = end_idx - overlap_size

        self.chunking_stats['total_chunks'] = chunk_count
        self.chunking_stats['avg_chunk_size'] = chunk_size

        print(f"  âœ… æ—¶åºåˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {chunk_count} ä¸ªé‡å å—")

    def estimate_window_count(self, chunk_size: int) -> int:
        """ä¼°ç®—å—èƒ½ç”Ÿæˆçš„çª—å£æ•°é‡"""
        if chunk_size < self.window_size:
            return 0
        return (chunk_size - self.window_size) // self.stride + 1

    def validate_temporal_consistency(self,
                                    chunk_results: List[Any],
                                    overlap_size: int) -> bool:
        """
        éªŒè¯é‡å å—ä¹‹é—´çš„æ—¶åºä¸€è‡´æ€§

        Args:
            chunk_results: å„å—çš„å¤„ç†ç»“æœ
            overlap_size: é‡å å¤§å°

        Returns:
            æ˜¯å¦æ—¶åºä¸€è‡´
        """
        if len(chunk_results) < 2:
            return True

        # ç®€å•çš„ä¸€è‡´æ€§æ£€æŸ¥ï¼šæ¯”è¾ƒé‡å åŒºåŸŸçš„ç‰¹å¾
        try:
            for i in range(len(chunk_results) - 1):
                current_result = chunk_results[i]
                next_result = chunk_results[i + 1]

                # è¿™é‡Œåº”è¯¥æ ¹æ®å…·ä½“çš„ç»“æœæ ¼å¼è¿›è¡Œä¸€è‡´æ€§æ£€æŸ¥
                # ç›®å‰åªåšåŸºæœ¬çš„å½¢çŠ¶æ£€æŸ¥
                if hasattr(current_result, 'shape') and hasattr(next_result, 'shape'):
                    if current_result.shape[1:] != next_result.shape[1:]:
                        return False

            return True
        except Exception:
            return False


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
if __name__ == '__main__':
    print("=== æ™ºèƒ½åˆ†å—å™¨æµ‹è¯• ===")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    np.random.seed(42)
    test_data = np.random.randn(50000, 8)  # 50kè¡Œï¼Œ8åˆ—
    test_df = pd.DataFrame(test_data, columns=[f'feature_{i}' for i in range(8)])

    print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_data.shape}")
    print(f"æµ‹è¯•æ•°æ®å¤§å°: {test_data.nbytes / (1024**2):.1f}MB")

    # æµ‹è¯•æ™®é€šåˆ†å—å™¨
    print("\n--- æµ‹è¯•æ™®é€šåˆ†å—å™¨ ---")
    chunker = IntelligentChunker(target_memory_usage_percent=50.0)

    chunk_count = 0
    total_processed = 0

    for start_idx, end_idx, chunk_data in chunker.create_chunks(test_df):
        chunk_count += 1
        total_processed += len(chunk_data)

        if chunk_count <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªå—çš„ä¿¡æ¯
            print(f"  å— {chunk_count}: è¡Œ {start_idx}-{end_idx}, å¤§å° {len(chunk_data)}")

    print(f"æ€»å…±å¤„ç† {chunk_count} ä¸ªå—ï¼Œ{total_processed} è¡Œæ•°æ®")

    # æµ‹è¯•æ—¶åºåˆ†å—å™¨
    print("\n--- æµ‹è¯•æ—¶åºåˆ†å—å™¨ ---")
    ts_chunker = TimeSeriesChunker(
        window_size=180,
        stride=30,
        target_memory_usage_percent=50.0
    )

    chunk_count = 0
    for start_idx, end_idx, chunk_data in ts_chunker.create_windowed_chunks(test_df):
        chunk_count += 1
        window_count = ts_chunker.estimate_window_count(len(chunk_data))

        if chunk_count <= 3:
            print(f"  æ—¶åºå— {chunk_count}: è¡Œ {start_idx}-{end_idx}, "
                  f"å¤§å° {len(chunk_data)}, å¯ç”Ÿæˆçª—å£ {window_count}")

    print(f"æ—¶åºåˆ†å—ç”Ÿæˆ {chunk_count} ä¸ªå—")

    # æµ‹è¯•å‡è¡¡åˆ†å—
    print("\n--- æµ‹è¯•å‡è¡¡åˆ†å— ---")
    balanced_chunks = chunker.create_balanced_chunks(test_df, target_chunks=4)
    for i, (start, end) in enumerate(balanced_chunks):
        print(f"  å‡è¡¡å— {i+1}: è¡Œ {start}-{end}, å¤§å° {end-start}")

    print("\n=== æ™ºèƒ½åˆ†å—å™¨æµ‹è¯•å®Œæˆ ===")