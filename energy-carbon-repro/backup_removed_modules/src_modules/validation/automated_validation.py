"""
è‡ªåŠ¨åŒ–éªŒè¯æ¡†æ¶ - Week 2 ä¼˜åŒ–
æä¾›å…¨é¢çš„æ¨¡å‹éªŒè¯ã€æ€§èƒ½æŒ‡æ ‡æ£€æŸ¥å’Œè´¨é‡ä¿è¯åŠŸèƒ½
"""

import numpy as np
import pandas as pd
import warnings
import time
from typing import Dict, Any, List, Tuple, Optional, Callable, Union
from sklearn.metrics import (
    r2_score, mean_squared_error, mean_absolute_error,
    mean_absolute_percentage_error
)
from sklearn.model_selection import cross_val_score, KFold
import psutil
from pathlib import Path
import json
from datetime import datetime


class AutomatedValidation:
    """
    è‡ªåŠ¨åŒ–éªŒè¯æ¡†æ¶
    æä¾›æ¨¡å‹æ€§èƒ½ã€æ•°å€¼ç¨³å®šæ€§ã€èµ„æºä½¿ç”¨çš„å…¨é¢éªŒè¯
    """

    def __init__(self, target_metrics: Dict[str, float] = None,
                 output_dir: str = "validation_results"):
        """
        åˆå§‹åŒ–è‡ªåŠ¨åŒ–éªŒè¯æ¡†æ¶

        Args:
            target_metrics: ç›®æ ‡æ€§èƒ½æŒ‡æ ‡
            output_dir: è¾“å‡ºç›®å½•
        """
        # é»˜è®¤ç›®æ ‡æŒ‡æ ‡ï¼ˆåŸºäºLNGé¡¹ç›®è¦æ±‚ï¼‰
        self.target_metrics = target_metrics or {
            'r2_min': 0.75,
            'cv_rmse_max': 0.06,  # 6%
            'nmbe_max': 0.006,    # 0.6%
            'mae_max_percent': 5.0,  # 5%
            'inference_time_per_sample_ms': 10.0  # 10ms per sample
        }

        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # éªŒè¯ç»“æœå­˜å‚¨
        self.validation_results = {}
        self.validation_history = []

        print(f"  è‡ªåŠ¨åŒ–éªŒè¯æ¡†æ¶åˆå§‹åŒ–")
        print(f"    ç›®æ ‡æŒ‡æ ‡: RÂ²â‰¥{self.target_metrics['r2_min']}, "
              f"CV(RMSE)â‰¤{self.target_metrics['cv_rmse_max']:.1%}, "
              f"|NMBE|â‰¤{self.target_metrics['nmbe_max']:.1%}")

    def run_full_validation(self, model, X_test: np.ndarray, y_test: np.ndarray,
                          X_train: np.ndarray = None, y_train: np.ndarray = None,
                          validation_name: str = "default") -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´éªŒè¯æµç¨‹

        Args:
            model: è®­ç»ƒå¥½çš„æ¨¡å‹
            X_test: æµ‹è¯•ç‰¹å¾
            y_test: æµ‹è¯•ç›®æ ‡
            X_train: è®­ç»ƒç‰¹å¾ï¼ˆç”¨äºäº¤å‰éªŒè¯ï¼‰
            y_train: è®­ç»ƒç›®æ ‡ï¼ˆç”¨äºäº¤å‰éªŒè¯ï¼‰
            validation_name: éªŒè¯åç§°

        Returns:
            å®Œæ•´çš„éªŒè¯ç»“æœ
        """
        print(f"=== å¼€å§‹è‡ªåŠ¨åŒ–éªŒè¯: {validation_name} ===")
        validation_start = time.time()

        # 1. æ€§èƒ½æŒ‡æ ‡éªŒè¯
        print("\n--- æ€§èƒ½æŒ‡æ ‡éªŒè¯ ---")
        performance_results = self._validate_performance_metrics(model, X_test, y_test)

        # 2. æ•°å€¼ç¨³å®šæ€§éªŒè¯
        print("\n--- æ•°å€¼ç¨³å®šæ€§éªŒè¯ ---")
        stability_results = self._validate_numerical_stability(model, X_test)

        # 3. èµ„æºä½¿ç”¨éªŒè¯
        print("\n--- èµ„æºä½¿ç”¨éªŒè¯ ---")
        resource_results = self._validate_resource_usage(model, X_test)

        # 4. äº¤å‰éªŒè¯ï¼ˆå¦‚æœæœ‰è®­ç»ƒæ•°æ®ï¼‰
        cross_validation_results = {}
        if X_train is not None and y_train is not None:
            print("\n--- äº¤å‰éªŒè¯ ---")
            cross_validation_results = self._perform_cross_validation(model, X_train, y_train)

        # 5. é¢„æµ‹ä¸€è‡´æ€§éªŒè¯
        print("\n--- é¢„æµ‹ä¸€è‡´æ€§éªŒè¯ ---")
        consistency_results = self._validate_prediction_consistency(model, X_test)

        # 6. è¾¹ç•Œæƒ…å†µéªŒè¯
        print("\n--- è¾¹ç•Œæƒ…å†µéªŒè¯ ---")
        edge_case_results = self._validate_edge_cases(model, X_test, y_test)

        # 7. ç”Ÿæˆç»¼åˆéªŒè¯ç»“æœ
        total_validation_time = time.time() - validation_start

        comprehensive_results = {
            'validation_name': validation_name,
            'timestamp': datetime.now().isoformat(),
            'total_validation_time': total_validation_time,
            'performance_metrics': performance_results,
            'numerical_stability': stability_results,
            'resource_usage': resource_results,
            'cross_validation': cross_validation_results,
            'prediction_consistency': consistency_results,
            'edge_cases': edge_case_results,
            'overall_assessment': self._generate_overall_assessment({
                'performance_metrics': performance_results,
                'numerical_stability': stability_results,
                'resource_usage': resource_results,
                'cross_validation': cross_validation_results,
                'prediction_consistency': consistency_results,
                'edge_cases': edge_case_results
            })
        }

        # 8. ä¿å­˜éªŒè¯ç»“æœ
        self._save_validation_results(comprehensive_results, validation_name)

        # 9. ç”ŸæˆéªŒè¯æŠ¥å‘Š
        report = self._generate_validation_report(comprehensive_results)
        print(report)

        # 10. æ›´æ–°éªŒè¯å†å²
        self.validation_results[validation_name] = comprehensive_results
        self.validation_history.append(comprehensive_results)

        print(f"\n=== éªŒè¯å®Œæˆï¼Œæ€»ç”¨æ—¶: {total_validation_time:.2f}ç§’ ===")

        return comprehensive_results

    def _validate_performance_metrics(self, model, X_test: np.ndarray,
                                    y_test: np.ndarray) -> Dict[str, Any]:
        """éªŒè¯æ€§èƒ½æŒ‡æ ‡"""
        try:
            # è¿›è¡Œé¢„æµ‹
            y_pred = model.predict(X_test)

            # åŸºç¡€æŒ‡æ ‡
            r2 = r2_score(y_test, y_pred)
            mse = mean_squared_error(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            rmse = np.sqrt(mse)

            # ç›¸å¯¹æŒ‡æ ‡
            y_mean = np.mean(y_test)
            cv_rmse = rmse / abs(y_mean) if abs(y_mean) > 1e-10 else float('inf')
            nmbe = np.mean(y_pred - y_test) / y_mean if abs(y_mean) > 1e-10 else 0
            mape = mean_absolute_percentage_error(y_test, y_pred)

            # ç›®æ ‡ä¸€è‡´æ€§æŒ‡æ ‡
            residuals = y_pred - y_test
            residuals_std = np.std(residuals)
            residuals_normalized = residuals / (abs(y_mean) + 1e-10)

            results = {
                'r2': r2,
                'mse': mse,
                'rmse': rmse,
                'mae': mae,
                'cv_rmse': cv_rmse,
                'nmbe': nmbe,
                'mape': mape,
                'residuals_std': residuals_std,
                'residuals_mean': np.mean(residuals),
                'residuals_normalized_std': np.std(residuals_normalized),

                # ç›®æ ‡è¾¾æˆæƒ…å†µ
                'r2_passed': r2 >= self.target_metrics['r2_min'],
                'cv_rmse_passed': cv_rmse <= self.target_metrics['cv_rmse_max'],
                'nmbe_passed': abs(nmbe) <= self.target_metrics['nmbe_max'],
                'mape_passed': mape <= self.target_metrics['mae_max_percent'] / 100,

                # é¢„æµ‹è´¨é‡åˆ†æ
                'prediction_range': (float(np.min(y_pred)), float(np.max(y_pred))),
                'actual_range': (float(np.min(y_test)), float(np.max(y_test))),
                'prediction_std': float(np.std(y_pred)),
                'actual_std': float(np.std(y_test))
            }

            # è®¡ç®—æ€§èƒ½ç­‰çº§
            performance_score = self._calculate_performance_score(results)
            results['performance_score'] = performance_score
            results['performance_grade'] = self._get_performance_grade(performance_score)

            print(f"    RÂ²: {r2:.4f} {'âœ…' if results['r2_passed'] else 'âŒ'}")
            print(f"    CV(RMSE): {cv_rmse:.4f} ({cv_rmse:.1%}) {'âœ…' if results['cv_rmse_passed'] else 'âŒ'}")
            print(f"    NMBE: {nmbe:.4f} ({nmbe:.1%}) {'âœ…' if results['nmbe_passed'] else 'âŒ'}")
            print(f"    MAPE: {mape:.4f} ({mape:.1%}) {'âœ…' if results['mape_passed'] else 'âŒ'}")
            print(f"    æ€§èƒ½ç­‰çº§: {results['performance_grade']}")

            return results

        except Exception as e:
            print(f"    âŒ æ€§èƒ½æŒ‡æ ‡éªŒè¯å¤±è´¥: {e}")
            return {'error': str(e), 'failed': True}

    def _validate_numerical_stability(self, model, X_test: np.ndarray) -> Dict[str, Any]:
        """éªŒè¯æ•°å€¼ç¨³å®šæ€§"""
        try:
            results = {
                'has_nan_predictions': False,
                'has_inf_predictions': False,
                'prediction_variance_stability': 0.0,
                'repeated_prediction_consistency': True,
                'numerical_precision_loss': False
            }

            # å¤šæ¬¡é¢„æµ‹æ£€æŸ¥ç¨³å®šæ€§
            predictions_list = []
            n_repeats = 5

            for i in range(n_repeats):
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    pred = model.predict(X_test)
                    predictions_list.append(pred)

                # æ£€æŸ¥NaNå’ŒInf
                if np.any(np.isnan(pred)):
                    results['has_nan_predictions'] = True
                if np.any(np.isinf(pred)):
                    results['has_inf_predictions'] = True

            # è®¡ç®—å¤šæ¬¡é¢„æµ‹çš„ä¸€è‡´æ€§
            if len(predictions_list) > 1:
                pred_array = np.array(predictions_list)
                prediction_variance = np.var(pred_array, axis=0)
                max_variance = np.max(prediction_variance)
                mean_variance = np.mean(prediction_variance)

                results['prediction_variance_stability'] = float(mean_variance)
                results['max_prediction_variance'] = float(max_variance)
                results['repeated_prediction_consistency'] = max_variance < 1e-10

            # æ£€æŸ¥æ•°å€¼ç²¾åº¦
            large_input = X_test * 1e6
            small_input = X_test * 1e-6

            try:
                pred_large = model.predict(large_input)
                pred_small = model.predict(small_input)

                if np.any(np.isnan(pred_large)) or np.any(np.isnan(pred_small)):
                    results['numerical_precision_loss'] = True

            except Exception:
                results['numerical_precision_loss'] = True

            # æ•´ä½“æ•°å€¼ç¨³å®šæ€§è¯„ä¼°
            stability_issues = sum([
                results['has_nan_predictions'],
                results['has_inf_predictions'],
                not results['repeated_prediction_consistency'],
                results['numerical_precision_loss']
            ])

            results['numerical_stability_score'] = max(0, 100 - stability_issues * 25)
            results['numerical_stability_passed'] = stability_issues == 0

            print(f"    NaNé¢„æµ‹: {'âŒ å‘ç°' if results['has_nan_predictions'] else 'âœ… æ— '}")
            print(f"    Infé¢„æµ‹: {'âŒ å‘ç°' if results['has_inf_predictions'] else 'âœ… æ— '}")
            print(f"    é¢„æµ‹ä¸€è‡´æ€§: {'âŒ ä¸ç¨³å®š' if not results['repeated_prediction_consistency'] else 'âœ… ç¨³å®š'}")
            print(f"    æ•°å€¼ç²¾åº¦: {'âŒ æŸå¤±' if results['numerical_precision_loss'] else 'âœ… æ­£å¸¸'}")
            print(f"    ç¨³å®šæ€§è¯„åˆ†: {results['numerical_stability_score']}/100")

            return results

        except Exception as e:
            print(f"    âŒ æ•°å€¼ç¨³å®šæ€§éªŒè¯å¤±è´¥: {e}")
            return {'error': str(e), 'failed': True}

    def _validate_resource_usage(self, model, X_test: np.ndarray) -> Dict[str, Any]:
        """éªŒè¯èµ„æºä½¿ç”¨æƒ…å†µ"""
        try:
            # å†…å­˜ä½¿ç”¨æµ‹è¯•
            memory_before = psutil.Process().memory_info().rss / 1024**2

            # æ¨ç†æ—¶é—´æµ‹è¯•
            inference_times = []
            batch_sizes = [1, 10, 100, min(1000, len(X_test))]

            for batch_size in batch_sizes:
                if batch_size > len(X_test):
                    continue

                batch_data = X_test[:batch_size]

                # é¢„çƒ­
                _ = model.predict(batch_data)

                # æµ‹è¯•æ¨ç†æ—¶é—´
                start_time = time.time()
                _ = model.predict(batch_data)
                end_time = time.time()

                batch_time = end_time - start_time
                per_sample_time = batch_time / batch_size * 1000  # ms

                inference_times.append({
                    'batch_size': batch_size,
                    'total_time_ms': batch_time * 1000,
                    'per_sample_time_ms': per_sample_time
                })

            memory_after = psutil.Process().memory_info().rss / 1024**2
            memory_usage = memory_after - memory_before

            # å¤§æ‰¹é‡æ¨ç†æµ‹è¯•
            large_batch_size = min(len(X_test), 5000)
            start_time = time.time()
            cpu_before = psutil.cpu_percent()

            _ = model.predict(X_test[:large_batch_size])

            end_time = time.time()
            cpu_after = psutil.cpu_percent()

            large_batch_time = end_time - start_time
            large_batch_per_sample_ms = (large_batch_time / large_batch_size) * 1000

            results = {
                'memory_usage_mb': memory_usage,
                'inference_times': inference_times,
                'large_batch_inference_time_ms': large_batch_time * 1000,
                'large_batch_per_sample_ms': large_batch_per_sample_ms,
                'cpu_usage_change': cpu_after - cpu_before,

                # æ€§èƒ½è¦æ±‚éªŒè¯
                'inference_time_passed': large_batch_per_sample_ms <= self.target_metrics['inference_time_per_sample_ms'],
                'memory_efficient': memory_usage < 1000,  # < 1GB
                'cpu_efficient': (cpu_after - cpu_before) < 90,  # CPUå¢é•¿ < 90%

                # å¯æ‰©å±•æ€§è¯„ä¼°
                'scalability_score': self._calculate_scalability_score(inference_times)
            }

            print(f"    å†…å­˜ä½¿ç”¨: {memory_usage:.1f}MB")
            print(f"    å¤§æ‰¹é‡æ¨ç†: {large_batch_per_sample_ms:.2f}ms/æ ·æœ¬ "
                  f"{'âœ…' if results['inference_time_passed'] else 'âŒ'}")
            print(f"    CPUæ•ˆç‡: {'âœ… é«˜æ•ˆ' if results['cpu_efficient'] else 'âŒ ä½æ•ˆ'}")
            print(f"    å¯æ‰©å±•æ€§è¯„åˆ†: {results['scalability_score']}/100")

            return results

        except Exception as e:
            print(f"    âŒ èµ„æºä½¿ç”¨éªŒè¯å¤±è´¥: {e}")
            return {'error': str(e), 'failed': True}

    def _perform_cross_validation(self, model, X_train: np.ndarray,
                                y_train: np.ndarray, cv_folds: int = 5) -> Dict[str, Any]:
        """æ‰§è¡Œäº¤å‰éªŒè¯"""
        try:
            # KæŠ˜äº¤å‰éªŒè¯
            kfold = KFold(n_splits=cv_folds, shuffle=True, random_state=42)

            # æ‰§è¡Œäº¤å‰éªŒè¯
            cv_scores = cross_val_score(model, X_train, y_train,
                                      cv=kfold, scoring='r2')

            # æ‰‹åŠ¨è®¡ç®—æ›´è¯¦ç»†çš„äº¤å‰éªŒè¯æŒ‡æ ‡
            cv_detailed_results = []

            for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X_train)):
                X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]
                y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]

                # è®­ç»ƒæ¨¡å‹ï¼ˆæ³¨æ„ï¼šè¿™é‡Œå‡è®¾æ¨¡å‹å¯ä»¥é‡æ–°æ‹Ÿåˆï¼‰
                try:
                    fold_model = type(model)()  # åˆ›å»ºåŒç±»å‹çš„æ–°æ¨¡å‹
                    if hasattr(fold_model, 'fit'):
                        fold_model.fit(X_fold_train, y_fold_train)
                        y_pred = fold_model.predict(X_fold_val)
                    else:
                        # å¦‚æœæ¨¡å‹ä¸æ”¯æŒé‡æ–°è®­ç»ƒï¼Œä½¿ç”¨åŸæ¨¡å‹
                        y_pred = model.predict(X_fold_val)

                    fold_r2 = r2_score(y_fold_val, y_pred)
                    fold_rmse = np.sqrt(mean_squared_error(y_fold_val, y_pred))

                    cv_detailed_results.append({
                        'fold': fold_idx + 1,
                        'r2': fold_r2,
                        'rmse': fold_rmse,
                        'train_size': len(train_idx),
                        'val_size': len(val_idx)
                    })

                except Exception:
                    # å¦‚æœæ— æ³•é‡æ–°è®­ç»ƒï¼Œè·³è¿‡è¯¦ç»†åˆ†æ
                    pass

            results = {
                'cv_scores': cv_scores.tolist(),
                'cv_mean': float(np.mean(cv_scores)),
                'cv_std': float(np.std(cv_scores)),
                'cv_min': float(np.min(cv_scores)),
                'cv_max': float(np.max(cv_scores)),
                'detailed_results': cv_detailed_results,

                # äº¤å‰éªŒè¯è´¨é‡è¯„ä¼°
                'cv_consistency': float(np.std(cv_scores)) < 0.05,  # æ ‡å‡†å·®å°äº0.05
                'cv_performance_passed': float(np.mean(cv_scores)) >= self.target_metrics['r2_min'],
                'cv_stability_score': max(0, 100 - float(np.std(cv_scores)) * 1000)
            }

            print(f"    äº¤å‰éªŒè¯RÂ²: {results['cv_mean']:.4f} Â± {results['cv_std']:.4f}")
            print(f"    æ€§èƒ½ä¸€è‡´æ€§: {'âœ… ç¨³å®š' if results['cv_consistency'] else 'âŒ ä¸ç¨³å®š'}")
            print(f"    CVæ€§èƒ½è¦æ±‚: {'âœ… è¾¾æ ‡' if results['cv_performance_passed'] else 'âŒ ä¸è¾¾æ ‡'}")

            return results

        except Exception as e:
            print(f"    âŒ äº¤å‰éªŒè¯å¤±è´¥: {e}")
            return {'error': str(e), 'failed': True}

    def _validate_prediction_consistency(self, model, X_test: np.ndarray) -> Dict[str, Any]:
        """éªŒè¯é¢„æµ‹ä¸€è‡´æ€§"""
        try:
            # æµ‹è¯•è¾“å…¥é¡ºåºä¸€è‡´æ€§
            indices = np.arange(len(X_test))
            shuffled_indices = np.random.permutation(indices)

            pred_original = model.predict(X_test)
            pred_shuffled = model.predict(X_test[shuffled_indices])
            pred_reordered = pred_shuffled[np.argsort(shuffled_indices)]

            order_consistency = np.allclose(pred_original, pred_reordered, rtol=1e-10)

            # æµ‹è¯•æ•°æ®å‰¯æœ¬ä¸€è‡´æ€§
            X_copy = X_test.copy()
            pred_copy = model.predict(X_copy)
            copy_consistency = np.allclose(pred_original, pred_copy, rtol=1e-10)

            # æµ‹è¯•å­é›†é¢„æµ‹ä¸€è‡´æ€§
            subset_size = min(100, len(X_test))
            subset_indices = np.random.choice(len(X_test), subset_size, replace=False)

            pred_subset_standalone = model.predict(X_test[subset_indices])
            pred_subset_from_full = pred_original[subset_indices]

            subset_consistency = np.allclose(pred_subset_standalone, pred_subset_from_full, rtol=1e-10)

            results = {
                'order_consistency': order_consistency,
                'copy_consistency': copy_consistency,
                'subset_consistency': subset_consistency,
                'max_order_diff': float(np.max(np.abs(pred_original - pred_reordered))) if not order_consistency else 0.0,
                'max_copy_diff': float(np.max(np.abs(pred_original - pred_copy))) if not copy_consistency else 0.0,
                'consistency_score': sum([order_consistency, copy_consistency, subset_consistency]) / 3 * 100,
                'overall_consistency_passed': all([order_consistency, copy_consistency, subset_consistency])
            }

            print(f"    é¡ºåºä¸€è‡´æ€§: {'âœ…' if order_consistency else 'âŒ'}")
            print(f"    å‰¯æœ¬ä¸€è‡´æ€§: {'âœ…' if copy_consistency else 'âŒ'}")
            print(f"    å­é›†ä¸€è‡´æ€§: {'âœ…' if subset_consistency else 'âŒ'}")
            print(f"    ä¸€è‡´æ€§è¯„åˆ†: {results['consistency_score']:.1f}/100")

            return results

        except Exception as e:
            print(f"    âŒ é¢„æµ‹ä¸€è‡´æ€§éªŒè¯å¤±è´¥: {e}")
            return {'error': str(e), 'failed': True}

    def _validate_edge_cases(self, model, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, Any]:
        """éªŒè¯è¾¹ç•Œæƒ…å†µå¤„ç†"""
        try:
            results = {
                'zero_input_handling': True,
                'extreme_value_handling': True,
                'small_batch_handling': True,
                'single_sample_handling': True,
                'edge_case_errors': []
            }

            # æµ‹è¯•é›¶è¾“å…¥
            try:
                zero_input = np.zeros_like(X_test[:1])
                zero_pred = model.predict(zero_input)
                if np.any(np.isnan(zero_pred)) or np.any(np.isinf(zero_pred)):
                    results['zero_input_handling'] = False
                    results['edge_case_errors'].append("Zero input produces NaN/Inf")
            except Exception as e:
                results['zero_input_handling'] = False
                results['edge_case_errors'].append(f"Zero input error: {str(e)}")

            # æµ‹è¯•æå€¼è¾“å…¥
            try:
                extreme_input = X_test[:1] * 1000
                extreme_pred = model.predict(extreme_input)
                if np.any(np.isnan(extreme_pred)) or np.any(np.isinf(extreme_pred)):
                    results['extreme_value_handling'] = False
                    results['edge_case_errors'].append("Extreme values produce NaN/Inf")
            except Exception as e:
                results['extreme_value_handling'] = False
                results['edge_case_errors'].append(f"Extreme value error: {str(e)}")

            # æµ‹è¯•å•æ ·æœ¬é¢„æµ‹
            try:
                single_pred = model.predict(X_test[:1])
                if len(single_pred) != 1:
                    results['single_sample_handling'] = False
                    results['edge_case_errors'].append("Single sample prediction shape error")
            except Exception as e:
                results['single_sample_handling'] = False
                results['edge_case_errors'].append(f"Single sample error: {str(e)}")

            # æµ‹è¯•å°æ‰¹é‡å¤„ç†
            try:
                small_batch_pred = model.predict(X_test[:3])
                if len(small_batch_pred) != 3:
                    results['small_batch_handling'] = False
                    results['edge_case_errors'].append("Small batch prediction shape error")
            except Exception as e:
                results['small_batch_handling'] = False
                results['edge_case_errors'].append(f"Small batch error: {str(e)}")

            # è®¡ç®—è¾¹ç•Œæƒ…å†µå¤„ç†è¯„åˆ†
            edge_cases_passed = sum([
                results['zero_input_handling'],
                results['extreme_value_handling'],
                results['small_batch_handling'],
                results['single_sample_handling']
            ])

            results['edge_case_score'] = (edge_cases_passed / 4) * 100
            results['edge_case_robustness_passed'] = edge_cases_passed >= 3

            print(f"    é›¶å€¼è¾“å…¥: {'âœ…' if results['zero_input_handling'] else 'âŒ'}")
            print(f"    æå€¼å¤„ç†: {'âœ…' if results['extreme_value_handling'] else 'âŒ'}")
            print(f"    å•æ ·æœ¬é¢„æµ‹: {'âœ…' if results['single_sample_handling'] else 'âŒ'}")
            print(f"    å°æ‰¹é‡å¤„ç†: {'âœ…' if results['small_batch_handling'] else 'âŒ'}")
            print(f"    é²æ£’æ€§è¯„åˆ†: {results['edge_case_score']:.1f}/100")

            if results['edge_case_errors']:
                print(f"    è¾¹ç•Œæƒ…å†µé”™è¯¯: {len(results['edge_case_errors'])}ä¸ª")

            return results

        except Exception as e:
            print(f"    âŒ è¾¹ç•Œæƒ…å†µéªŒè¯å¤±è´¥: {e}")
            return {'error': str(e), 'failed': True}

    def _calculate_performance_score(self, performance_results: Dict) -> float:
        """è®¡ç®—æ€§èƒ½ç»¼åˆè¯„åˆ†"""
        score = 0.0

        # RÂ²æƒé‡40%
        if 'r2' in performance_results:
            r2_score = min(100, max(0, performance_results['r2'] * 100))
            score += r2_score * 0.4

        # CV(RMSE)æƒé‡30%
        if 'cv_rmse' in performance_results:
            cv_rmse = performance_results['cv_rmse']
            cv_rmse_score = max(0, 100 - cv_rmse * 1000)  # è½¬æ¢ä¸º0-100åˆ†
            score += cv_rmse_score * 0.3

        # NMBEæƒé‡20%
        if 'nmbe' in performance_results:
            nmbe = abs(performance_results['nmbe'])
            nmbe_score = max(0, 100 - nmbe * 10000)  # è½¬æ¢ä¸º0-100åˆ†
            score += nmbe_score * 0.2

        # MAPEæƒé‡10%
        if 'mape' in performance_results:
            mape = performance_results['mape']
            mape_score = max(0, 100 - mape * 100)
            score += mape_score * 0.1

        return min(100, max(0, score))

    def _get_performance_grade(self, score: float) -> str:
        """æ ¹æ®è¯„åˆ†è·å–æ€§èƒ½ç­‰çº§"""
        if score >= 90:
            return "A+ (ä¼˜ç§€)"
        elif score >= 80:
            return "A (è‰¯å¥½)"
        elif score >= 70:
            return "B (æ»¡æ„)"
        elif score >= 60:
            return "C (ä¸€èˆ¬)"
        else:
            return "D (ä¸è¾¾æ ‡)"

    def _calculate_scalability_score(self, inference_times: List[Dict]) -> float:
        """è®¡ç®—å¯æ‰©å±•æ€§è¯„åˆ†"""
        if len(inference_times) < 2:
            return 50.0

        # åˆ†ææ¨ç†æ—¶é—´ä¸æ‰¹é‡å¤§å°çš„å…³ç³»
        batch_sizes = [t['batch_size'] for t in inference_times]
        per_sample_times = [t['per_sample_time_ms'] for t in inference_times]

        # ç†æƒ³æƒ…å†µä¸‹ï¼Œper_sample_timeåº”è¯¥éšbatch_sizeå¢å¤§è€Œå‡å°
        time_improvement_ratio = per_sample_times[0] / per_sample_times[-1] if per_sample_times[-1] > 0 else 1

        # è¯„åˆ†åŸºäºæ‰¹é‡å¤„ç†æ•ˆç‡
        if time_improvement_ratio >= 5:
            return 100.0
        elif time_improvement_ratio >= 3:
            return 80.0
        elif time_improvement_ratio >= 2:
            return 60.0
        elif time_improvement_ratio >= 1.5:
            return 40.0
        else:
            return 20.0

    def _generate_overall_assessment(self, all_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ€»ä½“è¯„ä¼°"""
        assessment = {
            'validation_passed': True,
            'critical_issues': [],
            'warnings': [],
            'recommendations': [],
            'overall_score': 0.0,
            'deployment_ready': False
        }

        # æ£€æŸ¥å„é¡¹éªŒè¯ç»“æœ
        scores = []

        # æ€§èƒ½æŒ‡æ ‡æƒé‡50%
        perf_results = all_results.get('performance_metrics', {})
        if not perf_results.get('failed', False):
            perf_score = perf_results.get('performance_score', 0)
            scores.append(('performance', perf_score, 0.5))

            # æ£€æŸ¥å…³é”®æ€§èƒ½æŒ‡æ ‡
            if not perf_results.get('r2_passed', False):
                assessment['critical_issues'].append(f"RÂ²æœªè¾¾æ ‡: {perf_results.get('r2', 0):.4f} < {self.target_metrics['r2_min']}")
                assessment['validation_passed'] = False

            if not perf_results.get('cv_rmse_passed', False):
                assessment['critical_issues'].append(f"CV(RMSE)æœªè¾¾æ ‡: {perf_results.get('cv_rmse', 0):.4f} > {self.target_metrics['cv_rmse_max']}")

        # æ•°å€¼ç¨³å®šæ€§æƒé‡20%
        stability_results = all_results.get('numerical_stability', {})
        if not stability_results.get('failed', False):
            stability_score = stability_results.get('numerical_stability_score', 0)
            scores.append(('stability', stability_score, 0.2))

            if not stability_results.get('numerical_stability_passed', False):
                assessment['critical_issues'].append("æ•°å€¼ç¨³å®šæ€§ä¸åˆæ ¼")
                assessment['validation_passed'] = False

        # èµ„æºä½¿ç”¨æƒé‡15%
        resource_results = all_results.get('resource_usage', {})
        if not resource_results.get('failed', False):
            # åŸºäºæ¨ç†æ—¶é—´å’Œå†…å­˜ä½¿ç”¨è®¡ç®—èµ„æºè¯„åˆ†
            resource_score = 100
            if not resource_results.get('inference_time_passed', True):
                resource_score -= 30
            if not resource_results.get('memory_efficient', True):
                resource_score -= 20
            if not resource_results.get('cpu_efficient', True):
                resource_score -= 20

            scores.append(('resource', resource_score, 0.15))

        # é¢„æµ‹ä¸€è‡´æ€§æƒé‡10%
        consistency_results = all_results.get('prediction_consistency', {})
        if not consistency_results.get('failed', False):
            consistency_score = consistency_results.get('consistency_score', 0)
            scores.append(('consistency', consistency_score, 0.1))

        # è¾¹ç•Œæƒ…å†µæƒé‡5%
        edge_case_results = all_results.get('edge_cases', {})
        if not edge_case_results.get('failed', False):
            edge_score = edge_case_results.get('edge_case_score', 0)
            scores.append(('edge_cases', edge_score, 0.05))

        # è®¡ç®—åŠ æƒæ€»åˆ†
        if scores:
            weighted_sum = sum(score * weight for name, score, weight in scores)
            total_weight = sum(weight for name, score, weight in scores)
            assessment['overall_score'] = weighted_sum / total_weight if total_weight > 0 else 0
        else:
            assessment['overall_score'] = 0

        # ç”Ÿæˆå»ºè®®
        if assessment['overall_score'] >= 85:
            assessment['deployment_ready'] = len(assessment['critical_issues']) == 0
            assessment['recommendations'].append("æ¨¡å‹æ€§èƒ½ä¼˜ç§€ï¼Œå»ºè®®éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ")
        elif assessment['overall_score'] >= 70:
            assessment['recommendations'].append("æ¨¡å‹æ€§èƒ½è‰¯å¥½ï¼Œå»ºè®®è¿›è¡Œå°è§„æ¨¡è¯•è¿è¡Œ")
        elif assessment['overall_score'] >= 60:
            assessment['recommendations'].append("æ¨¡å‹æ€§èƒ½ä¸€èˆ¬ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–åéƒ¨ç½²")
        else:
            assessment['recommendations'].append("æ¨¡å‹æ€§èƒ½ä¸è¾¾æ ‡ï¼Œä¸å»ºè®®éƒ¨ç½²ï¼Œéœ€è¦é‡æ–°ä¼˜åŒ–")

        # ç”Ÿæˆå…·ä½“æ”¹è¿›å»ºè®®
        if len(assessment['critical_issues']) > 0:
            assessment['recommendations'].append("ä¼˜å…ˆè§£å†³å…³é”®é—®é¢˜ï¼š" + "; ".join(assessment['critical_issues']))

        return assessment

    def _generate_validation_report(self, comprehensive_results: Dict[str, Any]) -> str:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        report_lines = [
            "=" * 80,
            f"è‡ªåŠ¨åŒ–éªŒè¯æŠ¥å‘Š - {comprehensive_results['validation_name']}",
            "=" * 80,
            f"éªŒè¯æ—¶é—´: {comprehensive_results['timestamp']}",
            f"æ€»éªŒè¯ç”¨æ—¶: {comprehensive_results['total_validation_time']:.2f}ç§’",
            "",
            "## éªŒè¯ç»“æœæ‘˜è¦",
            ""
        ]

        # æ€»ä½“è¯„ä¼°
        overall = comprehensive_results['overall_assessment']
        report_lines.extend([
            f"æ€»ä½“è¯„åˆ†: {overall['overall_score']:.1f}/100",
            f"éªŒè¯çŠ¶æ€: {'âœ… é€šè¿‡' if overall['validation_passed'] else 'âŒ æœªé€šè¿‡'}",
            f"éƒ¨ç½²å°±ç»ª: {'âœ… æ˜¯' if overall['deployment_ready'] else 'âŒ å¦'}",
            ""
        ])

        # è¯¦ç»†ç»“æœ
        perf = comprehensive_results.get('performance_metrics', {})
        if not perf.get('failed', False):
            report_lines.extend([
                "## ğŸ“Š æ€§èƒ½æŒ‡æ ‡",
                f"- RÂ²: {perf.get('r2', 0):.4f} {'âœ…' if perf.get('r2_passed', False) else 'âŒ'}",
                f"- CV(RMSE): {perf.get('cv_rmse', 0):.4f} ({perf.get('cv_rmse', 0):.1%}) {'âœ…' if perf.get('cv_rmse_passed', False) else 'âŒ'}",
                f"- NMBE: {perf.get('nmbe', 0):.4f} ({perf.get('nmbe', 0):.1%}) {'âœ…' if perf.get('nmbe_passed', False) else 'âŒ'}",
                f"- MAPE: {perf.get('mape', 0):.4f} ({perf.get('mape', 0):.1%}) {'âœ…' if perf.get('mape_passed', False) else 'âŒ'}",
                f"- æ€§èƒ½ç­‰çº§: {perf.get('performance_grade', 'N/A')}",
                ""
            ])

        stability = comprehensive_results.get('numerical_stability', {})
        if not stability.get('failed', False):
            report_lines.extend([
                "## ğŸ”¢ æ•°å€¼ç¨³å®šæ€§",
                f"- NaNé¢„æµ‹: {'âŒ å‘ç°' if stability.get('has_nan_predictions', False) else 'âœ… æ— '}",
                f"- Infé¢„æµ‹: {'âŒ å‘ç°' if stability.get('has_inf_predictions', False) else 'âœ… æ— '}",
                f"- é¢„æµ‹ä¸€è‡´æ€§: {'âœ… ç¨³å®š' if stability.get('repeated_prediction_consistency', True) else 'âŒ ä¸ç¨³å®š'}",
                f"- æ•°å€¼ç²¾åº¦: {'âœ… æ­£å¸¸' if not stability.get('numerical_precision_loss', False) else 'âŒ æŸå¤±'}",
                f"- ç¨³å®šæ€§è¯„åˆ†: {stability.get('numerical_stability_score', 0):.1f}/100",
                ""
            ])

        resource = comprehensive_results.get('resource_usage', {})
        if not resource.get('failed', False):
            report_lines.extend([
                "## ğŸ’» èµ„æºä½¿ç”¨",
                f"- å†…å­˜ä½¿ç”¨: {resource.get('memory_usage_mb', 0):.1f}MB",
                f"- æ¨ç†æ—¶é—´: {resource.get('large_batch_per_sample_ms', 0):.2f}ms/æ ·æœ¬ {'âœ…' if resource.get('inference_time_passed', False) else 'âŒ'}",
                f"- CPUæ•ˆç‡: {'âœ… é«˜æ•ˆ' if resource.get('cpu_efficient', True) else 'âŒ ä½æ•ˆ'}",
                f"- å¯æ‰©å±•æ€§è¯„åˆ†: {resource.get('scalability_score', 0):.1f}/100",
                ""
            ])

        # å…³é”®é—®é¢˜å’Œå»ºè®®
        if overall['critical_issues']:
            report_lines.extend([
                "## âš ï¸ å…³é”®é—®é¢˜",
                ""
            ])
            for issue in overall['critical_issues']:
                report_lines.append(f"- {issue}")
            report_lines.append("")

        if overall['recommendations']:
            report_lines.extend([
                "## ğŸ’¡ å»ºè®®",
                ""
            ])
            for rec in overall['recommendations']:
                report_lines.append(f"- {rec}")
            report_lines.append("")

        report_lines.extend([
            "=" * 80,
            f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "=" * 80
        ])

        return "\\n".join(report_lines)

    def _save_validation_results(self, results: Dict[str, Any], validation_name: str):
        """ä¿å­˜éªŒè¯ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"validation_{validation_name}_{timestamp}.json"
        filepath = self.output_dir / filename

        # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½å¯ä»¥JSONåºåˆ—åŒ–
        serializable_results = self._make_json_serializable(results)

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2)

        print(f"    éªŒè¯ç»“æœå·²ä¿å­˜: {filepath}")

    def _make_json_serializable(self, obj):
        """ä½¿å¯¹è±¡å¯JSONåºåˆ—åŒ–"""
        if isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(item) for item in obj]
        elif isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        else:
            return obj


# ä¾¿æ·å‡½æ•°
def validate_model(model, X_test: np.ndarray, y_test: np.ndarray,
                  X_train: np.ndarray = None, y_train: np.ndarray = None,
                  target_metrics: Dict[str, float] = None) -> Dict[str, Any]:
    """
    ä¾¿æ·çš„æ¨¡å‹éªŒè¯å‡½æ•°

    Args:
        model: å¾…éªŒè¯çš„æ¨¡å‹
        X_test: æµ‹è¯•ç‰¹å¾
        y_test: æµ‹è¯•ç›®æ ‡
        X_train: è®­ç»ƒç‰¹å¾ï¼ˆå¯é€‰ï¼‰
        y_train: è®­ç»ƒç›®æ ‡ï¼ˆå¯é€‰ï¼‰
        target_metrics: ç›®æ ‡æŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰

    Returns:
        éªŒè¯ç»“æœ
    """
    validator = AutomatedValidation(target_metrics=target_metrics)
    return validator.run_full_validation(
        model, X_test, y_test, X_train, y_train, "quick_validation"
    )


# æµ‹è¯•ä»£ç 
if __name__ == '__main__':
    print("=== è‡ªåŠ¨åŒ–éªŒè¯æ¡†æ¶æµ‹è¯• ===")

    # åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹
    class MockModel:
        def predict(self, X):
            # ç®€å•çš„çº¿æ€§æ¨¡å‹æ¨¡æ‹Ÿ
            return np.sum(X * [1, 2, -1, 0.5, -0.5, 1], axis=1) + np.random.randn(len(X)) * 0.1

    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    np.random.seed(42)
    n_samples = 1000
    n_features = 6

    X_test = np.random.randn(n_samples, n_features)
    y_test = np.sum(X_test * [1, 2, -1, 0.5, -0.5, 1], axis=1) + np.random.randn(n_samples) * 0.2

    X_train = np.random.randn(n_samples * 2, n_features)
    y_train = np.sum(X_train * [1, 2, -1, 0.5, -0.5, 1], axis=1) + np.random.randn(n_samples * 2) * 0.2

    print(f"æµ‹è¯•æ•°æ®: X_test{X_test.shape}, y_test{y_test.shape}")
    print(f"è®­ç»ƒæ•°æ®: X_train{X_train.shape}, y_train{y_train.shape}")

    # åˆ›å»ºæ¨¡å‹
    model = MockModel()

    # è¿è¡ŒéªŒè¯
    validation_results = validate_model(
        model, X_test, y_test, X_train, y_train
    )

    print("\\néªŒè¯å®Œæˆ!")
    print(f"æ€»ä½“è¯„åˆ†: {validation_results['overall_assessment']['overall_score']:.1f}/100")
    print(f"éªŒè¯é€šè¿‡: {validation_results['overall_assessment']['validation_passed']}")
    print(f"éƒ¨ç½²å°±ç»ª: {validation_results['overall_assessment']['deployment_ready']}")

    print("\\n=== è‡ªåŠ¨åŒ–éªŒè¯æ¡†æ¶æµ‹è¯•å®Œæˆ ===")