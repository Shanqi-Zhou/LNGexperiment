#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å†…å­˜ä¼˜åŒ–è·¨æ¨¡æ€éªŒè¯
Memory-Optimized Cross-Modal Validation

ä¸“æ³¨äºè·¨æ¨¡æ€èåˆæ¨¡å‹ï¼Œä½¿ç”¨åˆ†æ‰¹å¤„ç†é€‚é…RTX 4060 8GBæ˜¾å­˜
"""

import sys
import pandas as pd
import numpy as np
import torch
import logging
import time
import gc
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    """å†…å­˜ä¼˜åŒ–çš„è·¨æ¨¡æ€èåˆéªŒè¯"""
    logger.info("=" * 60)
    logger.info("LNGè·¨æ¨¡æ€èåˆæ¨¡å‹éªŒè¯ - RTX 4060å†…å­˜ä¼˜åŒ–")
    logger.info("=" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"GPUè®¾å¤‡: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")

    # GPUå†…å­˜ç®¡ç†
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        logger.info(f"GPUå†…å­˜æ¸…ç†å®Œæˆ")

    # 1. åŠ è½½å®Œæ•´ç°å®åŒ–æ•°æ®
    logger.info("\n--- é˜¶æ®µ 1: åŠ è½½ç°å®åŒ–æ•°æ® ---")
    df = pd.read_csv('data/sim_lng/enhanced_simulation_data.csv', parse_dates=['ts'])

    power_columns = ['booster_pump_power_kw', 'hp_pump_power_kw', 'bog_compressor_total_power_kw']
    df['energy'] = df[power_columns].sum(axis=1)

    logger.info(f"æ•°æ®é›†: {len(df):,} è¡Œ, èƒ½è€—CV: {df['energy'].std()/df['energy'].mean():.4f}")

    # 2. é«˜æ€§èƒ½ç‰¹å¾æå–
    logger.info("\n--- é˜¶æ®µ 2: é«˜æ€§èƒ½ç‰¹å¾æå– ---")
    from src.features.high_performance_engine import HighPerformanceFeatureEngine

    engine = HighPerformanceFeatureEngine(window_size=180, stride=30)
    feature_df = df.drop(columns=['ts', 'energy'])

    start_time = time.time()
    X_dynamic, X_static = engine.fast_extract_features(feature_df)
    feature_time = time.time() - start_time

    # ç”Ÿæˆæ ‡ç­¾
    y_list = []
    for i in range(0, len(df) - 180 + 1, 30):
        y_list.append(df['energy'].iloc[i:i+180].mean())
    y = np.array(y_list[:len(X_dynamic)])

    logger.info(f"ç‰¹å¾æå–å®Œæˆ: {feature_time:.2f}ç§’, é€Ÿåº¦: {len(X_dynamic)/feature_time:.1f} çª—å£/ç§’")
    logger.info(f"æ•°æ®å½¢çŠ¶: åŠ¨æ€{X_dynamic.shape}, é™æ€{X_static.shape}, æ ‡ç­¾{y.shape}")

    # 3. å†…å­˜ä¼˜åŒ–çš„è·¨æ¨¡æ€è®­ç»ƒ
    logger.info("\n--- é˜¶æ®µ 3: è·¨æ¨¡æ€èåˆè®­ç»ƒ (å†…å­˜ä¼˜åŒ–) ---")
    from src.models.cross_modal_fusion import CrossModalFusionWithResidual

    # è®­ç»ƒéªŒè¯åˆ†å‰²
    train_size = int(0.8 * len(X_dynamic))
    indices = np.random.permutation(len(X_dynamic))
    train_idx, val_idx = indices[:train_size], indices[train_size:]

    X_dynamic_val = X_dynamic[val_idx]
    X_static_val = X_static[val_idx]
    y_val = y[val_idx]

    # æ ‡å‡†åŒ–
    y_scaler = StandardScaler().fit(y[train_idx].reshape(-1, 1))
    y_val_s = y_scaler.transform(y_val.reshape(-1, 1)).flatten()

    # åˆ›å»ºæ¨¡å‹ (å†…å­˜ä¼˜åŒ–)
    model = CrossModalFusionWithResidual(
        dynamic_dim=X_dynamic.shape[-1],
        static_dim=X_static.shape[-1],
        seq_len=X_dynamic.shape[1],
        d_model=64,  # å‡å°æ¨¡å‹ç»´åº¦
        n_heads=4,
        n_layers=2   # å‡å°‘å±‚æ•°
    ).to(device)

    logger.info(f"æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")

    # åˆ†æ‰¹è®­ç»ƒ (å†…å­˜ä¼˜åŒ–)
    batch_size = 512  # é€‚åº”RTX 4060 8GB
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = torch.nn.MSELoss()

    logger.info(f"å¼€å§‹åˆ†æ‰¹è®­ç»ƒ (batch_size={batch_size})...")

    model.train()
    train_loss_total = 0
    n_batches = 0

    for epoch in range(20):
        epoch_loss = 0
        epoch_batches = 0

        # éšæœºæ‰“ä¹±è®­ç»ƒæ•°æ®
        train_indices = np.random.permutation(train_idx)

        for i in range(0, len(train_indices), batch_size):
            batch_indices = train_indices[i:i+batch_size]

            # è·å–æ‰¹æ¬¡æ•°æ®
            batch_dynamic = torch.FloatTensor(X_dynamic[batch_indices]).to(device)
            batch_static = torch.FloatTensor(X_static[batch_indices]).to(device)
            batch_y = y_scaler.transform(y[batch_indices].reshape(-1, 1)).flatten()
            batch_labels = torch.FloatTensor(batch_y).unsqueeze(1).to(device)

            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            pred, _ = model(batch_dynamic, batch_static)
            loss = criterion(pred, batch_labels)

            # åå‘ä¼ æ’­
            if torch.isfinite(loss):
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()

                epoch_loss += loss.item()
                epoch_batches += 1

            # æ¸…ç†GPUå†…å­˜
            del batch_dynamic, batch_static, batch_labels
            torch.cuda.empty_cache()

        if epoch_batches > 0:
            avg_loss = epoch_loss / epoch_batches
            if (epoch + 1) % 5 == 0:
                logger.info(f"  Epoch {epoch+1}/20, å¹³å‡Loss: {avg_loss:.6f}")

    # 4. åˆ†æ‰¹è¯„ä¼°
    logger.info("\n--- é˜¶æ®µ 4: è·¨æ¨¡æ€èåˆè¯„ä¼° ---")
    model.eval()
    predictions = []

    with torch.no_grad():
        for i in range(0, len(val_idx), batch_size):
            end_idx = min(i + batch_size, len(val_idx))
            batch_indices = val_idx[i:end_idx]

            val_dynamic = torch.FloatTensor(X_dynamic[batch_indices]).to(device)
            val_static = torch.FloatTensor(X_static[batch_indices]).to(device)

            pred, uncertainty = model(val_dynamic, val_static)
            predictions.append(pred.cpu().numpy())

            del val_dynamic, val_static
            torch.cuda.empty_cache()

    # åˆå¹¶é¢„æµ‹ç»“æœ
    y_pred_s = np.concatenate(predictions).flatten()
    y_pred_orig = y_scaler.inverse_transform(y_pred_s.reshape(-1, 1)).flatten()
    y_val_orig = y_scaler.inverse_transform(y_val_s.reshape(-1, 1)).flatten()

    # è®¡ç®—å­¦æœ¯æŒ‡æ ‡
    r2 = r2_score(y_val_orig, y_pred_orig)
    rmse = np.sqrt(mean_squared_error(y_val_orig, y_pred_orig))
    cv_rmse = rmse / np.mean(y_val_orig)
    nmbe = np.mean(y_pred_orig - y_val_orig) / np.mean(y_val_orig)

    # 5. ç»“æœæŠ¥å‘Š
    logger.info("\n" + "=" * 60)
    logger.info("è·¨æ¨¡æ€èåˆæ¨¡å‹éªŒè¯ç»“æœ")
    logger.info("=" * 60)

    logger.info(f"\nğŸ“Š æ•°æ®ç‰¹æ€§:")
    logger.info(f"  æ•°æ®è§„æ¨¡: {len(df):,} è¡Œ â†’ {len(X_dynamic):,} çª—å£")
    logger.info(f"  ç°å®åŒ–èƒ½è€—: å‡å€¼={np.mean(y_val_orig):.2f} kW, CV=0.5841")

    logger.info(f"\nğŸš€ æ€§èƒ½æŒ‡æ ‡:")
    logger.info(f"  ç‰¹å¾æå–: {feature_time:.2f}ç§’ ({len(X_dynamic)/feature_time:.1f} çª—å£/ç§’)")
    logger.info(f"  GPUè®­ç»ƒ: åˆ†æ‰¹ä¼˜åŒ–ï¼Œé€‚é…8GBæ˜¾å­˜")

    logger.info(f"\nğŸ“Š è·¨æ¨¡æ€èåˆå­¦æœ¯æŒ‡æ ‡:")
    logger.info(f"  RÂ² Score: {r2:.4f} {'âœ…' if r2 >= 0.75 else 'âŒ'} (ç›®æ ‡ â‰¥ 0.75)")
    logger.info(f"  CV(RMSE): {cv_rmse:.4f} {'âœ…' if cv_rmse <= 0.06 else 'âŒ'} (ç›®æ ‡ â‰¤ 0.06)")
    logger.info(f"  NMBE: {nmbe:.4f} {'âœ…' if abs(nmbe) <= 0.006 else 'âŒ'} (ç›®æ ‡ âˆˆ [-0.006, 0.006])")

    # è®ºæ–‡è¦æ±‚æ£€æŸ¥
    requirements_met = (r2 >= 0.75 and cv_rmse <= 0.06 and abs(nmbe) <= 0.006)

    if requirements_met:
        logger.info("\nğŸ‰ æ­å–œï¼è·¨æ¨¡æ€èåˆæ¨¡å‹è¾¾åˆ°è®ºæ–‡è¦æ±‚ï¼")
    else:
        logger.info("\nğŸ“ˆ è·¨æ¨¡æ€èåˆæ¨¡å‹åœ¨ç°å®æ•°æ®ä¸Šæ˜¾ç¤ºæŒ‘æˆ˜æ€§ï¼Œä¸ºè¿›ä¸€æ­¥ç ”ç©¶æä¾›æ–¹å‘")

    # ä¿å­˜ç»“æœ
    import yaml
    final_results = {
        'model': 'CrossModalFusion',
        'dataset': 'enhanced_simulation_data.csv',
        'data_size': len(df),
        'windows': len(X_dynamic),
        'performance': {
            'feature_extraction_time': feature_time,
            'processing_speed': len(X_dynamic)/feature_time
        },
        'academic_metrics': {
            'r2': float(r2),
            'cv_rmse': float(cv_rmse),
            'nmbe': float(nmbe)
        },
        'requirements_met': requirements_met,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    }

    results_path = Path("results") / f"cross_modal_final_{time.strftime('%Y%m%d_%H%M%S')}.yaml"
    results_path.parent.mkdir(exist_ok=True)

    with open(results_path, 'w', encoding='utf-8') as f:
        yaml.dump(final_results, f, allow_unicode=True)

    logger.info(f"\nç»“æœå·²ä¿å­˜: {results_path}")
    logger.info("\nğŸ¯ è·¨æ¨¡æ€èåˆæ¨¡å‹éªŒè¯å®Œæˆ")

    # æ¸…ç†GPUå†…å­˜
    torch.cuda.empty_cache()

if __name__ == "__main__":
    main()