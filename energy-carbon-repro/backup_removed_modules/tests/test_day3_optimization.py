"""
Day 3 ä¼˜åŒ–éªŒè¯æµ‹è¯•å¥—ä»¶
éªŒè¯å†…å­˜æ•ˆç‡ã€å¹¶è¡Œæ€§èƒ½å’Œç³»ç»Ÿç¨³å®šæ€§
"""
import numpy as np
import pandas as pd
import time
import unittest
import warnings
from typing import Dict, List, Tuple, Any
import psutil
import gc
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from features.parallel_processor import MemoryEfficientParallelProcessor, AdaptiveParallelProcessor
from features.optimized_extractor import OptimizedFeatureExtractor
from monitoring.resource_monitor import ResourceMonitor, ProcessResourceTracker


class Day3OptimizationValidator:
    """Day 3 ä¼˜åŒ–æ•ˆæœéªŒè¯å™¨"""

    def __init__(self):
        self.test_results = {}
        self.performance_baselines = {}

    def run_comprehensive_validation(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢éªŒè¯æµ‹è¯•"""
        print("=" * 60)
        print("Day 3 ä¼˜åŒ–éªŒè¯æµ‹è¯•å¼€å§‹")
        print("=" * 60)

        results = {
            'memory_efficiency': self._test_memory_efficiency(),
            'parallel_performance': self._test_parallel_performance(),
            'system_stability': self._test_system_stability(),
            'integration_quality': self._test_integration_quality(),
            'performance_targets': self._validate_performance_targets(),
            'overall_assessment': {}
        }

        # è®¡ç®—æ€»ä½“è¯„ä¼°
        results['overall_assessment'] = self._calculate_overall_assessment(results)

        return results

    def _test_memory_efficiency(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜æ•ˆç‡"""
        print("\n1. å†…å­˜æ•ˆç‡æµ‹è¯•...")

        test_cases = [
            {'size': (10000, 4), 'name': 'å°æ•°æ®é›†'},
            {'size': (50000, 8), 'name': 'ä¸­æ•°æ®é›†'},
            {'size': (100000, 12), 'name': 'å¤§æ•°æ®é›†'}
        ]

        memory_results = {}

        for case in test_cases:
            print(f"  æµ‹è¯•{case['name']}: {case['size']}")

            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            data = np.random.randn(*case['size']).astype(np.float32)
            data_size_mb = data.nbytes / 1024 / 1024

            # æµ‹è¯•å¹¶è¡Œå¤„ç†å™¨å†…å­˜ä½¿ç”¨
            with ProcessResourceTracker().context_manager() as tracker:
                processor = MemoryEfficientParallelProcessor(
                    window_size=180,
                    stride=30,
                    max_workers=4,
                    chunk_size_method='memory_aware'
                )

                gc.collect()  # æ¸…ç†å†…å­˜
                memory_before = psutil.Process().memory_info().rss / 1024 / 1024

                features, _ = processor.process_parallel(data)

                memory_after = psutil.Process().memory_info().rss / 1024 / 1024

            usage_stats = tracker.get_current_usage()

            # è®¡ç®—å†…å­˜æ•ˆç‡æŒ‡æ ‡
            memory_growth = memory_after - memory_before
            memory_efficiency = (data_size_mb / memory_growth) if memory_growth > 0 else float('inf')

            memory_results[case['name']] = {
                'data_size_mb': data_size_mb,
                'memory_growth_mb': memory_growth,
                'memory_efficiency_ratio': memory_efficiency,
                'peak_memory_mb': memory_after,
                'processing_time': usage_stats['duration_seconds'],
                'memory_per_second': memory_growth / usage_stats['duration_seconds'] if usage_stats['duration_seconds'] > 0 else 0
            }

            print(f"    æ•°æ®: {data_size_mb:.1f}MB, å†…å­˜å¢é•¿: {memory_growth:.1f}MB, æ•ˆç‡æ¯”: {memory_efficiency:.1f}")

        # å†…å­˜æ•ˆç‡è¯„ä¼°
        avg_efficiency = np.mean([r['memory_efficiency_ratio'] for r in memory_results.values() if r['memory_efficiency_ratio'] != float('inf')])

        assessment = {
            'test_cases': memory_results,
            'average_efficiency_ratio': avg_efficiency,
            'memory_leak_detected': any(r['memory_growth_mb'] > r['data_size_mb'] * 2 for r in memory_results.values()),
            'memory_efficiency_grade': 'A' if avg_efficiency > 2.0 else 'B' if avg_efficiency > 1.0 else 'C',
            'passed': avg_efficiency > 1.0 and not any(r['memory_growth_mb'] > r['data_size_mb'] * 3 for r in memory_results.values())
        }

        print(f"  å†…å­˜æ•ˆç‡æµ‹è¯•ç»“æœ: {'âœ… é€šè¿‡' if assessment['passed'] else 'âŒ å¤±è´¥'}")
        print(f"    å¹³å‡æ•ˆç‡æ¯”: {avg_efficiency:.2f}")
        print(f"    æ•ˆç‡ç­‰çº§: {assessment['memory_efficiency_grade']}")

        return assessment

    def _test_parallel_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶è¡Œæ€§èƒ½"""
        print("\n2. å¹¶è¡Œæ€§èƒ½æµ‹è¯•...")

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        np.random.seed(42)
        test_data = np.random.randn(40000, 6).astype(np.float32)

        parallel_results = {}

        # æµ‹è¯•ä¸åŒå¹¶è¡Œé…ç½®
        worker_configs = [1, 2, 4, 8]

        for workers in worker_configs:
            print(f"  æµ‹è¯• {workers} ä¸ªå·¥ä½œçº¿ç¨‹...")

            processor = MemoryEfficientParallelProcessor(
                window_size=180,
                stride=30,
                max_workers=workers,
                chunk_size_method='adaptive'
            )

            # å¤šæ¬¡è¿è¡Œå–å¹³å‡
            times = []
            for run in range(3):
                start_time = time.time()
                features, _ = processor.process_parallel(test_data)
                execution_time = time.time() - start_time
                times.append(execution_time)

            avg_time = np.mean(times)
            std_time = np.std(times)

            parallel_results[f'{workers}_workers'] = {
                'average_time': avg_time,
                'std_time': std_time,
                'consistency': (1 - std_time / avg_time) * 100,
                'performance_stats': processor.get_performance_stats()
            }

            print(f"    å¹³å‡æ—¶é—´: {avg_time:.2f}s Â± {std_time:.3f}s")

        # è®¡ç®—å¹¶è¡Œæ•ˆç‡
        baseline_time = parallel_results['1_workers']['average_time']

        for config, result in parallel_results.items():
            workers = int(config.split('_')[0])
            speedup = baseline_time / result['average_time']
            parallel_efficiency = (speedup / workers) * 100

            result['speedup'] = speedup
            result['parallel_efficiency'] = parallel_efficiency

        # æ‰¾åˆ°æœ€ä¼˜é…ç½®
        best_config = max(parallel_results.keys(),
                         key=lambda k: parallel_results[k]['speedup'])

        best_speedup = parallel_results[best_config]['speedup']
        best_efficiency = parallel_results[best_config]['parallel_efficiency']

        assessment = {
            'test_results': parallel_results,
            'best_configuration': best_config,
            'best_speedup': best_speedup,
            'best_efficiency': best_efficiency,
            'scalability_grade': 'A' if best_speedup > 3.0 else 'B' if best_speedup > 2.0 else 'C',
            'efficiency_grade': 'A' if best_efficiency > 80 else 'B' if best_efficiency > 60 else 'C',
            'passed': best_speedup >= 2.0 and best_efficiency >= 50.0
        }

        print(f"  å¹¶è¡Œæ€§èƒ½æµ‹è¯•ç»“æœ: {'âœ… é€šè¿‡' if assessment['passed'] else 'âŒ å¤±è´¥'}")
        print(f"    æœ€ä½³é…ç½®: {best_config}")
        print(f"    æœ€å¤§åŠ é€Ÿæ¯”: {best_speedup:.2f}x")
        print(f"    å¹¶è¡Œæ•ˆç‡: {best_efficiency:.1f}%")

        return assessment

    def _test_system_stability(self) -> Dict[str, Any]:
        """æµ‹è¯•ç³»ç»Ÿç¨³å®šæ€§"""
        print("\n3. ç³»ç»Ÿç¨³å®šæ€§æµ‹è¯•...")

        stability_results = {}

        # é•¿æ—¶é—´è¿è¡Œæµ‹è¯•
        print("  é•¿æ—¶é—´è¿è¡Œæµ‹è¯•...")
        test_data = np.random.randn(20000, 4).astype(np.float32)

        extractor = OptimizedFeatureExtractor(
            enable_parallel=True,
            enable_monitoring=True
        )

        run_times = []
        memory_usage = []

        # è¿ç»­è¿è¡Œ10æ¬¡
        for i in range(10):
            gc.collect()
            memory_before = psutil.Process().memory_info().rss / 1024 / 1024

            start_time = time.time()
            features, _ = extractor.extract_features(test_data)
            run_time = time.time() - start_time

            memory_after = psutil.Process().memory_info().rss / 1024 / 1024

            run_times.append(run_time)
            memory_usage.append(memory_after - memory_before)

            if (i + 1) % 3 == 0:
                print(f"    å®Œæˆ {i+1}/10 æ¬¡è¿è¡Œ")

        # ç¨³å®šæ€§æŒ‡æ ‡
        time_stability = (1 - np.std(run_times) / np.mean(run_times)) * 100
        memory_stability = (1 - np.std(memory_usage) / np.mean(memory_usage)) * 100 if np.mean(memory_usage) > 0 else 100

        # å†…å­˜æ³„æ¼æ£€æµ‹
        memory_trend = np.polyfit(range(len(memory_usage)), memory_usage, 1)[0]
        memory_leak_suspected = memory_trend > 5.0  # æ¯æ¬¡è¿è¡Œå†…å­˜å¢é•¿è¶…è¿‡5MB

        stability_results['long_running'] = {
            'run_times': run_times,
            'memory_usage': memory_usage,
            'time_stability_percent': time_stability,
            'memory_stability_percent': memory_stability,
            'memory_leak_suspected': memory_leak_suspected,
            'average_run_time': np.mean(run_times)
        }

        # é«˜è´Ÿè½½æµ‹è¯•
        print("  é«˜è´Ÿè½½æµ‹è¯•...")
        large_data = np.random.randn(80000, 8).astype(np.float32)

        try:
            with ProcessResourceTracker().context_manager() as tracker:
                start_time = time.time()
                features, _ = extractor.extract_features(large_data)
                processing_time = time.time() - start_time

            usage_stats = tracker.get_current_usage()
            high_load_success = True

        except Exception as e:
            print(f"    é«˜è´Ÿè½½æµ‹è¯•å¤±è´¥: {e}")
            high_load_success = False
            processing_time = 0
            usage_stats = {}

        stability_results['high_load'] = {
            'success': high_load_success,
            'processing_time': processing_time,
            'usage_stats': usage_stats
        }

        # æ€»ä½“ç¨³å®šæ€§è¯„ä¼°
        assessment = {
            'test_results': stability_results,
            'time_stability_grade': 'A' if time_stability > 90 else 'B' if time_stability > 80 else 'C',
            'memory_stability_grade': 'A' if memory_stability > 90 else 'B' if memory_stability > 80 else 'C',
            'no_memory_leaks': not memory_leak_suspected,
            'high_load_capable': high_load_success,
            'passed': time_stability > 80 and memory_stability > 80 and not memory_leak_suspected and high_load_success
        }

        print(f"  ç³»ç»Ÿç¨³å®šæ€§æµ‹è¯•ç»“æœ: {'âœ… é€šè¿‡' if assessment['passed'] else 'âŒ å¤±è´¥'}")
        print(f"    æ—¶é—´ç¨³å®šæ€§: {time_stability:.1f}%")
        print(f"    å†…å­˜ç¨³å®šæ€§: {memory_stability:.1f}%")
        print(f"    å†…å­˜æ³„æ¼: {'âŒ æœªæ£€æµ‹åˆ°' if not memory_leak_suspected else 'âš ï¸ å¯èƒ½å­˜åœ¨'}")
        print(f"    é«˜è´Ÿè½½å¤„ç†: {'âœ… æˆåŠŸ' if high_load_success else 'âŒ å¤±è´¥'}")

        return assessment

    def _test_integration_quality(self) -> Dict[str, Any]:
        """æµ‹è¯•é›†æˆè´¨é‡"""
        print("\n4. é›†æˆè´¨é‡æµ‹è¯•...")

        integration_results = {}

        # æµ‹è¯•Day 1-3ä¼˜åŒ–çš„é›†æˆ
        test_data = np.random.randn(25000, 5).astype(np.float32)

        extractor = OptimizedFeatureExtractor(
            enable_parallel=True,
            memory_efficient=True,
            enable_monitoring=True
        )

        # åŸºå‡†æµ‹è¯•æ‰€æœ‰å¤„ç†æ¨¡å¼
        benchmark_results = extractor.benchmark_processing_modes(test_data)

        # æ£€æŸ¥ä¸åŒæ¨¡å¼ç»“æœçš„ä¸€è‡´æ€§
        features_by_mode = {}
        for mode in benchmark_results.keys():
            if mode == 'vectorized_basic':
                from features.vectorized_extraction import VectorizedFeatureExtractor
                basic_extractor = VectorizedFeatureExtractor(180, 30)
                features, _ = basic_extractor.extract_all_windows_vectorized(test_data)
                features_by_mode[mode] = features
            # å…¶ä»–æ¨¡å¼çš„ç‰¹å¾å·²åœ¨åŸºå‡†æµ‹è¯•ä¸­è·å¾—

        # ç‰¹å¾ä¸€è‡´æ€§æ£€æŸ¥
        consistency_scores = {}
        base_features = features_by_mode.get('vectorized_basic')

        if base_features is not None:
            for mode, features in features_by_mode.items():
                if mode != 'vectorized_basic' and features is not None:
                    # è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦
                    correlation = np.corrcoef(base_features.flatten(), features.flatten())[0, 1]
                    mse = np.mean((base_features - features) ** 2)
                    consistency_scores[mode] = {
                        'correlation': correlation,
                        'mse': mse,
                        'consistent': correlation > 0.99 and mse < 1e-6
                    }

        integration_results['feature_consistency'] = consistency_scores
        integration_results['benchmark_results'] = benchmark_results

        # æ€§èƒ½æ”¹è¿›éªŒè¯
        baseline_time = benchmark_results.get('vectorized_basic', {}).get('processing_time', float('inf'))
        best_time = min(result.get('processing_time', float('inf'))
                       for result in benchmark_results.values())

        performance_improvement = ((baseline_time - best_time) / baseline_time) * 100 if baseline_time > 0 else 0

        integration_results['performance_improvement'] = performance_improvement

        # é›†æˆè´¨é‡è¯„ä¼°
        all_consistent = all(score.get('consistent', False)
                           for score in consistency_scores.values())

        assessment = {
            'test_results': integration_results,
            'feature_consistency_passed': all_consistent,
            'performance_improvement_percent': performance_improvement,
            'integration_grade': 'A' if all_consistent and performance_improvement > 25 else 'B' if all_consistent else 'C',
            'passed': all_consistent and performance_improvement > 20
        }

        print(f"  é›†æˆè´¨é‡æµ‹è¯•ç»“æœ: {'âœ… é€šè¿‡' if assessment['passed'] else 'âŒ å¤±è´¥'}")
        print(f"    ç‰¹å¾ä¸€è‡´æ€§: {'âœ… é€šè¿‡' if all_consistent else 'âŒ å¤±è´¥'}")
        print(f"    æ€§èƒ½æå‡: {performance_improvement:.1f}%")
        print(f"    é›†æˆç­‰çº§: {assessment['integration_grade']}")

        return assessment

    def _validate_performance_targets(self) -> Dict[str, Any]:
        """éªŒè¯Day 3æ€§èƒ½ç›®æ ‡"""
        print("\n5. Day 3æ€§èƒ½ç›®æ ‡éªŒè¯...")

        # Day 3ç›®æ ‡ï¼šåœ¨Day 2åŸºç¡€ä¸Šé¢å¤–è·å¾—25-30%æ€§èƒ½æå‡
        target_improvement = 25.0  # æœ€ä½ç›®æ ‡

        test_data = np.random.randn(35000, 6).astype(np.float32)

        # æµ‹è¯•Day 2å‘é‡åŒ–æ€§èƒ½ï¼ˆåŸºå‡†ï¼‰
        from features.vectorized_extraction import VectorizedFeatureExtractor
        day2_extractor = VectorizedFeatureExtractor(180, 30)

        start_time = time.time()
        day2_features, _ = day2_extractor.extract_all_windows_vectorized(test_data)
        day2_time = time.time() - start_time

        # æµ‹è¯•Day 3ä¼˜åŒ–æ€§èƒ½
        day3_extractor = OptimizedFeatureExtractor(
            enable_parallel=True,
            memory_efficient=True,
            enable_monitoring=False  # é¿å…ç›‘æ§å¼€é”€å½±å“æµ‹è¯•
        )

        start_time = time.time()
        day3_features, _ = day3_extractor.extract_features(test_data, adaptive_mode=True)
        day3_time = time.time() - start_time

        # è®¡ç®—æ€§èƒ½æå‡
        actual_improvement = ((day2_time - day3_time) / day2_time) * 100
        target_met = actual_improvement >= target_improvement

        # ç´¯ç§¯æ”¹è¿›è¯„ä¼°ï¼ˆç›¸å¯¹äºæœªä¼˜åŒ–åŸºçº¿ï¼‰
        # å‡è®¾Day 1+2å·²å®ç°40xæ”¹è¿›ï¼ŒDay 3åº”åœ¨æ­¤åŸºç¡€ä¸Šé¢å¤–æå‡25-30%
        expected_total_speedup = 40 * (1 + target_improvement / 100)

        assessment = {
            'day2_time': day2_time,
            'day3_time': day3_time,
            'actual_improvement_percent': actual_improvement,
            'target_improvement_percent': target_improvement,
            'target_met': target_met,
            'expected_total_speedup': expected_total_speedup,
            'performance_grade': 'A' if actual_improvement >= 30 else 'B' if actual_improvement >= 25 else 'C',
            'passed': target_met
        }

        print(f"  Day 3æ€§èƒ½ç›®æ ‡éªŒè¯: {'âœ… è¾¾æˆ' if target_met else 'âŒ æœªè¾¾æˆ'}")
        print(f"    Day 2åŸºå‡†æ—¶é—´: {day2_time:.2f}s")
        print(f"    Day 3ä¼˜åŒ–æ—¶é—´: {day3_time:.2f}s")
        print(f"    å®é™…æ”¹è¿›: {actual_improvement:.1f}%")
        print(f"    ç›®æ ‡æ”¹è¿›: {target_improvement:.1f}%")

        return assessment

    def _calculate_overall_assessment(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—æ€»ä½“è¯„ä¼°"""
        print("\n" + "=" * 60)
        print("Day 3 ä¼˜åŒ–æ€»ä½“è¯„ä¼°")
        print("=" * 60)

        # å„é¡¹æµ‹è¯•é€šè¿‡æƒ…å†µ
        test_passes = {
            'memory_efficiency': results['memory_efficiency']['passed'],
            'parallel_performance': results['parallel_performance']['passed'],
            'system_stability': results['system_stability']['passed'],
            'integration_quality': results['integration_quality']['passed'],
            'performance_targets': results['performance_targets']['passed']
        }

        total_tests = len(test_passes)
        passed_tests = sum(test_passes.values())
        overall_pass_rate = passed_tests / total_tests

        # æ€§èƒ½ç­‰çº§è¯„ä¼°
        grades = {
            'memory_efficiency': results['memory_efficiency']['memory_efficiency_grade'],
            'parallel_performance': results['parallel_performance']['scalability_grade'],
            'system_stability': results['system_stability']['time_stability_grade'],
            'integration_quality': results['integration_quality']['integration_grade']
        }

        # è®¡ç®—ç»¼åˆç­‰çº§
        grade_scores = {'A': 4, 'B': 3, 'C': 2, 'D': 1, 'F': 0}
        avg_score = np.mean([grade_scores.get(grade, 0) for grade in grades.values()])

        if avg_score >= 3.5:
            overall_grade = 'A'
        elif avg_score >= 2.5:
            overall_grade = 'B'
        elif avg_score >= 1.5:
            overall_grade = 'C'
        else:
            overall_grade = 'D'

        assessment = {
            'test_pass_rate': overall_pass_rate,
            'passed_tests': passed_tests,
            'total_tests': total_tests,
            'individual_grades': grades,
            'overall_grade': overall_grade,
            'day3_optimization_successful': overall_pass_rate >= 0.8 and results['performance_targets']['passed'],
            'recommendations': []
        }

        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        if not results['memory_efficiency']['passed']:
            assessment['recommendations'].append("ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼šè€ƒè™‘æ›´ä¿å®ˆçš„åˆ†å—ç­–ç•¥æˆ–å†…å­˜æ¸…ç†")

        if not results['parallel_performance']['passed']:
            assessment['recommendations'].append("è°ƒæ•´å¹¶è¡Œç­–ç•¥ï¼šä¼˜åŒ–å·¥ä½œçº¿ç¨‹æ•°æˆ–åˆ†å—ç®—æ³•")

        if not results['system_stability']['passed']:
            assessment['recommendations'].append("æå‡ç³»ç»Ÿç¨³å®šæ€§ï¼šä¿®å¤å†…å­˜æ³„æ¼æˆ–å¼‚å¸¸å¤„ç†")

        if not results['integration_quality']['passed']:
            assessment['recommendations'].append("æ”¹å–„é›†æˆè´¨é‡ï¼šç¡®ä¿å„ç»„ä»¶é—´ç‰¹å¾ä¸€è‡´æ€§")

        if not results['performance_targets']['passed']:
            assessment['recommendations'].append("è¾¾æˆæ€§èƒ½ç›®æ ‡ï¼šéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ç®—æ³•æˆ–ç³»ç»Ÿèµ„æºåˆ©ç”¨")

        # æ‰“å°æ€»ä½“è¯„ä¼°ç»“æœ
        print(f"æµ‹è¯•é€šè¿‡ç‡: {overall_pass_rate*100:.1f}% ({passed_tests}/{total_tests})")
        print(f"ç»¼åˆç­‰çº§: {overall_grade}")
        print(f"Day 3 ä¼˜åŒ–: {'âœ… æˆåŠŸ' if assessment['day3_optimization_successful'] else 'âŒ éœ€è¦æ”¹è¿›'}")

        if assessment['recommendations']:
            print(f"\næ”¹è¿›å»ºè®®:")
            for i, rec in enumerate(assessment['recommendations'], 1):
                print(f"  {i}. {rec}")
        else:
            print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ŒDay 3 ä¼˜åŒ–è¾¾åˆ°é¢„æœŸç›®æ ‡ï¼")

        return assessment


def run_day3_validation():
    """è¿è¡ŒDay 3ä¼˜åŒ–éªŒè¯çš„ä¸»å‡½æ•°"""
    validator = Day3OptimizationValidator()

    try:
        results = validator.run_comprehensive_validation()

        # ä¿å­˜éªŒè¯ç»“æœ
        import json
        from datetime import datetime

        results['validation_timestamp'] = datetime.now().isoformat()
        results['validation_summary'] = {
            'day3_success': results['overall_assessment']['day3_optimization_successful'],
            'overall_grade': results['overall_assessment']['overall_grade'],
            'performance_improvement': results['performance_targets']['actual_improvement_percent']
        }

        with open('day3_validation_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)

        print(f"\néªŒè¯ç»“æœå·²ä¿å­˜è‡³: day3_validation_results.json")

        return results

    except Exception as e:
        print(f"éªŒè¯è¿‡ç¨‹å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == '__main__':
    # è¿è¡ŒDay 3éªŒè¯
    validation_results = run_day3_validation()

    if validation_results:
        success = validation_results['overall_assessment']['day3_optimization_successful']
        grade = validation_results['overall_assessment']['overall_grade']
        improvement = validation_results['performance_targets']['actual_improvement_percent']

        print(f"\n" + "=" * 60)
        print(f"Day 3 ä¼˜åŒ–éªŒè¯å®Œæˆ")
        print(f"ç»“æœ: {'âœ… æˆåŠŸ' if success else 'âŒ éœ€æ”¹è¿›'}")
        print(f"ç­‰çº§: {grade}")
        print(f"æ€§èƒ½æå‡: {improvement:.1f}%")
        print("=" * 60)