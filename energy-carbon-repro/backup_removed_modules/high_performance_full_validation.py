#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜æ€§èƒ½å®Œæ•´æ•°æ®é›†éªŒè¯
High-Performance Full Dataset Validation

ä½¿ç”¨é«˜æ€§èƒ½ç‰¹å¾å¼•æ“å¤„ç†å®Œæ•´1.55Mè¡Œæ•°æ®é›†
"""

import sys
import pandas as pd
import numpy as np
import torch
import logging
import time
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import train_test_split

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    """ä½¿ç”¨é«˜æ€§èƒ½å¼•æ“çš„å®Œæ•´æ•°æ®é›†éªŒè¯"""
    logger.info("=" * 60)
    logger.info("LNGè®ºæ–‡å¤ç° - é«˜æ€§èƒ½å®Œæ•´æ•°æ®é›†éªŒè¯")
    logger.info("=" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"è®¾å¤‡: {device}")

    # 1. åŠ è½½å®Œæ•´ç°å®åŒ–æ•°æ®
    logger.info("\n--- é˜¶æ®µ 1: åŠ è½½å®Œæ•´ç°å®åŒ–æ•°æ® ---")
    df = pd.read_csv('data/sim_lng/enhanced_simulation_data.csv', parse_dates=['ts'])

    power_columns = ['booster_pump_power_kw', 'hp_pump_power_kw', 'bog_compressor_total_power_kw']
    df['energy'] = df[power_columns].sum(axis=1)

    logger.info(f"å®Œæ•´æ•°æ®é›†: {len(df):,} è¡Œ")
    logger.info(f"ç°å®åŒ–èƒ½è€—: å‡å€¼={df['energy'].mean():.2f} kW, CV={df['energy'].std()/df['energy'].mean():.4f}")

    # 2. é«˜æ€§èƒ½ç‰¹å¾æå–
    logger.info("\n--- é˜¶æ®µ 2: é«˜æ€§èƒ½ç‰¹å¾æå– ---")
    from src.features.high_performance_engine import HighPerformanceFeatureEngine

    engine = HighPerformanceFeatureEngine(window_size=180, stride=30)
    feature_df = df.drop(columns=['ts', 'energy'])

    logger.info("å¼€å§‹é«˜æ€§èƒ½ç‰¹å¾æå–...")
    start_time = time.time()
    X_dynamic, X_static = engine.fast_extract_features(feature_df)
    feature_time = time.time() - start_time

    # 3. ç”Ÿæˆå¯¹åº”æ ‡ç­¾
    logger.info("\n--- é˜¶æ®µ 3: æ ‡ç­¾ç”Ÿæˆ ---")
    y_list = []
    for i in range(0, len(df) - 180 + 1, 30):
        y_list.append(df['energy'].iloc[i:i+180].mean())
    y = np.array(y_list[:len(X_dynamic)])

    logger.info(f"æ•°æ®å‡†å¤‡å®Œæˆ:")
    logger.info(f"  åŠ¨æ€ç‰¹å¾: {X_dynamic.shape}")
    logger.info(f"  é™æ€ç‰¹å¾: {X_static.shape}")
    logger.info(f"  æ ‡ç­¾: {y.shape}")

    # 4. æ¨¡å‹å¯¹æ¯”éªŒè¯
    logger.info("\n--- é˜¶æ®µ 4: å¤šæ¨¡å‹æ€§èƒ½å¯¹æ¯” ---")

    # è®­ç»ƒéªŒè¯åˆ†å‰²
    indices = np.arange(len(X_dynamic))
    train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42)

    X_dynamic_train, X_dynamic_val = X_dynamic[train_idx], X_dynamic[val_idx]
    X_static_train, X_static_val = X_static[train_idx], X_static[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]

    # ä¸ºä¼ ç»Ÿæ¨¡å‹å‡†å¤‡ç‰¹å¾
    X_flat_train = X_dynamic_train.reshape(len(X_dynamic_train), -1)
    X_flat_val = X_dynamic_val.reshape(len(X_dynamic_val), -1)

    # æ ‡å‡†åŒ–
    scaler_X = StandardScaler().fit(X_flat_train)
    scaler_y = StandardScaler().fit(y_train.reshape(-1, 1))

    X_train_s = scaler_X.transform(X_flat_train)
    X_val_s = scaler_X.transform(X_flat_val)
    y_train_s = scaler_y.transform(y_train.reshape(-1, 1)).flatten()
    y_val_s = scaler_y.transform(y_val.reshape(-1, 1)).flatten()

    results = {}

    # 4.1 RandomForeståŸºå‡†
    logger.info("\n4.1 RandomForeståŸºå‡†æ¨¡å‹")
    from sklearn.ensemble import RandomForestRegressor

    rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)
    rf_start = time.time()
    rf.fit(X_train_s, y_train_s)
    rf_pred = rf.predict(X_val_s)
    rf_time = time.time() - rf_start

    rf_pred_orig = scaler_y.inverse_transform(rf_pred.reshape(-1, 1)).flatten()
    y_val_orig = scaler_y.inverse_transform(y_val_s.reshape(-1, 1)).flatten()

    results['RandomForest'] = {
        'r2': r2_score(y_val_orig, rf_pred_orig),
        'cv_rmse': np.sqrt(mean_squared_error(y_val_orig, rf_pred_orig)) / np.mean(y_val_orig),
        'nmbe': np.mean(rf_pred_orig - y_val_orig) / np.mean(y_val_orig),
        'time': rf_time
    }

    # 4.2 è·¨æ¨¡æ€èåˆæ¨¡å‹
    logger.info("\n4.2 è·¨æ¨¡æ€èåˆæ¨¡å‹ (è®ºæ–‡æ ¸å¿ƒ)")
    from src.models.cross_modal_fusion import CrossModalFusionWithResidual

    model = CrossModalFusionWithResidual(
        dynamic_dim=X_dynamic.shape[-1],
        static_dim=X_static.shape[-1],
        seq_len=X_dynamic.shape[1],
        d_model=128,
        n_heads=4,
        n_layers=3
    ).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = torch.nn.MSELoss()

    # æ•°æ®è½¬æ¢
    train_dynamic = torch.FloatTensor(X_dynamic_train).to(device)
    train_static = torch.FloatTensor(X_static_train).to(device)
    train_labels = torch.FloatTensor(y_train_s).unsqueeze(1).to(device)

    # è®­ç»ƒ
    cm_start = time.time()
    model.train()
    for epoch in range(30):
        optimizer.zero_grad()
        pred, _ = model(train_dynamic, train_static)
        loss = criterion(pred, train_labels)

        if torch.isfinite(loss):
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

        if (epoch + 1) % 10 == 0:
            logger.info(f"  CrossModal Epoch {epoch+1}/30, Loss: {loss.item():.6f}")

    # è¯„ä¼°
    model.eval()
    with torch.no_grad():
        val_dynamic = torch.FloatTensor(X_dynamic_val).to(device)
        val_static = torch.FloatTensor(X_static_val).to(device)
        cm_pred, _ = model(val_dynamic, val_static)
        cm_pred_cpu = cm_pred.cpu().numpy().flatten()

    cm_time = time.time() - cm_start

    # å¤„ç†NaN
    if np.any(np.isnan(cm_pred_cpu)):
        logger.warning("è·¨æ¨¡æ€é¢„æµ‹åŒ…å«NaNï¼Œä½¿ç”¨å‡å€¼æ›¿ä»£")
        cm_pred_cpu = np.nan_to_num(cm_pred_cpu, nan=np.mean(y_train_s))

    cm_pred_orig = scaler_y.inverse_transform(cm_pred_cpu.reshape(-1, 1)).flatten()

    results['CrossModalFusion'] = {
        'r2': r2_score(y_val_orig, cm_pred_orig),
        'cv_rmse': np.sqrt(mean_squared_error(y_val_orig, cm_pred_orig)) / np.mean(y_val_orig),
        'nmbe': np.mean(cm_pred_orig - y_val_orig) / np.mean(y_val_orig),
        'time': cm_time
    }

    # 5. ç»“æœå¯¹æ¯”
    logger.info("\n" + "=" * 60)
    logger.info("--- é˜¶æ®µ 5: å®Œæ•´æ•°æ®é›†æ¨¡å‹æ€§èƒ½å¯¹æ¯” ---")
    logger.info("=" * 60)

    logger.info(f"\nğŸ“Š ç‰¹å¾æå–æ€§èƒ½:")
    logger.info(f"  å¤„ç†æ—¶é—´: {feature_time:.2f}ç§’")
    logger.info(f"  å¤„ç†é€Ÿåº¦: {len(X_dynamic)/feature_time:.1f} çª—å£/ç§’")
    logger.info(f"  æ•°æ®è§„æ¨¡: {len(X_dynamic):,} çª—å£")

    logger.info(f"\nğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯” (ç°å®åŒ–æ•°æ®):")
    for model_name, metrics in results.items():
        logger.info(f"\n{model_name}:")
        logger.info(f"  RÂ² Score: {metrics['r2']:.4f} {'âœ…' if metrics['r2'] >= 0.75 else 'âŒ'}")
        logger.info(f"  CV(RMSE): {metrics['cv_rmse']:.4f} {'âœ…' if metrics['cv_rmse'] <= 0.06 else 'âŒ'}")
        logger.info(f"  NMBE: {metrics['nmbe']:.4f} {'âœ…' if abs(metrics['nmbe']) <= 0.006 else 'âŒ'}")
        logger.info(f"  è®­ç»ƒæ—¶é—´: {metrics['time']:.2f}ç§’")

    # 6. ä¿å­˜ç»“æœ
    import yaml
    final_results = {
        'dataset': 'enhanced_simulation_data.csv',
        'feature_extraction_time': feature_time,
        'processing_speed_windows_per_sec': len(X_dynamic)/feature_time,
        'models': results,
        'data_characteristics': {
            'rows': len(df),
            'windows': len(X_dynamic),
            'energy_cv': df['energy'].std()/df['energy'].mean()
        }
    }

    results_path = Path("results") / f"full_dataset_comparison_{time.strftime('%Y%m%d_%H%M%S')}.yaml"
    results_path.parent.mkdir(exist_ok=True)

    with open(results_path, 'w', encoding='utf-8') as f:
        yaml.dump(final_results, f, allow_unicode=True)

    logger.info(f"\nå®Œæ•´å¯¹æ¯”ç»“æœå·²ä¿å­˜: {results_path}")
    logger.info("\nğŸ¯ é«˜æ€§èƒ½å®Œæ•´æ•°æ®é›†éªŒè¯å®Œæˆ")

if __name__ == "__main__":
    main()