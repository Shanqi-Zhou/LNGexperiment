"""
è‡ªåŠ¨åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶ - Week 2 ä¼˜åŒ–
æä¾›å…¨é¢çš„æ€§èƒ½å¯¹æ¯”å’ŒåŸºå‡†æµ‹è¯•åŠŸèƒ½
"""

import time
import numpy as np
import pandas as pd
import psutil
import warnings
from typing import Dict, Any, List, Tuple, Optional, Callable
from pathlib import Path
import json
import matplotlib.pyplot as plt
import seaborn as sns
from concurrent.futures import ThreadPoolExecutor
import threading
from datetime import datetime


class PerformanceBenchmark:
    """
    æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶
    å¯¹æ¯”ä¼˜åŒ–å‰åçš„æ€§èƒ½å·®å¼‚ï¼Œç”Ÿæˆè¯¦ç»†çš„åŸºå‡†æŠ¥å‘Š
    """

    def __init__(self, test_data_sizes: List[int] = None, output_dir: str = "benchmark_results"):
        """
        åˆå§‹åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•

        Args:
            test_data_sizes: æµ‹è¯•æ•°æ®å¤§å°åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
        """
        self.test_data_sizes = test_data_sizes or [1000, 5000, 10000, 20000]
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # æµ‹è¯•ç»“æœå­˜å‚¨
        self.baseline_results = {}
        self.optimized_results = {}
        self.comparison_results = {}

        # èµ„æºç›‘æ§
        self.resource_monitor = ResourceMonitor()

        print(f"  æ€§èƒ½åŸºå‡†æµ‹è¯•åˆå§‹åŒ–: æµ‹è¯•å¤§å°={self.test_data_sizes}")

    def run_complete_benchmark(self, baseline_extractor: Callable, optimized_extractor: Callable,
                             test_data_generator: Callable = None) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•

        Args:
            baseline_extractor: åŸºçº¿ç‰¹å¾æå–å™¨
            optimized_extractor: ä¼˜åŒ–åç‰¹å¾æå–å™¨
            test_data_generator: æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨

        Returns:
            å®Œæ•´çš„åŸºå‡†æµ‹è¯•ç»“æœ
        """
        print("=== å¼€å§‹å®Œæ•´æ€§èƒ½åŸºå‡†æµ‹è¯• ===")

        if test_data_generator is None:
            test_data_generator = self._default_data_generator

        benchmark_start = time.time()

        # 1. è¿è¡ŒåŸºçº¿æµ‹è¯•
        print("\n--- åŸºçº¿ç‰ˆæœ¬æµ‹è¯• ---")
        self.baseline_results = self._run_extractor_benchmark(
            baseline_extractor, test_data_generator, "baseline"
        )

        # 2. è¿è¡Œä¼˜åŒ–ç‰ˆæœ¬æµ‹è¯•
        print("\n--- ä¼˜åŒ–ç‰ˆæœ¬æµ‹è¯• ---")
        self.optimized_results = self._run_extractor_benchmark(
            optimized_extractor, test_data_generator, "optimized"
        )

        # 3. ç”Ÿæˆå¯¹æ¯”åˆ†æ
        print("\n--- ç”Ÿæˆå¯¹æ¯”åˆ†æ ---")
        self.comparison_results = self._generate_comparison_analysis()

        # 4. ç”ŸæˆæŠ¥å‘Š
        total_benchmark_time = time.time() - benchmark_start
        report = self._generate_comprehensive_report(total_benchmark_time)

        # 5. ä¿å­˜ç»“æœ
        self._save_benchmark_results()

        # 6. ç”Ÿæˆå¯è§†åŒ–
        self._generate_visualizations()

        print(f"\n=== åŸºå‡†æµ‹è¯•å®Œæˆï¼Œæ€»ç”¨æ—¶: {total_benchmark_time:.2f}ç§’ ===")

        return {
            'baseline_results': self.baseline_results,
            'optimized_results': self.optimized_results,
            'comparison_results': self.comparison_results,
            'report': report
        }

    def _default_data_generator(self, size: int, n_features: int = 6) -> np.ndarray:
        """é»˜è®¤æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨"""
        np.random.seed(42)  # ç¡®ä¿ç»“æœå¯é‡å¤

        # ç”Ÿæˆå…·æœ‰æ—¶åºç‰¹å¾çš„æ•°æ®
        t = np.linspace(0, 10*np.pi, size)
        data = np.zeros((size, n_features))

        # æ·»åŠ ä¸åŒç±»å‹çš„ä¿¡å·
        data[:, 0] = np.sin(t) + 0.1 * np.random.randn(size)  # æ­£å¼¦ä¿¡å·
        data[:, 1] = np.cos(2*t) + 0.1 * np.random.randn(size)  # ä½™å¼¦ä¿¡å·
        data[:, 2] = np.cumsum(np.random.randn(size)) * 0.1  # éšæœºæ¸¸èµ°
        data[:, 3] = np.where(np.random.rand(size) > 0.95, 5, 0) + np.random.randn(size) * 0.1  # ç¨€ç–è„‰å†²
        data[:, 4] = np.linspace(0, 5, size) + np.random.randn(size) * 0.2  # çº¿æ€§è¶‹åŠ¿
        data[:, 5] = np.random.randn(size)  # ç™½å™ªå£°

        return data

    def _run_extractor_benchmark(self, extractor: Callable, data_generator: Callable,
                                version_name: str) -> Dict[str, Any]:
        """
        è¿è¡Œå•ä¸ªæå–å™¨çš„åŸºå‡†æµ‹è¯•

        Args:
            extractor: ç‰¹å¾æå–å™¨å‡½æ•°
            data_generator: æ•°æ®ç”Ÿæˆå™¨
            version_name: ç‰ˆæœ¬åç§°

        Returns:
            åŸºå‡†æµ‹è¯•ç»“æœ
        """
        results = {}

        for data_size in self.test_data_sizes:
            print(f"    æµ‹è¯•æ•°æ®å¤§å°: {data_size}")

            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            test_data = data_generator(data_size)

            # å¯åŠ¨èµ„æºç›‘æ§
            self.resource_monitor.start_monitoring()

            try:
                # é¢„çƒ­è¿è¡Œ
                _ = extractor(test_data[:min(100, data_size)])

                # æ­£å¼æµ‹è¯•ï¼ˆå¤šæ¬¡è¿è¡Œå–å¹³å‡å€¼ï¼‰
                run_times = []
                memory_peaks = []
                cpu_peaks = []

                n_runs = 3 if data_size <= 10000 else 1  # å¤§æ•°æ®é›†åªè¿è¡Œ1æ¬¡

                for run_idx in range(n_runs):
                    print(f"      ç¬¬ {run_idx + 1}/{n_runs} æ¬¡è¿è¡Œ...")

                    # é‡ç½®èµ„æºç›‘æ§
                    self.resource_monitor.reset_stats()

                    start_time = time.time()
                    start_memory = psutil.Process().memory_info().rss / 1024**2

                    # æ‰§è¡Œç‰¹å¾æå–
                    features = extractor(test_data)

                    end_time = time.time()
                    end_memory = psutil.Process().memory_info().rss / 1024**2

                    # è®°å½•æ€§èƒ½æŒ‡æ ‡
                    run_time = end_time - start_time
                    memory_usage = end_memory - start_memory

                    run_times.append(run_time)
                    memory_peaks.append(memory_usage)

                    # è·å–èµ„æºç›‘æ§ç»“æœ
                    monitor_stats = self.resource_monitor.get_summary()
                    cpu_peaks.append(monitor_stats.get('max_cpu', 0))

                    # éªŒè¯ç‰¹å¾å½¢çŠ¶
                    if hasattr(features, 'shape'):
                        feature_shape = features.shape
                    elif isinstance(features, (list, tuple)):
                        feature_shape = (len(features[0]) if features else 0, len(features) if features else 0)
                    else:
                        feature_shape = (0, 0)

                # è®¡ç®—ç»Ÿè®¡é‡
                avg_time = np.mean(run_times)
                std_time = np.std(run_times)
                avg_memory = np.mean(memory_peaks)
                max_cpu = np.max(cpu_peaks)

                results[data_size] = {
                    'avg_time': avg_time,
                    'std_time': std_time,
                    'min_time': np.min(run_times),
                    'max_time': np.max(run_times),
                    'all_times': run_times,
                    'avg_memory_mb': avg_memory,
                    'max_cpu_percent': max_cpu,
                    'throughput_samples_per_sec': data_size / avg_time,
                    'feature_shape': feature_shape,
                    'data_shape': test_data.shape,
                    'runs_count': n_runs
                }

                print(f"      å¹³å‡ç”¨æ—¶: {avg_time:.3f}Â±{std_time:.3f}ç§’")
                print(f"      å†…å­˜ä½¿ç”¨: {avg_memory:.1f}MB")
                print(f"      CPUå³°å€¼: {max_cpu:.1f}%")

            except Exception as e:
                print(f"      âŒ æµ‹è¯•å¤±è´¥: {e}")
                results[data_size] = {
                    'error': str(e),
                    'avg_time': float('inf'),
                    'failed': True
                }

            finally:
                self.resource_monitor.stop_monitoring()

        return results

    def _generate_comparison_analysis(self) -> Dict[str, Any]:
        """ç”Ÿæˆå¯¹æ¯”åˆ†æç»“æœ"""
        comparison = {}

        for data_size in self.test_data_sizes:
            baseline = self.baseline_results.get(data_size, {})
            optimized = self.optimized_results.get(data_size, {})

            if baseline.get('failed') or optimized.get('failed'):
                comparison[data_size] = {'error': 'One or both tests failed'}
                continue

            baseline_time = baseline.get('avg_time', float('inf'))
            optimized_time = optimized.get('avg_time', float('inf'))

            if baseline_time == 0 or optimized_time == 0:
                speedup = 1.0
            else:
                speedup = baseline_time / optimized_time

            time_saved = baseline_time - optimized_time
            efficiency_gain = ((baseline_time - optimized_time) / baseline_time) * 100

            baseline_throughput = baseline.get('throughput_samples_per_sec', 0)
            optimized_throughput = optimized.get('throughput_samples_per_sec', 0)

            throughput_improvement = (optimized_throughput - baseline_throughput) / max(baseline_throughput, 1) * 100

            comparison[data_size] = {
                'speedup': speedup,
                'time_saved_seconds': time_saved,
                'efficiency_gain_percent': efficiency_gain,
                'baseline_time': baseline_time,
                'optimized_time': optimized_time,
                'baseline_throughput': baseline_throughput,
                'optimized_throughput': optimized_throughput,
                'throughput_improvement_percent': throughput_improvement,
                'memory_comparison': {
                    'baseline_mb': baseline.get('avg_memory_mb', 0),
                    'optimized_mb': optimized.get('avg_memory_mb', 0),
                    'memory_reduction_mb': baseline.get('avg_memory_mb', 0) - optimized.get('avg_memory_mb', 0)
                },
                'cpu_comparison': {
                    'baseline_max_cpu': baseline.get('max_cpu_percent', 0),
                    'optimized_max_cpu': optimized.get('max_cpu_percent', 0)
                }
            }

        return comparison

    def _generate_comprehensive_report(self, total_time: float) -> str:
        """ç”Ÿæˆç»¼åˆæ€§èƒ½æŠ¥å‘Š"""
        report_lines = [
            "=" * 80,
            "LNGé¡¹ç›®ä¼˜åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š",
            "=" * 80,
            f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"æ€»æµ‹è¯•ç”¨æ—¶: {total_time:.2f}ç§’",
            f"æµ‹è¯•æ•°æ®è§„æ¨¡: {self.test_data_sizes}",
            "",
            "## æ€§èƒ½å¯¹æ¯”æ€»ç»“",
            ""
        ]

        # æ±‡æ€»ç»Ÿè®¡
        all_speedups = []
        all_efficiency_gains = []
        all_throughput_improvements = []

        valid_comparisons = []
        for data_size, comp in self.comparison_results.items():
            if 'error' not in comp:
                valid_comparisons.append((data_size, comp))
                all_speedups.append(comp['speedup'])
                all_efficiency_gains.append(comp['efficiency_gain_percent'])
                all_throughput_improvements.append(comp['throughput_improvement_percent'])

        if valid_comparisons:
            avg_speedup = np.mean(all_speedups)
            avg_efficiency = np.mean(all_efficiency_gains)
            avg_throughput_improvement = np.mean(all_throughput_improvements)

            report_lines.extend([
                f"å¹³å‡åŠ é€Ÿæ¯”: {avg_speedup:.2f}x",
                f"å¹³å‡æ•ˆç‡æå‡: {avg_efficiency:.1f}%",
                f"å¹³å‡ååé‡æå‡: {avg_throughput_improvement:.1f}%",
                "",
                "## è¯¦ç»†æ€§èƒ½æ•°æ®",
                "",
                f"{'æ•°æ®è§„æ¨¡':<10} {'åŸºçº¿ç”¨æ—¶':<12} {'ä¼˜åŒ–ç”¨æ—¶':<12} {'åŠ é€Ÿæ¯”':<10} {'æ•ˆç‡æå‡':<12} {'ååé‡æå‡':<12}",
                "-" * 80
            ])

            for data_size, comp in valid_comparisons:
                report_lines.append(
                    f"{data_size:<10} "
                    f"{comp['baseline_time']:<12.3f} "
                    f"{comp['optimized_time']:<12.3f} "
                    f"{comp['speedup']:<10.2f} "
                    f"{comp['efficiency_gain_percent']:<12.1f}% "
                    f"{comp['throughput_improvement_percent']:<12.1f}%"
                )

            report_lines.extend([
                "",
                "## å†…å­˜ä½¿ç”¨å¯¹æ¯”",
                ""
            ])

            for data_size, comp in valid_comparisons:
                mem_comp = comp['memory_comparison']
                report_lines.append(
                    f"æ•°æ®è§„æ¨¡ {data_size}: "
                    f"åŸºçº¿ {mem_comp['baseline_mb']:.1f}MB â†’ "
                    f"ä¼˜åŒ– {mem_comp['optimized_mb']:.1f}MB "
                    f"(èŠ‚çœ {mem_comp['memory_reduction_mb']:.1f}MB)"
                )

            report_lines.extend([
                "",
                "## æ€§èƒ½è¯„ä¼°",
                ""
            ])

            if avg_speedup >= 2.0:
                performance_rating = "ğŸš€ ä¼˜ç§€ (>2x)"
            elif avg_speedup >= 1.5:
                performance_rating = "âœ… è‰¯å¥½ (1.5-2x)"
            elif avg_speedup >= 1.2:
                performance_rating = "ğŸ‘ æ»¡æ„ (1.2-1.5x)"
            elif avg_speedup >= 1.0:
                performance_rating = "âš ï¸  ä¸€èˆ¬ (1.0-1.2x)"
            else:
                performance_rating = "âŒ æ€§èƒ½ä¸‹é™"

            report_lines.extend([
                f"æ•´ä½“æ€§èƒ½è¯„çº§: {performance_rating}",
                f"æ¨èéƒ¨ç½²: {'æ˜¯' if avg_speedup >= 1.2 else 'éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–'}",
                "",
                "## ç”Ÿäº§ç¯å¢ƒé¢„æœŸæ”¶ç›Š",
                ""
            ])

            # åŸºäºæœ€å¤§æ•°æ®è§„æ¨¡é¢„ä¼°ç”Ÿäº§ç¯å¢ƒæ”¶ç›Š
            largest_test = valid_comparisons[-1][1] if valid_comparisons else None
            if largest_test:
                production_estimate = self._estimate_production_benefits(largest_test)
                report_lines.extend(production_estimate)

        else:
            report_lines.extend([
                "âŒ æ‰€æœ‰æµ‹è¯•å‡å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š",
                ""
            ])

        report_lines.extend([
            "",
            "=" * 80,
            f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "=" * 80
        ])

        return "\\n".join(report_lines)

    def _estimate_production_benefits(self, largest_comparison: Dict) -> List[str]:
        """åŸºäºæµ‹è¯•ç»“æœä¼°ç®—ç”Ÿäº§ç¯å¢ƒæ”¶ç›Š"""
        speedup = largest_comparison['speedup']
        time_saved = largest_comparison['time_saved_seconds']

        # å‡è®¾åŸç‰¹å¾æå–æ—¶é—´ä¸º82åˆ†é’Ÿï¼ˆåŸºäºé¡¹ç›®æ–‡æ¡£ï¼‰
        original_time_minutes = 82
        optimized_time_minutes = original_time_minutes / speedup

        time_saved_minutes = original_time_minutes - optimized_time_minutes

        # å‡è®¾åŸé¡¹ç›®æ€»æ—¶é—´ä¸º2.3å°æ—¶
        original_total_hours = 2.3
        total_time_saved_hours = time_saved_minutes / 60
        new_total_hours = original_total_hours - total_time_saved_hours

        percentage_improvement = (total_time_saved_hours / original_total_hours) * 100

        return [
            f"åŸºäºæµ‹è¯•ç»“æœé¢„ä¼°ç”Ÿäº§ç¯å¢ƒæ”¶ç›Š:",
            f"- åŸç‰¹å¾æå–æ—¶é—´: {original_time_minutes}åˆ†é’Ÿ",
            f"- ä¼˜åŒ–åé¢„ä¼°æ—¶é—´: {optimized_time_minutes:.1f}åˆ†é’Ÿ",
            f"- é¢„è®¡èŠ‚çœæ—¶é—´: {time_saved_minutes:.1f}åˆ†é’Ÿ",
            f"- åŸé¡¹ç›®æ€»æ—¶é—´: {original_total_hours}å°æ—¶",
            f"- ä¼˜åŒ–åé¢„ä¼°æ€»æ—¶é—´: {new_total_hours:.1f}å°æ—¶",
            f"- æ€»ä½“æ”¹è¿›é¢„ä¼°: {percentage_improvement:.1f}%"
        ]

    def _save_benchmark_results(self):
        """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_data = {
            'timestamp': timestamp,
            'test_data_sizes': self.test_data_sizes,
            'baseline_results': self.baseline_results,
            'optimized_results': self.optimized_results,
            'comparison_results': self.comparison_results
        }

        results_file = self.output_dir / f"benchmark_results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, default=str)

        print(f"    è¯¦ç»†ç»“æœå·²ä¿å­˜: {results_file}")

        # ä¿å­˜æŠ¥å‘Š
        report = self._generate_comprehensive_report(0)
        report_file = self.output_dir / f"benchmark_report_{timestamp}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"    åŸºå‡†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    def _generate_visualizations(self):
        """ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å¯è§†åŒ–å›¾è¡¨"""
        try:
            plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')

            # åˆ›å»ºå­å›¾
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('LNGé¡¹ç›®ä¼˜åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ', fontsize=16, fontweight='bold')

            # å‡†å¤‡æ•°æ®
            data_sizes = []
            baseline_times = []
            optimized_times = []
            speedups = []
            throughput_baseline = []
            throughput_optimized = []

            for size, comp in self.comparison_results.items():
                if 'error' not in comp:
                    data_sizes.append(size)
                    baseline_times.append(comp['baseline_time'])
                    optimized_times.append(comp['optimized_time'])
                    speedups.append(comp['speedup'])
                    throughput_baseline.append(comp['baseline_throughput'])
                    throughput_optimized.append(comp['optimized_throughput'])

            if not data_sizes:
                print("    æ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡å¯è§†åŒ–ç”Ÿæˆ")
                return

            # 1. è¿è¡Œæ—¶é—´å¯¹æ¯”
            axes[0, 0].plot(data_sizes, baseline_times, 'o-', label='åŸºçº¿ç‰ˆæœ¬', color='red', linewidth=2)
            axes[0, 0].plot(data_sizes, optimized_times, 'o-', label='ä¼˜åŒ–ç‰ˆæœ¬', color='green', linewidth=2)
            axes[0, 0].set_xlabel('æ•°æ®è§„æ¨¡')
            axes[0, 0].set_ylabel('è¿è¡Œæ—¶é—´ (ç§’)')
            axes[0, 0].set_title('è¿è¡Œæ—¶é—´å¯¹æ¯”')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
            axes[0, 0].set_yscale('log')

            # 2. åŠ é€Ÿæ¯”
            axes[0, 1].plot(data_sizes, speedups, 'o-', color='blue', linewidth=2, markersize=8)
            axes[0, 1].axhline(y=1, color='red', linestyle='--', alpha=0.7, label='æ— æ”¹è¿›çº¿')
            axes[0, 1].set_xlabel('æ•°æ®è§„æ¨¡')
            axes[0, 1].set_ylabel('åŠ é€Ÿæ¯”')
            axes[0, 1].set_title('æ€§èƒ½åŠ é€Ÿæ¯”')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)

            # åœ¨æ¯ä¸ªç‚¹ä¸Šæ ‡æ³¨åŠ é€Ÿæ¯”æ•°å€¼
            for i, (size, speedup) in enumerate(zip(data_sizes, speedups)):
                axes[0, 1].annotate(f'{speedup:.1f}x',
                                   (size, speedup),
                                   textcoords="offset points",
                                   xytext=(0, 10),
                                   ha='center')

            # 3. ååé‡å¯¹æ¯”
            x_pos = np.arange(len(data_sizes))
            width = 0.35

            axes[1, 0].bar(x_pos - width/2, throughput_baseline, width, label='åŸºçº¿ç‰ˆæœ¬', color='red', alpha=0.7)
            axes[1, 0].bar(x_pos + width/2, throughput_optimized, width, label='ä¼˜åŒ–ç‰ˆæœ¬', color='green', alpha=0.7)
            axes[1, 0].set_xlabel('æ•°æ®è§„æ¨¡')
            axes[1, 0].set_ylabel('ååé‡ (æ ·æœ¬/ç§’)')
            axes[1, 0].set_title('å¤„ç†ååé‡å¯¹æ¯”')
            axes[1, 0].set_xticks(x_pos)
            axes[1, 0].set_xticklabels(data_sizes)
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)

            # 4. æ•ˆç‡æå‡ç™¾åˆ†æ¯”
            efficiency_gains = [comp['efficiency_gain_percent'] for comp in self.comparison_results.values() if 'error' not in comp]

            axes[1, 1].bar(range(len(data_sizes)), efficiency_gains, color='purple', alpha=0.7)
            axes[1, 1].set_xlabel('æ•°æ®è§„æ¨¡')
            axes[1, 1].set_ylabel('æ•ˆç‡æå‡ (%)')
            axes[1, 1].set_title('æ•ˆç‡æå‡ç™¾åˆ†æ¯”')
            axes[1, 1].set_xticks(range(len(data_sizes)))
            axes[1, 1].set_xticklabels(data_sizes)
            axes[1, 1].grid(True, alpha=0.3)

            # åœ¨æ¯ä¸ªæŸ±å­ä¸Šæ ‡æ³¨æ•°å€¼
            for i, gain in enumerate(efficiency_gains):
                axes[1, 1].annotate(f'{gain:.1f}%',
                                   (i, gain),
                                   textcoords="offset points",
                                   xytext=(0, 5),
                                   ha='center')

            plt.tight_layout()

            # ä¿å­˜å›¾è¡¨
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            chart_file = self.output_dir / f"benchmark_charts_{timestamp}.png"
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()

            print(f"    æ€§èƒ½å›¾è¡¨å·²ä¿å­˜: {chart_file}")

        except Exception as e:
            print(f"    å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")


class ResourceMonitor:
    """ç³»ç»Ÿèµ„æºç›‘æ§å™¨"""

    def __init__(self):
        self.monitoring = False
        self.stats = {
            'cpu_usage': [],
            'memory_usage': [],
            'timestamps': []
        }
        self.monitor_thread = None

    def start_monitoring(self, interval: float = 0.1):
        """å¼€å§‹ç›‘æ§ç³»ç»Ÿèµ„æº"""
        if self.monitoring:
            return

        self.monitoring = True
        self.stats = {'cpu_usage': [], 'memory_usage': [], 'timestamps': []}

        self.monitor_thread = threading.Thread(target=self._monitor_loop, args=(interval,))
        self.monitor_thread.daemon = True
        self.monitor_thread.start()

    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1)

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡æ•°æ®"""
        self.stats = {'cpu_usage': [], 'memory_usage': [], 'timestamps': []}

    def _monitor_loop(self, interval: float):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            try:
                cpu_percent = psutil.cpu_percent()
                memory_percent = psutil.virtual_memory().percent

                self.stats['cpu_usage'].append(cpu_percent)
                self.stats['memory_usage'].append(memory_percent)
                self.stats['timestamps'].append(time.time())

                time.sleep(interval)
            except:
                break

    def get_summary(self) -> Dict[str, float]:
        """è·å–ç›‘æ§æ‘˜è¦"""
        if not self.stats['cpu_usage']:
            return {'avg_cpu': 0, 'max_cpu': 0, 'avg_memory': 0, 'max_memory': 0}

        return {
            'avg_cpu': np.mean(self.stats['cpu_usage']),
            'max_cpu': np.max(self.stats['cpu_usage']),
            'avg_memory': np.mean(self.stats['memory_usage']),
            'max_memory': np.max(self.stats['memory_usage'])
        }


# æµ‹è¯•ä»£ç 
if __name__ == '__main__':
    print("=== æ€§èƒ½åŸºå‡†æµ‹è¯•ç³»ç»Ÿæµ‹è¯• ===")

    # æ¨¡æ‹Ÿç‰¹å¾æå–å‡½æ•°
    def baseline_extractor(data):
        """æ¨¡æ‹ŸåŸºçº¿ç‰¹å¾æå–å™¨ï¼ˆè¾ƒæ…¢ï¼‰"""
        time.sleep(0.001 * len(data) / 1000)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        return np.random.randn(len(data) // 10, 20)

    def optimized_extractor(data):
        """æ¨¡æ‹Ÿä¼˜åŒ–ç‰¹å¾æå–å™¨ï¼ˆè¾ƒå¿«ï¼‰"""
        time.sleep(0.0003 * len(data) / 1000)  # æ›´å¿«çš„å¤„ç†
        return np.random.randn(len(data) // 10, 20)

    # åˆ›å»ºåŸºå‡†æµ‹è¯•
    benchmark = PerformanceBenchmark(
        test_data_sizes=[1000, 2000, 5000],
        output_dir="test_benchmark_results"
    )

    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    results = benchmark.run_complete_benchmark(
        baseline_extractor=baseline_extractor,
        optimized_extractor=optimized_extractor
    )

    print("\\nåŸºå‡†æµ‹è¯•å®Œæˆ!")
    print(f"ç»“æœä¿å­˜åœ¨: {benchmark.output_dir}")

    print("\\n=== æ€§èƒ½åŸºå‡†æµ‹è¯•ç³»ç»Ÿæµ‹è¯•å®Œæˆ ===")