#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–çš„ä¸»ç¨‹åº - ä½¿ç”¨è®ºæ–‡æ ¸å¿ƒæ¶æ„
Enhanced Main Program with Paper's Core Architecture

é›†æˆè·¨æ¨¡æ€èåˆ + é«˜çº§ç‰¹å¾å·¥ç¨‹ + MLR/GPRæ®‹å·®å»ºæ¨¡
"""

import argparse
import yaml
import pandas as pd
import numpy as np
import torch
import warnings
import os
import sys
import io
import time
import gc
from datetime import datetime
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from torch.utils.data import TensorDataset, DataLoader
import logging

# è®¾ç½®UTF-8ç¼–ç ï¼Œè§£å†³Windows gbkç¼–ç é—®é¢˜
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from src.features.advanced_feature_engineering import AdvancedFeatureEngineering, FeatureConfig
from src.models.adaptive_strategy import create_adaptive_model
from src.models.cross_modal_fusion import CrossModalFusionWithResidual
from src.training.unified_framework import UnifiedTrainer
from src.training.purged_validation import PurgedWalkForwardCV
from src.eval.evaluator import ComprehensiveEvaluator

def setup_device():
    """é…ç½®GPUè®¾å¤‡"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    if torch.cuda.is_available():
        logger.info(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
        logger.info(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        torch.backends.cudnn.benchmark = True
        torch.cuda.empty_cache()
    else:
        logger.info("ä½¿ç”¨CPUè®­ç»ƒ")
    return device

def load_config(path):
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    with open(path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def load_data_with_progress(data_path):
    """åŠ è½½æ•°æ®å¹¶æ˜¾ç¤ºè¿›åº¦"""
    logger.info(f"åŠ è½½æ•°æ®: {data_path}")
    start_time = time.time()

    chunk_size = 100000
    chunks = []
    total_rows = 0

    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            total_lines = sum(1 for _ in f) - 1

        logger.info(f"æ•°æ®é›†æ€»è¡Œæ•°: {total_lines:,}")

        for chunk in pd.read_csv(data_path, parse_dates=['ts'], chunksize=chunk_size):
            chunks.append(chunk)
            total_rows += len(chunk)
            progress = (total_rows / total_lines) * 100
            logger.info(f"  åŠ è½½è¿›åº¦: {progress:.1f}% ({total_rows:,}/{total_lines:,} è¡Œ)")

        df = pd.concat(chunks, ignore_index=True)
        elapsed = time.time() - start_time
        logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: {len(df):,} è¡Œ, è€—æ—¶ {elapsed:.2f} ç§’")

        # å†…å­˜ä¼˜åŒ–
        for col in df.columns:
            if df[col].dtype == 'float64':
                df[col] = df[col].astype('float32')

        return df

    except Exception as e:
        logger.error(f"åŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}")
        raise

def extract_features_advanced(df, config):
    """ä½¿ç”¨é«˜çº§ç‰¹å¾å·¥ç¨‹ç³»ç»Ÿ"""
    logger.info("å¼€å§‹é«˜çº§ç‰¹å¾æå–...")

    # åˆ›å»ºç‰¹å¾å·¥ç¨‹é…ç½®
    feature_config = FeatureConfig(
        window_size=config['data_processing']['window_size'],
        stride=config['data_processing']['stride']
    )

    # åˆå§‹åŒ–é«˜çº§ç‰¹å¾å·¥ç¨‹
    feature_engine = AdvancedFeatureEngineering(feature_config)
    logger.info("å¯ç”¨è®ºæ–‡å®Œæ•´ç‰¹å¾ä½“ç³»: 9ç±»åŠ¨æ€ç‰¹å¾ + 32ç»´é™æ€ç‰¹å¾")

    # å¤„ç†æ•°æ®é›†ï¼Œç”Ÿæˆè·¨æ¨¡æ€ç‰¹å¾
    feature_df = df.drop(columns=['ts', 'energy'])
    X_dynamic, X_static = feature_engine.process_dataset(feature_df)

    # ç”Ÿæˆå¯¹åº”çš„æ ‡ç­¾
    y_list = []
    window_size = feature_config.window_size
    stride = feature_config.stride

    for i in range(0, len(df) - window_size + 1, stride):
        window_energy = df['energy'].iloc[i:i+window_size].mean()
        y_list.append(window_energy)

    y = np.array(y_list[:len(X_dynamic)])

    logger.info(f"é«˜çº§ç‰¹å¾æå–å®Œæˆ:")
    logger.info(f"  åŠ¨æ€ç‰¹å¾å½¢çŠ¶: {X_dynamic.shape}")
    logger.info(f"  é™æ€ç‰¹å¾å½¢çŠ¶: {X_static.shape}")
    logger.info(f"  æ ‡ç­¾å½¢çŠ¶: {y.shape}")

    return X_dynamic, X_static, y

def train_fold_cross_modal(fold_idx, X_dynamic_train, X_static_train, y_train,
                          X_dynamic_val, X_static_val, y_val, config, device):
    """ä½¿ç”¨è·¨æ¨¡æ€èåˆè®­ç»ƒ"""
    logger.info(f"\n===== è·¨æ¨¡æ€èåˆè®­ç»ƒ FOLD {fold_idx + 1} =====")

    # æ•°æ®æ ‡å‡†åŒ–
    y_scaler = StandardScaler().fit(y_train.reshape(-1, 1))
    y_train_s = y_scaler.transform(y_train.reshape(-1, 1)).flatten().astype(np.float32)
    y_val_s = y_scaler.transform(y_val.reshape(-1, 1)).flatten().astype(np.float32)

    # åˆ›å»ºè·¨æ¨¡æ€èåˆæ¨¡å‹
    n_dynamic_features = X_dynamic_train.shape[-1]
    n_static_features = X_static_train.shape[-1]
    seq_len = X_dynamic_train.shape[1]

    model = CrossModalFusionWithResidual(
        dynamic_dim=n_dynamic_features,
        static_dim=n_static_features,
        seq_len=seq_len,
        d_model=128,
        n_heads=4,
        n_layers=3
    )

    logger.info(f"è·¨æ¨¡æ€èåˆæ¨¡å‹åˆå§‹åŒ–:")
    logger.info(f"  åŠ¨æ€ç‰¹å¾ç»´åº¦: {n_dynamic_features}")
    logger.info(f"  é™æ€ç‰¹å¾ç»´åº¦: {n_static_features}")
    logger.info(f"  åºåˆ—é•¿åº¦: {seq_len}")

    # GPUè®­ç»ƒ
    model = model.to(device)

    # å‡†å¤‡æ•°æ®
    train_dynamic = torch.FloatTensor(X_dynamic_train).to(device)
    train_static = torch.FloatTensor(X_static_train).to(device)
    train_labels = torch.FloatTensor(y_train_s).unsqueeze(1).to(device)

    val_dynamic = torch.FloatTensor(X_dynamic_val).to(device)
    val_static = torch.FloatTensor(X_static_val).to(device)
    val_labels = torch.FloatTensor(y_val_s).unsqueeze(1).to(device)

    # è®­ç»ƒæ¨¡å‹
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = torch.nn.MSELoss()

    model.train()
    for epoch in range(50):  # å¿«é€Ÿè®­ç»ƒ
        optimizer.zero_grad()

        pred, uncertainty = model(train_dynamic, train_static)
        loss = criterion(pred, train_labels)

        loss.backward()
        optimizer.step()

        if (epoch + 1) % 10 == 0:
            logger.info(f"  Epoch {epoch+1}/50, Loss: {loss.item():.6f}")

    # è¯„ä¼°
    model.eval()
    with torch.no_grad():
        val_pred, _ = model(val_dynamic, val_static)
        val_pred_cpu = val_pred.cpu().numpy().flatten()

    # åæ ‡å‡†åŒ–
    y_pred_original = y_scaler.inverse_transform(val_pred_cpu.reshape(-1, 1)).flatten()
    y_val_original = y_scaler.inverse_transform(y_val_s.reshape(-1, 1)).flatten()

    # è®¡ç®—æŒ‡æ ‡
    from sklearn.metrics import r2_score, mean_squared_error
    r2 = r2_score(y_val_original, y_pred_original)
    rmse = np.sqrt(mean_squared_error(y_val_original, y_pred_original))
    cv_rmse = rmse / np.mean(y_val_original)
    nmbe = np.mean(y_pred_original - y_val_original) / np.mean(y_val_original)

    results = {
        'r2': r2,
        'rmse': rmse,
        'cv_rmse': cv_rmse,
        'nmbe': nmbe
    }

    logger.info(f"Fold {fold_idx + 1} è·¨æ¨¡æ€èåˆæŒ‡æ ‡:")
    logger.info(f"  RÂ²: {r2:.4f} (ç›®æ ‡ â‰¥ 0.75)")
    logger.info(f"  CV(RMSE): {cv_rmse:.4f} (ç›®æ ‡ â‰¤ 0.06)")
    logger.info(f"  NMBE: {nmbe:.4f} (ç›®æ ‡ âˆˆ [-0.006, 0.006])")

    # æ¸…ç†GPUå†…å­˜
    torch.cuda.empty_cache()

    return {'overall': results}

def main(config):
    """ä¼˜åŒ–çš„ä¸»æ‰§è¡Œå‡½æ•° - ä½¿ç”¨è®ºæ–‡å®Œæ•´æ¶æ„"""
    start_time = time.time()
    logger.info("=" * 60)
    logger.info("LNGè®ºæ–‡å¤ç°é¡¹ç›® - ä¼˜åŒ–æ¶æ„éªŒè¯")
    logger.info("=" * 60)

    device = setup_device()

    # 1. åŠ è½½ç°å®åŒ–æ•°æ®
    logger.info("\n--- é˜¶æ®µ 1: åŠ è½½ç°å®åŒ–æ•°æ® ---")
    data_path = Path("data/sim_lng/enhanced_simulation_data.csv")
    df = load_data_with_progress(data_path)

    # 2. è®¡ç®—èƒ½è€—
    logger.info("\n--- é˜¶æ®µ 2: èƒ½è€—è®¡ç®— ---")
    power_columns = [
        'booster_pump_power_kw',
        'hp_pump_power_kw',
        'bog_compressor_total_power_kw'
    ]

    df['energy'] = df[power_columns].sum(axis=1)
    logger.info(f"ç°å®åŒ–èƒ½è€—ç»Ÿè®¡: å‡å€¼={df['energy'].mean():.2f} kW, å˜å¼‚ç³»æ•°={df['energy'].std()/df['energy'].mean():.4f}")

    # 3. é«˜çº§ç‰¹å¾å·¥ç¨‹
    logger.info("\n--- é˜¶æ®µ 3: é«˜çº§ç‰¹å¾å·¥ç¨‹ (è®ºæ–‡å®Œæ•´ä½“ç³») ---")
    X_dynamic, X_static, y = extract_features_advanced(df, config)

    del df
    gc.collect()

    # 4. è·¨æ¨¡æ€èåˆè®­ç»ƒ
    logger.info("\n--- é˜¶æ®µ 4: è·¨æ¨¡æ€èåˆäº¤å‰éªŒè¯ ---")
    cv_config = config['validation']
    cv = PurgedWalkForwardCV(
        n_splits=cv_config['n_splits'],
        embargo_size=cv_config['embargo_size']
    )

    all_fold_results = []

    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_dynamic)):
        X_dynamic_train = X_dynamic[train_idx]
        X_static_train = X_static[train_idx]
        y_train = y[train_idx]

        X_dynamic_val = X_dynamic[val_idx]
        X_static_val = X_static[val_idx]
        y_val = y[val_idx]

        logger.info(f"\nè·¨æ¨¡æ€æ•°æ®: è®­ç»ƒ{len(train_idx):,}, éªŒè¯{len(val_idx):,}")

        results = train_fold_cross_modal(
            fold_idx, X_dynamic_train, X_static_train, y_train,
            X_dynamic_val, X_static_val, y_val, config, device
        )

        all_fold_results.append(results['overall'])
        gc.collect()

    # 5. æœ€ç»ˆè¯„ä¼°
    logger.info("\n" + "=" * 60)
    logger.info("--- é˜¶æ®µ 5: è·¨æ¨¡æ€èåˆæœ€ç»ˆæŒ‡æ ‡ ---")
    logger.info("=" * 60)

    avg_metrics = {
        'r2': np.mean([r['r2'] for r in all_fold_results]),
        'cv_rmse': np.mean([r['cv_rmse'] for r in all_fold_results]),
        'nmbe': np.mean([r['nmbe'] for r in all_fold_results]),
    }

    logger.info("\nğŸ“Š è·¨æ¨¡æ€èåˆå­¦æœ¯æŒ‡æ ‡:")
    logger.info("-" * 40)
    logger.info(f"  RÂ² Score:        {avg_metrics['r2']:.4f}  {'âœ…' if avg_metrics['r2'] >= 0.75 else 'âŒ'} (ç›®æ ‡ â‰¥ 0.75)")
    logger.info(f"  CV(RMSE):        {avg_metrics['cv_rmse']:.4f}  {'âœ…' if avg_metrics['cv_rmse'] <= 0.06 else 'âŒ'} (ç›®æ ‡ â‰¤ 0.06)")
    logger.info(f"  NMBE:            {avg_metrics['nmbe']:.4f}  {'âœ…' if -0.006 <= avg_metrics['nmbe'] <= 0.006 else 'âŒ'} (ç›®æ ‡ âˆˆ [-0.006, 0.006])")

    requirements_met = (
        avg_metrics['r2'] >= 0.75 and
        avg_metrics['cv_rmse'] <= 0.06 and
        -0.006 <= avg_metrics['nmbe'] <= 0.006
    )

    if requirements_met:
        logger.info("\nğŸ‰ æ­å–œï¼è·¨æ¨¡æ€èåˆæ¨¡å‹è¾¾åˆ°è®ºæ–‡è¦æ±‚ï¼")
    else:
        logger.info("\nâš ï¸ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–è·¨æ¨¡æ€èåˆæ¶æ„")

    # ä¿å­˜ç»“æœ
    results_dir = Path("results")
    results_dir.mkdir(exist_ok=True)

    final_results = {
        'architecture': 'CrossModalFusion + MLR/GPR',
        'feature_system': '9_categories_dynamic + 32_static',
        'metrics': avg_metrics,
        'all_folds': all_fold_results,
        'requirements_met': requirements_met,
        'timestamp': datetime.now().isoformat()
    }

    results_path = results_dir / f"cross_modal_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.yaml"
    with open(results_path, 'w', encoding='utf-8') as f:
        yaml.dump(final_results, f, allow_unicode=True)

    logger.info(f"\nç»“æœå·²ä¿å­˜è‡³: {results_path}")
    total_time = time.time() - start_time
    logger.info(f"\næ€»è¿è¡Œæ—¶é—´: {total_time/60:.2f} åˆ†é’Ÿ")

    return avg_metrics

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="LNGè®ºæ–‡å¤ç° - è·¨æ¨¡æ€èåˆéªŒè¯")
    parser.add_argument('--config', type=str, default='configs/train.yaml')
    parser.add_argument('--debug', action='store_true')
    args = parser.parse_args()

    config = load_config(args.config)

    if args.debug:
        logger.info("è°ƒè¯•æ¨¡å¼ï¼šä½¿ç”¨ç¼©å‡å‚æ•°")
        config['data_processing']['window_size'] = 60
        config['validation']['n_splits'] = 2

    try:
        metrics = main(config)
        sys.exit(0 if metrics['r2'] >= 0.75 else 1)
    except KeyboardInterrupt:
        logger.info("\nç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        sys.exit(1)
    except Exception as e:
        logger.error(f"æ‰§è¡Œå‡ºé”™: {e}", exc_info=True)
        sys.exit(1)