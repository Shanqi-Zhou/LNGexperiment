#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LNGè®ºæ–‡å¤ç°é¡¹ç›® - æ”¹è¿›çš„ä¸»ç¨‹åº
è§£å†³ç¼–ç é—®é¢˜ï¼Œæ·»åŠ è¿›åº¦æ˜¾ç¤ºï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨
"""

import argparse
import yaml
import pandas as pd
import numpy as np
import torch
import warnings
import os
import sys
import io
import time
import gc
from datetime import datetime
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from torch.utils.data import TensorDataset, DataLoader
import logging

# è®¾ç½®UTF-8ç¼–ç ï¼Œè§£å†³Windows gbkç¼–ç é—®é¢˜
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from src.features.advanced_feature_engineering import AdvancedFeatureEngineering
from src.models.adaptive_strategy import create_adaptive_model
from src.training.purged_validation import PurgedWalkForwardCV
from src.eval.evaluator import ComprehensiveEvaluator

def setup_device():
    """é…ç½®GPUè®¾å¤‡"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    if torch.cuda.is_available():
        logger.info(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
        logger.info(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        # è®¾ç½®æ··åˆç²¾åº¦ä»¥é€‚é…RTX 4060 8GB
        torch.backends.cudnn.benchmark = True
        torch.cuda.empty_cache()
    else:
        logger.info("ä½¿ç”¨CPUè®­ç»ƒ")
    return device

def load_config(path):
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    with open(path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def load_data_with_progress(data_path):
    """åŠ è½½æ•°æ®å¹¶æ˜¾ç¤ºè¿›åº¦"""
    logger.info(f"åŠ è½½æ•°æ®: {data_path}")
    start_time = time.time()

    # åˆ†å—è¯»å–å¤§æ–‡ä»¶
    chunk_size = 100000
    chunks = []
    total_rows = 0

    try:
        # å…ˆè·å–æ€»è¡Œæ•°
        with open(data_path, 'r', encoding='utf-8') as f:
            total_lines = sum(1 for _ in f) - 1  # å‡å»header

        logger.info(f"æ•°æ®é›†æ€»è¡Œæ•°: {total_lines:,}")

        # åˆ†å—è¯»å–
        for chunk in pd.read_csv(data_path, parse_dates=['ts'], chunksize=chunk_size):
            chunks.append(chunk)
            total_rows += len(chunk)
            progress = (total_rows / total_lines) * 100
            logger.info(f"  åŠ è½½è¿›åº¦: {progress:.1f}% ({total_rows:,}/{total_lines:,} è¡Œ)")

        df = pd.concat(chunks, ignore_index=True)
        elapsed = time.time() - start_time
        logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: {len(df):,} è¡Œ, è€—æ—¶ {elapsed:.2f} ç§’")

        # å†…å­˜ä¼˜åŒ–ï¼šè½¬æ¢æ•°æ®ç±»å‹
        for col in df.columns:
            if df[col].dtype == 'float64':
                df[col] = df[col].astype('float32')

        return df

    except FileNotFoundError:
        logger.error(f"æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {data_path}")
        raise
    except Exception as e:
        logger.error(f"åŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}")
        raise

def extract_features_optimized(df, config):
    """ä¼˜åŒ–çš„ç‰¹å¾æå–"""
    logger.info("å¼€å§‹ç‰¹å¾æå–...")
    feature_config = config['data_processing']

    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    logger.info(f"  çª—å£å¤§å°: {feature_config['window_size']}")
    logger.info(f"  æ­¥é•¿: {feature_config['stride']}")

    # è®¡ç®—é¢„æœŸçª—å£æ•°
    expected_windows = (len(df) - feature_config['window_size']) // feature_config['stride'] + 1
    logger.info(f"  é¢„æœŸçª—å£æ•°: {expected_windows:,}")

    # ç‰¹å¾æå–
    feature_engine = AdvancedFeatureEngineering(
        window_size=feature_config['window_size'],
        stride=feature_config['stride']
    )

    # åˆ†æ‰¹å¤„ç†ä»¥å‡å°‘å†…å­˜å‹åŠ›
    batch_size = 50000
    n_batches = (len(df) - 1) // batch_size + 1

    feature_batches = []
    label_batches = []

    for i in range(n_batches):
        start_idx = i * batch_size
        end_idx = min((i + 1) * batch_size + feature_config['window_size'], len(df))

        batch_df = df.iloc[start_idx:end_idx]

        if len(batch_df) >= feature_config['window_size']:
            # æå–ç‰¹å¾
            X_dynamic, X_static = feature_engine.extract_features_from_window(batch_df.drop(columns=['ts', 'energy']))

            # æå–æ ‡ç­¾
            batch_labels = batch_df['energy'].rolling(
                feature_config['window_size']
            ).mean().iloc[::feature_config['stride']]

            feature_batches.append(batch_features)
            label_batches.append(batch_labels)

            logger.info(f"  æ‰¹æ¬¡ {i+1}/{n_batches} å®Œæˆ ({(i+1)/n_batches*100:.1f}%)")

            # æ¸…ç†å†…å­˜
            gc.collect()

    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
    feature_df = pd.concat(feature_batches, ignore_index=True)
    labels = pd.concat(label_batches, ignore_index=True)

    # æ¸…ç†NaN
    labels = labels.dropna()

    # å¯¹é½é•¿åº¦
    min_len = min(len(feature_df), len(labels))
    X = feature_df.iloc[:min_len].values
    y = labels.iloc[:min_len].values

    logger.info(f"ç‰¹å¾æå–å®Œæˆ: X shape={X.shape}, y shape={y.shape}")

    return X, y

def train_fold_with_checkpoint(fold_idx, X_train, y_train, X_val, y_val, config, device):
    """è®­ç»ƒå•ä¸ªfoldå¹¶ä¿å­˜checkpoint"""
    logger.info(f"\n===== å¼€å§‹è®­ç»ƒ FOLD {fold_idx + 1} =====")

    # æ•°æ®æ ‡å‡†åŒ–
    x_scaler = StandardScaler().fit(X_train)
    y_scaler = StandardScaler().fit(y_train.reshape(-1, 1))

    X_train_s = x_scaler.transform(X_train).astype(np.float32)
    y_train_s = y_scaler.transform(y_train.reshape(-1, 1)).flatten().astype(np.float32)
    X_val_s = x_scaler.transform(X_val).astype(np.float32)
    y_val_s = y_scaler.transform(y_val.reshape(-1, 1)).flatten().astype(np.float32)

    # åˆ›å»ºè‡ªé€‚åº”æ¨¡å‹
    n_samples = len(X_train)
    model = create_adaptive_model(n_samples)

    # è®­ç»ƒæ¨¡å‹
    results = {}
    if isinstance(model, torch.nn.Module):
        # PyTorchæ¨¡å‹
        model = model.to(device)

        # å‡†å¤‡æ•°æ®é›†
        train_dataset = TensorDataset(
            torch.FloatTensor(X_train_s),
            torch.FloatTensor(y_train_s).unsqueeze(1)
        )
        val_dataset = TensorDataset(
            torch.FloatTensor(X_val_s),
            torch.FloatTensor(y_val_s).unsqueeze(1)
        )

        # è°ƒæ•´batch_sizeä»¥é€‚åº”RTX 4060
        batch_size = min(config['training_loop']['batch_size'], 32)

        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            pin_memory=True,
            num_workers=2
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            pin_memory=True,
            num_workers=2
        )

        # è®­ç»ƒ
        trainer = UnifiedTrainer(model, train_loader, val_loader, config['training_loop'])
        trainer.train()

        # è¯„ä¼°
        model.eval()
        with torch.no_grad():
            X_val_tensor = torch.FloatTensor(X_val_s).to(device)
            y_pred_s = model(X_val_tensor).cpu().numpy()

        # æ¸…ç†GPUå†…å­˜
        if device.type == 'cuda':
            torch.cuda.empty_cache()

    else:
        # Scikit-learnæ¨¡å‹
        logger.info(f"è®­ç»ƒ {type(model).__name__} æ¨¡å‹...")
        model.fit(X_train_s, y_train_s)
        y_pred_s = model.predict(X_val_s)

    # è¯„ä¼°ç»“æœ
    evaluator = ComprehensiveEvaluator(target_scaler=y_scaler)
    results = evaluator.evaluate(y_val_s, y_pred_s.flatten())

    # æ˜¾ç¤ºæ ¸å¿ƒå­¦æœ¯æŒ‡æ ‡
    logger.info(f"Fold {fold_idx + 1} å­¦æœ¯æŒ‡æ ‡:")
    logger.info(f"  RÂ²: {results['overall']['r2']:.4f} (ç›®æ ‡ â‰¥ 0.75)")
    logger.info(f"  CV(RMSE): {results['overall']['cv_rmse']:.4f} (ç›®æ ‡ â‰¤ 0.06)")
    logger.info(f"  NMBE: {results['overall']['nmbe']:.4f} (ç›®æ ‡ âˆˆ [-0.006, 0.006])")

    # ä¿å­˜checkpoint
    checkpoint_dir = Path("checkpoints")
    checkpoint_dir.mkdir(exist_ok=True)

    checkpoint = {
        'fold': fold_idx,
        'model_state': model.state_dict() if isinstance(model, torch.nn.Module) else None,
        'results': results,
        'x_scaler': x_scaler,
        'y_scaler': y_scaler,
        'timestamp': datetime.now().isoformat()
    }

    checkpoint_path = checkpoint_dir / f"fold_{fold_idx}_checkpoint.pt"
    torch.save(checkpoint, checkpoint_path)
    logger.info(f"  Checkpoint ä¿å­˜è‡³: {checkpoint_path}")

    return results

def main(config):
    """æ”¹è¿›çš„ä¸»æ‰§è¡Œå‡½æ•°"""
    start_time = time.time()
    logger.info("=" * 60)
    logger.info("LNGè®ºæ–‡å¤ç°é¡¹ç›® - å­¦æœ¯æŒ‡æ ‡éªŒè¯")
    logger.info("=" * 60)

    # é…ç½®è®¾å¤‡
    device = setup_device()

    # 1. åŠ è½½æ•°æ®
    logger.info("\n--- é˜¶æ®µ 1: æ•°æ®åŠ è½½ ---")
    data_path = Path("data/sim_lng/enhanced_simulation_data.csv")
    df = load_data_with_progress(data_path)

    # 2. è®¡ç®—èƒ½è€—
    logger.info("\n--- é˜¶æ®µ 2: èƒ½è€—è®¡ç®— ---")
    power_columns = [
        'booster_pump_power_kw',
        'hp_pump_power_kw',
        'bog_compressor_total_power_kw'
    ]

    # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
    missing_cols = [col for col in power_columns if col not in df.columns]
    if missing_cols:
        logger.error(f"ç¼ºå¤±åŠŸè€—åˆ—: {missing_cols}")
        raise ValueError(f"æ•°æ®é›†ç¼ºå¤±å¿…éœ€çš„åŠŸè€—åˆ—")

    df['energy'] = df[power_columns].sum(axis=1)
    logger.info(f"èƒ½è€—è®¡ç®—å®Œæˆ: å¹³å‡å€¼ = {df['energy'].mean():.2f} kW")

    # 3. ç‰¹å¾æå–
    logger.info("\n--- é˜¶æ®µ 3: ç‰¹å¾å·¥ç¨‹ ---")
    X, y = extract_features_optimized(df, config)

    # é‡Šæ”¾åŸå§‹æ•°æ®å†…å­˜
    del df
    gc.collect()

    # 4. äº¤å‰éªŒè¯è®­ç»ƒ
    logger.info("\n--- é˜¶æ®µ 4: äº¤å‰éªŒè¯ä¸æ¨¡å‹è®­ç»ƒ ---")
    cv_config = config['validation']
    cv = PurgedWalkForwardCV(
        n_splits=cv_config['n_splits'],
        embargo_size=cv_config['embargo_size']
    )

    all_fold_results = []

    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X)):
        X_train, y_train = X[train_idx], y[train_idx]
        X_val, y_val = X[val_idx], y[val_idx]

        logger.info(f"\nè®­ç»ƒé›†å¤§å°: {len(train_idx):,}, éªŒè¯é›†å¤§å°: {len(val_idx):,}")

        # è®­ç»ƒå¹¶è¯„ä¼°
        results = train_fold_with_checkpoint(
            fold_idx, X_train, y_train, X_val, y_val, config, device
        )

        all_fold_results.append(results['overall'])

        # æ¸…ç†å†…å­˜
        gc.collect()
        if device.type == 'cuda':
            torch.cuda.empty_cache()

    # 5. æœ€ç»ˆè¯„ä¼°
    logger.info("\n" + "=" * 60)
    logger.info("--- é˜¶æ®µ 5: æœ€ç»ˆå­¦æœ¯æŒ‡æ ‡è¯„ä¼° ---")
    logger.info("=" * 60)

    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_metrics = {
        'r2': np.mean([r['r2'] for r in all_fold_results]),
        'cv_rmse': np.mean([r['cv_rmse'] for r in all_fold_results]),
        'nmbe': np.mean([r.get('nmbe', 0) for r in all_fold_results]),
        'mape': np.mean([r.get('mape', 0) for r in all_fold_results])
    }

    # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
    logger.info("\nğŸ“Š å­¦æœ¯æŒ‡æ ‡éªŒè¯ç»“æœ:")
    logger.info("-" * 40)
    logger.info(f"  RÂ² Score:        {avg_metrics['r2']:.4f}  {'âœ…' if avg_metrics['r2'] >= 0.75 else 'âŒ'} (ç›®æ ‡ â‰¥ 0.75)")
    logger.info(f"  CV(RMSE):        {avg_metrics['cv_rmse']:.4f}  {'âœ…' if avg_metrics['cv_rmse'] <= 0.06 else 'âŒ'} (ç›®æ ‡ â‰¤ 0.06)")
    logger.info(f"  NMBE:            {avg_metrics['nmbe']:.4f}  {'âœ…' if -0.006 <= avg_metrics['nmbe'] <= 0.006 else 'âŒ'} (ç›®æ ‡ âˆˆ [-0.006, 0.006])")
    logger.info(f"  MAPE:            {avg_metrics['mape']:.4f}")
    logger.info("-" * 40)

    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è®ºæ–‡è¦æ±‚
    requirements_met = (
        avg_metrics['r2'] >= 0.75 and
        avg_metrics['cv_rmse'] <= 0.06 and
        -0.006 <= avg_metrics['nmbe'] <= 0.006
    )

    if requirements_met:
        logger.info("\nğŸ‰ æ­å–œï¼æ‰€æœ‰å­¦æœ¯æŒ‡æ ‡å‡è¾¾åˆ°è®ºæ–‡è¦æ±‚ï¼")
    else:
        logger.info("\nâš ï¸ éƒ¨åˆ†æŒ‡æ ‡æœªè¾¾åˆ°è®ºæ–‡è¦æ±‚ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")

    # ä¿å­˜æœ€ç»ˆç»“æœ
    results_dir = Path("results")
    results_dir.mkdir(exist_ok=True)

    final_results = {
        'metrics': avg_metrics,
        'all_folds': all_fold_results,
        'requirements_met': requirements_met,
        'timestamp': datetime.now().isoformat()
    }

    results_path = results_dir / f"academic_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.yaml"
    with open(results_path, 'w', encoding='utf-8') as f:
        yaml.dump(final_results, f, allow_unicode=True)

    logger.info(f"\nç»“æœå·²ä¿å­˜è‡³: {results_path}")

    # æ˜¾ç¤ºæ€»è€—æ—¶
    total_time = time.time() - start_time
    logger.info(f"\næ€»è¿è¡Œæ—¶é—´: {total_time/60:.2f} åˆ†é’Ÿ")

    return avg_metrics

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="LNGè®ºæ–‡å¤ç° - å­¦æœ¯æŒ‡æ ‡éªŒè¯")
    parser.add_argument('--config', type=str, default='configs/train.yaml',
                      help='è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--debug', action='store_true',
                      help='è°ƒè¯•æ¨¡å¼ï¼Œä½¿ç”¨å°æ•°æ®é›†')
    args = parser.parse_args()

    # åŠ è½½é…ç½®
    config = load_config(args.config)

    # è°ƒè¯•æ¨¡å¼ä¸‹å‡å°æ•°æ®è§„æ¨¡
    if args.debug:
        logger.info("è°ƒè¯•æ¨¡å¼ï¼šä½¿ç”¨ç¼©å‡çš„æ•°æ®é›†")
        config['data_processing']['window_size'] = 60
        config['data_processing']['stride'] = 30
        config['validation']['n_splits'] = 2
        config['training_loop']['epochs'] = 5

    try:
        # è¿è¡Œä¸»ç¨‹åº
        metrics = main(config)

        # è¿”å›æˆåŠŸçŠ¶æ€
        sys.exit(0 if metrics['r2'] >= 0.75 else 1)

    except KeyboardInterrupt:
        logger.info("\nç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        sys.exit(1)
    except Exception as e:
        logger.error(f"æ‰§è¡Œå‡ºé”™: {e}", exc_info=True)
        sys.exit(1)