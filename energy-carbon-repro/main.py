#!/usr/bin/env python3
"""
LNGè¿‡æ‹Ÿåˆä¿®æ­£å®éªŒ
ç§»é™¤æ•°æ®æ³„éœ²ç‰¹å¾ï¼Œè¿›è¡Œå…¬å¹³çš„æ¨¡å‹è¯„ä¼°
"""

import pandas as pd
import numpy as np
import logging
import time
import json
from pathlib import Path
import warnings
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import sys

warnings.filterwarnings('ignore')
sys.path.insert(0, 'src')

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def identify_leaky_features(X, y, threshold=0.99):
    """è¯†åˆ«å¯èƒ½å­˜åœ¨æ•°æ®æ³„éœ²çš„ç‰¹å¾"""
    logger = setup_logging()
    
    # è®¡ç®—ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§
    correlations = X.corrwith(y).abs().sort_values(ascending=False)
    leaky_features = correlations[correlations > threshold].index.tolist()
    
    logger.info(f"å‘ç° {len(leaky_features)} ä¸ªå¯èƒ½å­˜åœ¨æ•°æ®æ³„éœ²çš„ç‰¹å¾ (|r| > {threshold}):")
    for i, feature in enumerate(leaky_features, 1):
        logger.info(f"  {i}. {feature}: {correlations[feature]:.6f}")
    
    return leaky_features, correlations

def clean_experiment():
    """æ¸…æ´ç‰ˆæœ¬çš„å®éªŒï¼Œç§»é™¤æ•°æ®æ³„éœ²ç‰¹å¾"""
    logger = setup_logging()
    
    logger.info("ğŸ§¹ å¼€å§‹æ¸…æ´ç‰ˆLNGå®éªŒ - ç§»é™¤æ•°æ®æ³„éœ²ç‰¹å¾")
    logger.info("=" * 80)
    
    # åŠ è½½æ•°æ®
    data = pd.read_csv('data/sim_lng/full_simulation_data.csv', low_memory=False)
    numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()
    data_clean = data[numeric_columns].copy()
    data_clean = data_clean.fillna(data_clean.median())
    
    target_col = 'orv_Q_MW'
    X = data_clean.drop(columns=[target_col])
    y = data_clean[target_col]
    
    logger.info(f"åŸå§‹æ•°æ®: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
    
    # è¯†åˆ«æ³„éœ²ç‰¹å¾
    leaky_features, correlations = identify_leaky_features(X, y, threshold=0.99)
    
    # ç§»é™¤æ³„éœ²ç‰¹å¾
    X_clean = X.drop(columns=leaky_features)
    logger.info(f"ç§»é™¤ {len(leaky_features)} ä¸ªæ³„éœ²ç‰¹å¾å: {X_clean.shape[1]} ç‰¹å¾")
    
    # è¿›ä¸€æ­¥ç§»é™¤é«˜ç›¸å…³ç‰¹å¾ (0.95-0.99)
    high_corr_features = []
    remaining_correlations = X_clean.corrwith(y).abs().sort_values(ascending=False)
    
    for feature, corr in remaining_correlations.items():
        if corr > 0.95:
            high_corr_features.append(feature)
    
    if len(high_corr_features) > 0:
        logger.info(f"\nå‘ç° {len(high_corr_features)} ä¸ªé«˜ç›¸å…³ç‰¹å¾ (0.95 < |r| < 0.99):")
        for feature in high_corr_features[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            logger.info(f"  - {feature}: {remaining_correlations[feature]:.4f}")
        
        # å¯é€‰ï¼šç§»é™¤ä¸€äº›é«˜ç›¸å…³ç‰¹å¾
        # è¿™é‡Œæˆ‘ä»¬ä¿ç•™å®ƒä»¬ï¼Œä½†åœ¨æ—¥å¿—ä¸­æ ‡è®°
        logger.info("ä¿ç•™é«˜ç›¸å…³ç‰¹å¾ä»¥è¿›è¡Œå¯¹æ¯”åˆ†æ")
    
    # æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X_clean, y, test_size=0.2, random_state=42, shuffle=True
    )
    
    logger.info(f"\næ•°æ®åˆ†å‰²: è®­ç»ƒé›† {X_train.shape}, æµ‹è¯•é›† {X_test.shape}")
    
    # æµ‹è¯•å¤šç§æ¨¡å‹
    results = []
    
    # 1. è‡ªé€‚åº”HGBR
    logger.info("\n1. æµ‹è¯•è‡ªé€‚åº”HGBR (æ¸…æ´ç‰¹å¾)...")
    start_time = time.time()
    
    from src.models.hgbr_baseline import AdaptiveHGBRBaseline
    model_hgbr = AdaptiveHGBRBaseline(logger=logger)
    model_hgbr.fit(X_train.values, y_train.values)
    
    y_pred_train_hgbr = model_hgbr.predict(X_train.values)
    y_pred_test_hgbr = model_hgbr.predict(X_test.values)
    
    hgbr_time = time.time() - start_time
    
    results.append({
        'model': 'HGBR_Clean',
        'train_r2': r2_score(y_train, y_pred_train_hgbr),
        'test_r2': r2_score(y_test, y_pred_test_hgbr),
        'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train_hgbr)),
        'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test_hgbr)),
        'training_time': hgbr_time,
        'features_used': X_clean.shape[1],
        'best_config': model_hgbr.best_model_name
    })
    
    # 2. éšæœºæ£®æ—
    logger.info("2. æµ‹è¯•éšæœºæ£®æ— (æ¸…æ´ç‰¹å¾)...")
    start_time = time.time()
    
    from sklearn.ensemble import RandomForestRegressor
    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
    rf.fit(X_train, y_train)
    
    y_pred_train_rf = rf.predict(X_train)
    y_pred_test_rf = rf.predict(X_test)
    
    rf_time = time.time() - start_time
    
    results.append({
        'model': 'RandomForest_Clean',
        'train_r2': r2_score(y_train, y_pred_train_rf),
        'test_r2': r2_score(y_test, y_pred_test_rf),
        'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train_rf)),
        'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test_rf)),
        'training_time': rf_time,
        'features_used': X_clean.shape[1],
        'best_config': 'n_estimators=100'
    })
    
    # 3. Ridgeå›å½’ (åŸºçº¿)
    logger.info("3. æµ‹è¯•Ridgeå›å½’ (æ¸…æ´ç‰¹å¾)...")
    start_time = time.time()
    
    from sklearn.linear_model import Ridge
    from sklearn.preprocessing import StandardScaler
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    ridge = Ridge(alpha=1.0, random_state=42)
    ridge.fit(X_train_scaled, y_train)
    
    y_pred_train_ridge = ridge.predict(X_train_scaled)
    y_pred_test_ridge = ridge.predict(X_test_scaled)
    
    ridge_time = time.time() - start_time
    
    results.append({
        'model': 'Ridge_Clean',
        'train_r2': r2_score(y_train, y_pred_train_ridge),
        'test_r2': r2_score(y_test, y_pred_test_ridge),
        'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train_ridge)),
        'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test_ridge)),
        'training_time': ridge_time,
        'features_used': X_clean.shape[1],
        'best_config': 'alpha=1.0'
    })
    
    # è¾“å‡ºç»“æœ
    logger.info("\n" + "=" * 80)
    logger.info("æ¸…æ´å®éªŒç»“æœ (ç§»é™¤æ•°æ®æ³„éœ²ç‰¹å¾)")
    logger.info("=" * 80)
    
    df_results = pd.DataFrame(results)
    df_results['overfitting_ratio'] = df_results['test_rmse'] / df_results['train_rmse']
    df_results['performance_drop'] = df_results['train_r2'] - df_results['test_r2']
    
    logger.info("æ¨¡å‹æ€§èƒ½å¯¹æ¯” (è®­ç»ƒé›† vs æµ‹è¯•é›†):")
    logger.info("=" * 100)
    logger.info(f"{'æ¨¡å‹':<15} {'è®­ç»ƒRÂ²':<8} {'æµ‹è¯•RÂ²':<8} {'è®­ç»ƒRMSE':<10} {'æµ‹è¯•RMSE':<10} {'è¿‡æ‹Ÿåˆæ¯”':<8} {'æ€§èƒ½ä¸‹é™':<8}")
    logger.info("-" * 100)
    
    for _, row in df_results.iterrows():
        logger.info(f"{row['model']:<15} {row['train_r2']:<8.4f} {row['test_r2']:<8.4f} "
                   f"{row['train_rmse']:<10.6f} {row['test_rmse']:<10.6f} "
                   f"{row['overfitting_ratio']:<8.3f} {row['performance_drop']:<8.4f}")
    
    # è¿‡æ‹Ÿåˆåˆ†æ
    logger.info("\nè¿‡æ‹Ÿåˆé£é™©è¯„ä¼°:")
    for _, row in df_results.iterrows():
        if row['overfitting_ratio'] > 2.0:
            risk = "ä¸¥é‡è¿‡æ‹Ÿåˆ âŒ"
        elif row['overfitting_ratio'] > 1.5:
            risk = "ä¸­åº¦è¿‡æ‹Ÿåˆ âš ï¸"
        elif row['overfitting_ratio'] > 1.2:
            risk = "è½»å¾®è¿‡æ‹Ÿåˆ ?"
        else:
            risk = "æ³›åŒ–è‰¯å¥½ âœ…"
        
        logger.info(f"  {row['model']}: {risk} (æµ‹è¯•/è®­ç»ƒRMSE = {row['overfitting_ratio']:.3f})")
    
    # ä¿å­˜ç»“æœ
    output_dir = Path('results')
    
    # ä¿å­˜æ¸…æ´å®éªŒç»“æœ
    clean_results = {
        'removed_leaky_features': leaky_features,
        'feature_correlations': correlations.to_dict(),
        'model_results': df_results.to_dict('records'),
        'summary': {
            'original_features': X.shape[1],
            'clean_features': X_clean.shape[1],
            'removed_features': len(leaky_features),
            'best_model': df_results.loc[df_results['test_r2'].idxmax(), 'model'],
            'best_test_r2': df_results['test_r2'].max()
        }
    }
    
    with open(output_dir / 'clean_experiment_results.json', 'w', encoding='utf-8') as f:
        json.dump(clean_results, f, indent=2, ensure_ascii=False)
    
    df_results.to_csv(output_dir / 'clean_model_comparison.csv', index=False)
    
    logger.info("\n" + "=" * 80)
    logger.info("æ¸…æ´å®éªŒæ€»ç»“")
    logger.info("=" * 80)
    logger.info(f"ç§»é™¤ç‰¹å¾: {len(leaky_features)} ä¸ªæ•°æ®æ³„éœ²ç‰¹å¾")
    logger.info(f"ä½¿ç”¨ç‰¹å¾: {X_clean.shape[1]} ä¸ªæ¸…æ´ç‰¹å¾")
    logger.info(f"æœ€ä½³æ¨¡å‹: {clean_results['summary']['best_model']}")
    logger.info(f"æœ€ä½³æµ‹è¯•RÂ²: {clean_results['summary']['best_test_r2']:.4f}")
    logger.info(f"ç»“æœä¿å­˜: {output_dir}/clean_experiment_results.json")
    
    return clean_results

if __name__ == '__main__':
    results = clean_experiment()