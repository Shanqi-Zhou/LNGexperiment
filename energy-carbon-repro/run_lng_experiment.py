#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LNGè®ºæ–‡å¤ç°é¡¹ç›® - ç»Ÿä¸€å…¥å£
LNG Paper Reproduction Project - Unified Entry Point

å•ä¸€å…¥å£æ–‡ä»¶é›†æˆæ‰€æœ‰ä¼˜åŒ–ï¼š
- ç°å®åŒ–æ•°æ® + é«˜æ€§èƒ½ç‰¹å¾å·¥ç¨‹ + è·¨æ¨¡æ€èåˆ + GPUåŠ é€Ÿ
"""

import sys
import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import logging
import time
import gc
import yaml
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import train_test_split
from datetime import datetime

# è®¾ç½®UTF-8ç¼–ç 
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('lng_experiment.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class LNGExperiment:
    """LNGè®ºæ–‡å¤ç°å®éªŒä¸»ç±»"""

    def __init__(self, random_seed=42):
        """åˆå§‹åŒ–å®éªŒç¯å¢ƒ"""
        # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
        self.random_seed = random_seed
        np.random.seed(random_seed)
        torch.manual_seed(random_seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(random_seed)
            torch.cuda.manual_seed_all(random_seed)
        logger.info(f"éšæœºç§å­è®¾ç½®ä¸º: {random_seed}")
        
        # GPUè®¾ç½®
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info(f"GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
            logger.info(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        else:
            logger.info("ä½¿ç”¨CPUæ¨¡å¼")

    def load_enhanced_data(self):
        """åŠ è½½ç°å®åŒ–æ•°æ®"""
        logger.info("åŠ è½½ç°å®åŒ–LNGæ•°æ®...")

        # æ£€æŸ¥æ•°æ®æ–‡ä»¶
        data_path = Path("data/sim_lng/anti_overfitting_data.csv")
        if not data_path.exists():
            # å¦‚æœæ²¡æœ‰å¢å¼ºæ•°æ®ï¼Œç”Ÿæˆå®ƒ
            logger.info("å¢å¼ºæ•°æ®ä¸å­˜åœ¨ï¼Œæ­£åœ¨ç”Ÿæˆ...")
            self.generate_enhanced_data()

        # åŠ è½½æ•°æ®
        df = pd.read_csv(data_path, parse_dates=['ts'])
        power_columns = ['booster_pump_power_kw', 'hp_pump_power_kw', 'bog_compressor_total_power_kw']
        df['energy'] = df[power_columns].sum(axis=1)

        logger.info(f"æ•°æ®è§„æ¨¡: {len(df):,} è¡Œ")
        logger.info(f"èƒ½è€—ç‰¹æ€§: å‡å€¼={df['energy'].mean():.2f} kW, CV={df['energy'].std()/df['energy'].mean():.4f}")

        return df

    def generate_enhanced_data(self):
        """ç”Ÿæˆç°å®åŒ–å¢å¼ºæ•°æ®"""
        logger.info("å¼€å§‹ç”Ÿæˆç°å®åŒ–æ•°æ®...")

        # æ£€æŸ¥åŸå§‹æ•°æ®
        original_path = Path("data/sim_lng/full_simulation_data.csv")
        if not original_path.exists():
            raise FileNotFoundError(f"åŸå§‹æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {original_path}")

        # ä½¿ç”¨å†…ç½®çš„æ•°æ®å¢å¼ºé€»è¾‘
        df = pd.read_csv(original_path, parse_dates=['ts'])
        enhanced_df = self.apply_realistic_enhancement(df)

        # ä¿å­˜å¢å¼ºæ•°æ®
        output_path = Path("data/sim_lng/anti_overfitting_data.csv")
        enhanced_df.to_csv(output_path, index=False)
        logger.info(f"ç°å®åŒ–æ•°æ®å·²ç”Ÿæˆ: {output_path}")

    def apply_realistic_enhancement(self, df):
        """åº”ç”¨ç°å®åŒ–å¢å¼º"""
        # é‡æ–°è®¾ç½®éšæœºç§å­ç¡®ä¿æ•°æ®å¢å¼ºçš„ä¸€è‡´æ€§
        np.random.seed(self.random_seed + 1)
        
        enhanced_df = df.copy()
        n_samples = len(df)

        # 1. èˆ¹èˆ¶è£…å¸å½±å“ (æ¯10å¤©ä¸€æ¬¡)
        ship_schedule = np.ones(n_samples)
        for i in range(0, n_samples, 1440):  # æ¯10å¤©
            loading_duration = np.random.randint(36, 72)  # 6-12å°æ—¶
            end_idx = min(i + loading_duration, n_samples)
            ship_schedule[i:end_idx] = np.random.uniform(2.0, 4.0)

        # 2. æ—¥å‘¨æœŸå’Œå­£èŠ‚å˜åŒ–
        hours = np.arange(n_samples) / 6
        daily_variation = 1.0 + 0.3 * np.sin(2 * np.pi * hours / 24)

        days = np.arange(n_samples) / 144
        seasonal_variation = 1.0 + 0.2 * np.sin(2 * np.pi * days / 365)

        # 3. è®¾å¤‡åŠ¨æ€å’Œå™ªå£°
        efficiency_variation = np.random.normal(1.0, 0.08, n_samples)
        measurement_noise = np.random.normal(1.0, 0.02, n_samples)

        # åº”ç”¨æ‰€æœ‰å˜åŒ–
        total_variation = (ship_schedule * daily_variation *
                          seasonal_variation * efficiency_variation * measurement_noise)

        enhanced_df['booster_pump_power_kw'] *= total_variation
        enhanced_df['hp_pump_power_kw'] *= total_variation
        enhanced_df['bog_compressor_total_power_kw'] *= np.random.normal(1.0, 0.15, n_samples)

        return enhanced_df

    def extract_features_fast(self, df):
        """é«˜æ€§èƒ½ç‰¹å¾æå–"""
        logger.info("å¼€å§‹é«˜æ€§èƒ½ç‰¹å¾æå–...")

        window_size, stride = 180, 30
        feature_df = df.drop(columns=['ts', 'energy'])

        # ä½¿ç”¨æ»‘åŠ¨çª—å£åˆ›å»ºè§†å›¾
        from numpy.lib.stride_tricks import sliding_window_view
        data = feature_df.values
        windows = sliding_window_view(data, (window_size, data.shape[1]))
        windows = windows[::stride, 0, :, :]

        # å¿«é€Ÿç»Ÿè®¡ç‰¹å¾è®¡ç®—
        features_list = []
        for feature_idx in range(data.shape[1]):
            window_data = windows[:, :, feature_idx]

            # åŸºç¡€ç»Ÿè®¡
            feat = np.column_stack([
                np.mean(window_data, axis=1),
                np.std(window_data, axis=1),
                np.min(window_data, axis=1),
                np.max(window_data, axis=1),
                np.median(window_data, axis=1)
            ])
            features_list.append(feat)

        X = np.concatenate(features_list, axis=1)

        # ç”Ÿæˆå¯¹åº”æ ‡ç­¾
        y_list = []
        for i in range(0, len(df) - window_size + 1, stride):
            y_list.append(df['energy'].iloc[i:i+window_size].mean())
        y = np.array(y_list[:len(X)])

        logger.info(f"ç‰¹å¾æå–å®Œæˆ: {X.shape}, æ ‡ç­¾: {y.shape}")
        return X, y

    def create_cross_modal_model(self, input_dim, hidden_dim=128):
        """åˆ›å»ºç®€åŒ–çš„è·¨æ¨¡æ€èåˆæ¨¡å‹"""
        class SimpleCrossModalFusion(nn.Module):
            def __init__(self, input_dim, hidden_dim):
                super().__init__()
                self.input_projection = nn.Linear(input_dim, hidden_dim)
                self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, batch_first=True)
                self.feedforward = nn.Sequential(
                    nn.Linear(hidden_dim, hidden_dim * 2),
                    nn.ReLU(),
                    nn.Dropout(0.1),
                    nn.Linear(hidden_dim * 2, hidden_dim)
                )
                self.output_head = nn.Linear(hidden_dim, 1)
                self.layer_norm = nn.LayerNorm(hidden_dim)

            def forward(self, x):
                # x shape: [batch, features]
                x = self.input_projection(x)  # [batch, hidden_dim]
                x = x.unsqueeze(1)  # [batch, 1, hidden_dim] for attention

                # Self-attention
                attn_out, _ = self.attention(x, x, x)
                x = self.layer_norm(x + attn_out)

                # Feedforward
                ff_out = self.feedforward(x)
                x = self.layer_norm(x + ff_out)

                # Output
                output = self.output_head(x.squeeze(1))  # [batch, 1]
                return output

        return SimpleCrossModalFusion(input_dim, hidden_dim)

    def train_model(self, X, y):
        """è®­ç»ƒè·¨æ¨¡æ€èåˆæ¨¡å‹"""
        logger.info("å¼€å§‹è·¨æ¨¡æ€èåˆæ¨¡å‹è®­ç»ƒ...")
        
        # é‡æ–°è®¾ç½®éšæœºç§å­ç¡®ä¿è®­ç»ƒè¿‡ç¨‹çš„ä¸€è‡´æ€§
        np.random.seed(self.random_seed + 2)
        torch.manual_seed(self.random_seed + 2)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(self.random_seed + 2)

        # æ—¶é—´åºåˆ—æ„ŸçŸ¥çš„éªŒè¯åˆ†å‰²ï¼ˆé˜²æ­¢æ•°æ®æ³„æ¼ï¼‰
        # ä½¿ç”¨æ—¶é—´åºåˆ—åˆ†å‰²è€Œééšæœºåˆ†å‰²
        split_point = int(len(X) * 0.8)  # å‰80%ä½œä¸ºè®­ç»ƒï¼Œå20%ä½œä¸ºéªŒè¯
        X_train, X_val = X[:split_point], X[split_point:]
        y_train, y_val = y[:split_point], y[split_point:]

        logger.info(f"æ—¶é—´åºåˆ—åˆ†å‰²: è®­ç»ƒé›†{len(X_train)}æ ·æœ¬, éªŒè¯é›†{len(X_val)}æ ·æœ¬")

        # æ ‡å‡†åŒ–
        scaler_X = StandardScaler().fit(X_train)
        scaler_y = StandardScaler().fit(y_train.reshape(-1, 1))

        X_train_s = scaler_X.transform(X_train).astype(np.float32)
        X_val_s = scaler_X.transform(X_val).astype(np.float32)
        y_train_s = scaler_y.transform(y_train.reshape(-1, 1)).flatten().astype(np.float32)
        y_val_s = scaler_y.transform(y_val.reshape(-1, 1)).flatten().astype(np.float32)

        # åˆ›å»ºæ¨¡å‹
        model = self.create_cross_modal_model(input_dim=X_train_s.shape[1]).to(self.device)
        logger.info(f"æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")

        # è®­ç»ƒè®¾ç½®
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.MSELoss()
        batch_size = 512  # RTX 4060é€‚é…

        # è®­ç»ƒå¾ªç¯
        model.train()
        for epoch in range(30):
            epoch_loss = 0
            n_batches = 0

            # åˆ†æ‰¹è®­ç»ƒ
            for i in range(0, len(X_train_s), batch_size):
                batch_X = torch.FloatTensor(X_train_s[i:i+batch_size]).to(self.device)
                batch_y = torch.FloatTensor(y_train_s[i:i+batch_size]).unsqueeze(1).to(self.device)

                optimizer.zero_grad()
                pred = model(batch_X)
                loss = criterion(pred, batch_y)

                if torch.isfinite(loss):
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    optimizer.step()

                    epoch_loss += loss.item()
                    n_batches += 1

                del batch_X, batch_y
                torch.cuda.empty_cache()

            if n_batches > 0 and (epoch + 1) % 10 == 0:
                avg_loss = epoch_loss / n_batches
                logger.info(f"  Epoch {epoch+1}/30, Loss: {avg_loss:.6f}")

        # è¯„ä¼°
        logger.info("å¼€å§‹æ¨¡å‹è¯„ä¼°...")
        model.eval()
        predictions = []

        with torch.no_grad():
            for i in range(0, len(X_val_s), batch_size):
                batch_X = torch.FloatTensor(X_val_s[i:i+batch_size]).to(self.device)
                pred = model(batch_X)
                predictions.append(pred.cpu().numpy())
                del batch_X
                torch.cuda.empty_cache()

        y_pred_s = np.concatenate(predictions).flatten()

        # åæ ‡å‡†åŒ–
        y_pred_orig = scaler_y.inverse_transform(y_pred_s.reshape(-1, 1)).flatten()
        y_val_orig = scaler_y.inverse_transform(y_val_s.reshape(-1, 1)).flatten()

        # è®¡ç®—å­¦æœ¯æŒ‡æ ‡
        r2 = r2_score(y_val_orig, y_pred_orig)
        rmse = np.sqrt(mean_squared_error(y_val_orig, y_pred_orig))
        cv_rmse = rmse / np.mean(y_val_orig)
        nmbe = np.mean(y_pred_orig - y_val_orig) / np.mean(y_val_orig)

        return {
            'r2': r2,
            'cv_rmse': cv_rmse,
            'nmbe': nmbe,
            'rmse': rmse
        }

    def run_experiment(self):
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        logger.info("=" * 60)
        logger.info("LNGè®ºæ–‡å¤ç°é¡¹ç›® - ç»Ÿä¸€å®éªŒå…¥å£")
        logger.info("=" * 60)

        start_time = time.time()

        try:
            # 1. æ•°æ®åŠ è½½
            logger.info("\n--- é˜¶æ®µ 1: æ•°æ®å‡†å¤‡ ---")
            df = self.load_enhanced_data()

            # 2. ç‰¹å¾å·¥ç¨‹
            logger.info("\n--- é˜¶æ®µ 2: é«˜æ€§èƒ½ç‰¹å¾å·¥ç¨‹ ---")
            feature_start = time.time()
            X, y = self.extract_features_fast(df)
            feature_time = time.time() - feature_start
            logger.info(f"ç‰¹å¾æå–å®Œæˆ: {feature_time:.2f}ç§’, é€Ÿåº¦: {len(X)/feature_time:.1f} çª—å£/ç§’")

            # 3. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°
            logger.info("\n--- é˜¶æ®µ 3: è·¨æ¨¡æ€èåˆè®­ç»ƒ ---")
            results = self.train_model(X, y)

            # 4. ç»“æœæŠ¥å‘Š
            logger.info("\n" + "=" * 60)
            logger.info("LNGè·¨æ¨¡æ€èåˆæ¨¡å‹æœ€ç»ˆç»“æœ")
            logger.info("=" * 60)

            logger.info(f"\nğŸ“Š å­¦æœ¯æŒ‡æ ‡:")
            logger.info(f"  RÂ² Score: {results['r2']:.4f} {'âœ…' if results['r2'] >= 0.75 else 'âŒ'} (ç›®æ ‡ â‰¥ 0.75)")
            logger.info(f"  CV(RMSE): {results['cv_rmse']:.4f} {'âœ…' if results['cv_rmse'] <= 0.06 else 'âŒ'} (ç›®æ ‡ â‰¤ 0.06)")
            logger.info(f"  NMBE: {results['nmbe']:.4f} {'âœ…' if abs(results['nmbe']) <= 0.006 else 'âŒ'} (ç›®æ ‡ âˆˆ [-0.006, 0.006])")

            # è®ºæ–‡è¦æ±‚æ£€æŸ¥
            requirements_met = (
                results['r2'] >= 0.75 and
                results['cv_rmse'] <= 0.06 and
                abs(results['nmbe']) <= 0.006
            )

            if requirements_met:
                logger.info("\nğŸ‰ æ­å–œï¼è·¨æ¨¡æ€èåˆæ¨¡å‹è¾¾åˆ°è®ºæ–‡è¦æ±‚ï¼")
            else:
                logger.info("\nğŸ“ˆ æ¨¡å‹åœ¨ç°å®æ•°æ®ä¸Šå±•ç°æŒ‘æˆ˜æ€§ï¼Œä¸ºè¿›ä¸€æ­¥ç ”ç©¶æä¾›æ–¹å‘")

            # 5. ä¿å­˜ç»“æœ
            final_results = {
                'experiment': 'LNG_CrossModal_Fusion',
                'dataset': {
                    'source': 'anti_overfitting_data.csv',
                    'rows': len(df),
                    'windows': len(X),
                    'energy_cv': df['energy'].std()/df['energy'].mean()
                },
                'performance': {
                    'feature_extraction_time': feature_time,
                    'processing_speed': len(X)/feature_time
                },
                'academic_metrics': {
                    'r2': float(results['r2']),
                    'cv_rmse': float(results['cv_rmse']),
                    'nmbe': float(results['nmbe']),
                    'rmse': float(results['rmse'])
                },
                'requirements_met': requirements_met,
                'total_time': time.time() - start_time,
                'timestamp': datetime.now().isoformat()
            }

            # ä¿å­˜ç»“æœ
            results_dir = Path("results")
            results_dir.mkdir(exist_ok=True)
            results_path = results_dir / f"lng_experiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}.yaml"

            with open(results_path, 'w', encoding='utf-8') as f:
                yaml.dump(final_results, f, allow_unicode=True)

            logger.info(f"\nç»“æœå·²ä¿å­˜: {results_path}")
            logger.info(f"æ€»è¿è¡Œæ—¶é—´: {(time.time() - start_time)/60:.2f} åˆ†é’Ÿ")

            return results

        except Exception as e:
            logger.error(f"å®éªŒæ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            raise

def main():
    """ä¸»å…¥å£å‡½æ•°"""
    try:
        # åˆ›å»ºå¹¶è¿è¡Œå®éªŒ
        experiment = LNGExperiment()
        results = experiment.run_experiment()

        # æ ¹æ®ç»“æœè¿”å›é€€å‡ºç 
        if results['r2'] >= 0.75:
            sys.exit(0)  # æˆåŠŸ
        else:
            sys.exit(1)  # éœ€è¦æ”¹è¿›

    except KeyboardInterrupt:
        logger.info("\nç”¨æˆ·ä¸­æ–­å®éªŒ")
        sys.exit(1)
    except Exception as e:
        logger.error(f"å®éªŒå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()